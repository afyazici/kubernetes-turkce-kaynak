# Kubernetes

![Kubernetes](https://img.shields.io/badge/kubernetes-%23326ce5.svg?style=for-the-badge&logo=kubernetes&logoColor=white)

* Buradaki notlar, TechWorld Nina kanalÄ±na ait olan 3.5 saatlik "Kubernetes Tutorial for Beginners [FULL COURSE in 4 Hours]" videosunun TÃ¼rkÃ§e Ã§evirisidir.
* Video linki aÅŸaÄŸÄ±da yer almaktadÄ±r.

[![TeÅŸekkÃ¼rler](https://img.youtube.com/vi/X48VuDVv0do/0.jpg)](https://www.youtube.com/watch?v=X48VuDVv0do&t=2949s)


## Ä°Ã§indekiler


- [Kubernetes Nedir?](#kubernetes-nedir)
- [Kubernetes AvantajlarÄ±?](#kubernetes-avantajlarÄ±)
- [Kubernetes Componentleri?](#kubernetes-componentleri)
  - [Pod](#pod)
  - [Service ve Ingress](#service-ve-ingress)
  - [ConfigMap ve Secret](#configmap-ve-secret)
  - [Volumes](#volumes)
  - [Deployment ve StatefulSet](#deployment-ve-statefulset)
- [Kubernetes Mimarisi](#kubernetes-mimarisi)
  - [Node Process](#node-process)
    - [1) Container Runtime](#1-container-runtime)
    - [2) Kubelet](#2-kubelet)
    - [3) Kube Proxy](#3-kube-proxy)
  - [Master Node](#master-node)
    - [1) API Server](#1-api-server)
    - [2) Scheduler](#2-scheduler)
    - [3) Controller Manager](#3-controller-manager)
    - [4) Etcd](#4-etcd)
- [Cluster YapÄ±sÄ±](#cluster-yapÄ±sÄ±)
- [Minikube ve Kubectl Kurulumu](#minikube-ve-kubectl-kurulumu)
  - [Minikube](#minikube)
  - [Kubectl](#kubectl)
  - [Kurulum](#kurulum)
- [Ä°lk Cluster](#ilk-cluster)
- [Main Kubectl KomutlarÄ±](#main-kubectl-komutlarÄ±)
  - [LayerlarÄ±n Ã‡alÄ±ÅŸma Åekli](#layerlarÄ±n-Ã§alÄ±ÅŸma-ÅŸekli)
  - [Debugging Pods](#debugging-pods)
  - [Deployment Silme ve Apply Configuration File](#deployment-silme-ve-apply-configuration-file)
  - [Kubectl Apply](#kubectl-apply)
    - [Crud Commands](#crud-commands)
    - [Status of Different K8s Components](#status-of-different-k8s-components)
    - [Debugging Pods](#debugging-pods)
    - [Use Configuration File for CRUD](#use-configuration-file-for-crud)


## Kubernetes Nedir?
- **Kubernetes**, aÃ§Ä±k kaynaklÄ± bir konteyner yÃ¶netim aracÄ±dÄ±r.
- Google tarafÄ±ndan geliÅŸtirilmiÅŸtir.
- FarklÄ± daÄŸÄ±tÄ±m ortamlarÄ±nda konteynerleÅŸtirilmiÅŸ uygulamalarÄ± yÃ¶netmenize yardÄ±mcÄ± olur.
    - fiziksel ortamda
    - sanal ortamda
    - bulut ortamÄ±nda

## Kubernetes AvantajlarÄ±:

- **YÃ¼ksek eriÅŸilebilirlik** veya kesintisiz Ã§alÄ±ÅŸma
- **Ã–lÃ§eklenebilirlik** veya yÃ¼ksek performans
- **Afet kurtarma** - yedekleme ve geri yÃ¼kleme

## Kubernetes Componentleri
### Pod
* Pod Kubernetes'in en kÃ¼Ã§Ã¼k birimidir.
* Konteyner Ã¼zerinde sanallaÅŸtÄ±rma yapar. (abstraction)

![](images/1.png)


* Bizler yalnÄ±zca Kubernetes katmanÄ±yla etkileÅŸime geÃ§eriz.
* Bir pod iÃ§inde birden fazla konteyner Ã§alÄ±ÅŸtÄ±rabiliriz, ancak genellikle bir pod baÅŸÄ±na bir uygulama bulunur.
* Her pod kendi IP adresine sahiptir. Her pod, birbirleriyle bu internal IP adresini kullanarak iletiÅŸim kurabilir.

Ancak, Kubernetes'teki pod bileÅŸenleri de geÃ§icidir: yani Ã§ok kolay bir ÅŸekilde Ã¶lebilirler.
Ã–rneÄŸin, bir veritabanÄ± konteynerÄ±nÄ± kaybettiÄŸimizi dÃ¼ÅŸÃ¼nelim (konteyner iÃ§indeki uygulama Ã§Ã¶ktÃ¼ÄŸÃ¼ veya sunucu kaynaklarÄ± tÃ¼kendiÄŸi iÃ§in pod Ã¶lÃ¼r ve yerine yeni bir tane oluÅŸturulur). Bu durumda da yeni bir IP adresi atanÄ±r. VeritabanÄ±yla IP adresini kullanarak iletiÅŸim kuruyorsak elbette sakÄ±ncalÄ±dÄ±r. Her pod yeniden baÅŸladÄ±ÄŸÄ±nda, iletiÅŸimi her seferinde yeniden ayarlamamÄ±z gerekir. Bu nedenle, pod'un yeniden baÅŸladÄ±ÄŸÄ±nda IP adresini ayarlamamÄ±za gerek kalmadan, veritabanÄ±yla iletiÅŸim kurmanÄ±zÄ± saÄŸlayan baÅŸka bir Kubernetes bileÅŸeni olan `Service` kullanÄ±lÄ±r.

### Service ve Ingress

* Service, her pod'a baÄŸlanabilen sabit bir IP adresidir. UygulamamÄ±zÄ±n kendi service'i olacak ve veritabanÄ± pod'u kendi service'ine sahip olacak. Buradaki gÃ¼zel ÅŸey, service ve Pod'un yaÅŸam dÃ¶ngÃ¼leri birbirine baÄŸlÄ± deÄŸil. Bu yÃ¼zden Pod Ã¶lse bile Service ve bu service'e ait IP adresi kalÄ±r. Bu ÅŸekilde endpoint'i deÄŸiÅŸtirmemize gerek yoktur.

![](images/2.png)

* Tabii ki, uygulamamÄ±zÄ±n bir tarayÄ±cÄ± aracÄ±lÄ±ÄŸÄ±yla eriÅŸilebilir olmasÄ±nÄ± isteriz deÄŸil mi? Bunun iÃ§in bir external service oluÅŸturmamÄ±z gerekiyor. `External Service`, dÄ±ÅŸ kaynaklardan iletiÅŸimi aÃ§an bir servistir. Ancak, veritabanÄ±mÄ±zÄ± halka aÃ§Ä±k isteklere aÃ§mak istemeyiz. Bunun iÃ§in `Internal Service` adÄ±nÄ± verdiÄŸimiz bir ÅŸey oluÅŸtururuz.

![](images/3.png)

* External Service URL'sinin Ã§ok pratik olmadÄ±ÄŸÄ±nÄ± fark ettiniz mi? Temelde, bir HTTP protokolÃ¼yle bir node IP adresi ve service port numarasÄ±na sahibiz. Bu hÄ±zlÄ± bir ÅŸekilde bir ÅŸeyleri test etmek istiyorsak iyidir, ancak end-product iÃ§in iyi deÄŸildir. Genellikle, uygulamamÄ±zla gÃ¼venli bir protokol ve bir alan adÄ± kullanmak isteriz.

![](images/4.png)

Bunun iÃ§in Kubernetes'in baÅŸka bir bileÅŸeni olan `Ingress` var. Bu ÅŸekilde, istek Ã¶nce service'e deÄŸil, Ingress'e gider ve oradan Service'e yÃ¶nlendirilir.

![](images/5.png)

### ConfigMap ve Secret

Pod'lar birbirleriyle `service` aracÄ±lÄ±ÄŸÄ±yla iletiÅŸim kurar. UygulamamÄ±zÄ±n, veritabanÄ± ile iletiÅŸim kurmak iÃ§in kullandÄ±ÄŸÄ± bir database endpoint `Ã¶rneÄŸin mongodb service`'i olacak. Ancak bu veritabanÄ± URL'sini (ya da endpoint) genellikle nerede yapÄ±landÄ±rÄ±rÄ±z?

![](images/6.png)

Genellikle bunu application properties file veya bazÄ± external environmental variable olarak yaparÄ±z, ancak genellikle yapÄ±landÄ±rma, uygulamanÄ±n iÃ§erisindeki built image'tedir.

Ã–rneÄŸin, service endpoint (ya da service name) 'mongodb' olarak deÄŸiÅŸirse, uygulamadaki bu URL'i ayarlamalÄ±yÄ±z. Genellikle yeni bir sÃ¼rÃ¼mle uygulamayÄ± rebuild etmemiz ve repoya pushlamamÄ±z gerekir. ArdÄ±ndan bu yeni image'i pod'umuzda pull'layÄ±p tÃ¼m uygulamayÄ± yeniden baÅŸlatmamÄ±z gerekebilir.

![](images/7.png)

VeritabanÄ± URL'i gibi kÃ¼Ã§Ã¼k bir deÄŸiÅŸiklik iÃ§in bu gerÃ§ekten zahmetli. Bu sebeple, Kubernetes'in `configmap` adÄ±nda bir bileÅŸeni var. YapÄ±sÄ±, uygulamamÄ±za `external configuration` saÄŸlar. ConfigMap genellikle kullandÄ±ÄŸÄ±mÄ±z veritabanÄ± URL'leri gibi yapÄ±landÄ±rma verilerini iÃ§erir. Kubernetes'te bunu Pod'a baÄŸlarÄ±z. Pod, ConfigMap'in iÃ§erdiÄŸi verileri alÄ±r. Ve ÅŸimdi, service adÄ±nÄ± deÄŸiÅŸtirirsek (service end point), sadece ConfigMap'i ayarlamamÄ±z yeterlidir. Yeni bir image oluÅŸturmamÄ±za ve tÃ¼m dÃ¶ngÃ¼yÃ¼ geÃ§irmemize gerek yoktur. BÃ¼yÃ¼k avantaj!

![](images/8.png)

Åimdi, external configuration'Ä±n bir parÃ§asÄ± aynÄ± zamanda database kullanÄ±cÄ± adÄ± ve ÅŸifresi olabilir deÄŸil mi? Bu veriler de uygulama daÄŸÄ±tÄ±m sÃ¼recinde deÄŸiÅŸebilir. Ancak, bir ÅŸifreyi veya diÄŸer kimlik bilgilerini dÃ¼z metin formatÄ±nda bir configmap'e koymamÄ±z gÃ¼vensiz olur.

![](images/9.png)

Bu amaÃ§la, Kubernetes'in `Secret` adÄ±nda bir baÅŸka bileÅŸeni daha var. Yani, Secret, ConfigMap gibi, ancak fark ÅŸu ki; ÅŸifre gibi gizli verileri saklamak iÃ§in kullanÄ±lÄ±r. Ve tabii ki, dÃ¼z metin formatÄ±nda deÄŸil, base64 formatÄ±nda kodlanmÄ±ÅŸ olarak saklanÄ±r. Yani, Secret, kullanÄ±cÄ± adlarÄ± gibi kimlik bilgilerini iÃ§erecek ve veritabanÄ± kullanÄ±cÄ±larÄ±nÄ± iÃ§erecektir. ConfigMap'e de koyabiliriz, ancak Ã¶nemli olan ÅŸifreler, sertifikalar, baÅŸkalarÄ±nÄ±n eriÅŸimini istemediÄŸimiz ÅŸeyler Secret'e koyulmalÄ±dÄ±r. AynÄ± ConfigMap gibi, sadece Pod'umuza baÄŸlarÄ±z, bÃ¶ylece Pod bu verileri gÃ¶rebilir ve Secret'ten okuyabilir. ConfigMap veya Secret'ten verileri, Ã¶rneÄŸin environment variables olarak veya hatta bir Ã¶zellikler dosyasÄ± olarak uygulamamÄ±zÄ±n iÃ§inde kullanabiliriz.

![](images/10.png)

AslÄ±nda en Ã§ok kullanÄ±lan Kubernetes temel bileÅŸenlerinin neredeyse tamamÄ±nÄ± gÃ¶rdÃ¼k. Pod'a gÃ¶z attÄ±k. Hizmetlerin nasÄ±l kullanÄ±ldÄ±ÄŸÄ±nÄ±, Ingress bileÅŸeninin ne iÅŸe yaradÄ±ÄŸÄ±nÄ± gÃ¶rdÃ¼k ve ayrÄ±ca ConfigMap ve Secrets'Ä± kullanan harici yapÄ±landÄ±rmayÄ± da gÃ¶rdÃ¼k.

![](images/11.png)

### Volumes

SÄ±ra geldi Ã§ok Ã¶nemli bir kavrama. `Veri depolama` nedir ve Kubernetes iÃ§erisinde nasÄ±l Ã§alÄ±ÅŸÄ±r? UygulamamÄ±zÄ±n kullandÄ±ÄŸÄ± bir database pod'umuz ve de bir miktar verimiz var. Åu anda gÃ¶rdÃ¼ÄŸÃ¼nÃ¼z bu kurulumla, eÄŸer veritabanÄ± container veya pod'u yeniden baÅŸlatÄ±lÄ±rsa veri kaybolur. Bu aÃ§Ä±kÃ§a sorunlu ve elveriÅŸsizdir Ã§Ã¼nkÃ¼ database'deki verilerinizin veya gÃ¼nlÃ¼k verilerinizin uzun sÃ¼reli gÃ¼venilir ve kalÄ±cÄ± olmasÄ±nÄ± istersiniz. Bunu Kubernetes'te yapmanÄ±n yolu, Kubernetes'in baÅŸka bir bileÅŸeni olan `Volumes` kullanmaktÄ±r.

![](images/12.png)

Ã‡alÄ±ÅŸma ÅŸekli ÅŸÃ¶yledir: Temelde bir fiziksel depolama birimini -yani bir sabit diski- pod'umuza baÄŸlarÄ±z.
* Bu depolama yerel bir makinede olabilir.
* Pod'un Ã§alÄ±ÅŸtÄ±ÄŸÄ± aynÄ± sunucu node'unda da olabilir.
* Kubernetes kÃ¼mesinin dÄ±ÅŸÄ±nda(Bulut depolama, Kubernetes kÃ¼mesinin bir parÃ§asÄ± olmayan kendi yerleÅŸke depolamanÄ±z) olabilir.
Bu yÃ¼zden bununla ilgili external reference var.

![](images/13.png)

BÃ¶ylece, database pod'u veya container yeniden baÅŸlatÄ±ldÄ±ÄŸÄ±nda, tÃ¼m veri kalÄ±cÄ± bir ÅŸekilde saklanmÄ±ÅŸ olacaktÄ±r.

Kubernetes kÃ¼mesi ve tÃ¼m bileÅŸenlerinin ve depolama arasÄ±ndaki farkÄ± anlamamÄ±z Ã¶nemlidir. Yerel veya uzak bir depolama olmasÄ± fark etmeksizin, depolamayÄ± Kubernetes kÃ¼mesine takÄ±lmÄ±ÅŸ harici bir sabit diske benzetebilirsiniz. Ã‡Ã¼nkÃ¼ buradaki Ã¶nemli nokta; Kubernetes kÃ¼mesi aÃ§Ä±kÃ§a hiÃ§bir veri kalÄ±cÄ±lÄ±ÄŸÄ±nÄ± yÃ¶netmez. Kubernetes kullanÄ±cÄ±sÄ± veya yÃ¶neticisi olarak sizin veriyi yedeklemenizden, Ã§oÄŸaltmanÄ±zdan, yÃ¶netmenizden ve uygun donanÄ±mda saklamanÄ±zdan emin olmanÄ±z gerektiÄŸi anlamÄ±na gelir.

### Deployment ve StatefulSet

Åimdi, her ÅŸey mÃ¼kemmel bir ÅŸekilde Ã§alÄ±ÅŸÄ±yor ve bir kullanÄ±cÄ± bir tarayÄ±cÄ± aracÄ±lÄ±ÄŸÄ±yla uygulamaya eriÅŸebiliyor. Bu kurulumla, application pod'u Ã¶lÃ¼rse, crashlerse veya yeni bir container image oluÅŸturduÄŸumuz  iÃ§in pod'u restart etmemiz gerekiyorsa ne olurdu? BasitÃ§e cevap verecek olursak, bir kullanÄ±cÄ±nÄ±n uygulamamÄ±za ulaÅŸamadÄ±ÄŸÄ± bir sÃ¼re olan bir kesintimiz olurdu. BÃ¶yle bir durum end product'ta gerÃ§ekleÅŸmesi Ã§ok kÃ¶tÃ¼ bir durumdur.

![](images/14.png)

Distributed systems ve konteynerlarÄ±n avantajÄ± tam olarak budur. YalnÄ±zca 1 application pod'u ve 1 database pod'u gibi bir ÅŸeye gÃ¼venmek yerine, her ÅŸeyi birden fazla sunucuda replikasÄ±nÄ± oluÅŸturuyoruz. Yani uygulamamÄ±zÄ±n bir klonu veya Ã§oÄŸaltmasÄ± Ã§alÄ±ÅŸacaÄŸÄ± baÅŸka bir node olacak ve bu da service'e baÄŸlÄ± olacak. HatÄ±rlarsanÄ±z service'in, bir pod Ã¶ldÃ¼ÄŸÃ¼nde end point'i sÃ¼rekli ayarlamamÄ±za gerek olmadÄ±ÄŸÄ±, kalÄ±cÄ± statik IP adresi ve bir DNS adÄ±na sahip olduÄŸunu sÃ¶ylemiÅŸtik.

* Service aynÄ± zamanda bir `load balancer`dÄ±r. Yani, service isteÄŸi yakalayacak ve en az meÅŸgul olan pod'a yÃ¶nlendirecektir.


Ancak application pod'unun ikinci replikasÄ±nÄ± oluÅŸturmak iÃ§in ikinci bir pod oluÅŸturmayÄ±z. Bunun yerine uygulama pod'umuzun bir blueprint'ini tanÄ±mlarÄ±z ve o pod'un kaÃ§ tane replikasÄ±nÄ±n olmasÄ±nÄ± istediÄŸimizi belirtiriz. Ve bu component veya blueprint'e `deployment` denir. Deployment, Kubernetes'in baÅŸka bir componentidir. Pratikte, pod'larla Ã§alÄ±ÅŸmayÄ±z veya pod'lar oluÅŸturmayÄ±z. Ã‡Ã¼nkÃ¼ zaten kaÃ§ tane replika olacaÄŸÄ±nÄ± belirtebilir ve ihtiyacÄ±mÄ±z olan pod'larÄ±n replika sayÄ±sÄ±nÄ± artÄ±rabilir veya azaltabiliriz. Yani pod, container'larÄ±n Ã¼zerinde bir soyutlama katmanÄ±dÄ±r(layer of abstraction). Deployment ise, podlarÄ±n Ã¼zerinde baÅŸka bir soyutlama katmanÄ±dÄ±r(layer of abstraction). Bu durum; pod'larla etkileÅŸimi, kopyalama ve diÄŸer yapÄ±landÄ±rmalarÄ± daha kullanÄ±ÅŸlÄ± hale getirir.

![](images/15.png)

Yani sonuÃ§ olarak Ã§oÄŸunlukla pod'larla deÄŸil, deployment'larla Ã§alÄ±ÅŸÄ±rÄ±z. Uygulama pod'umuzun replikalarÄ±ndan biri Ã¶lÃ¼rse, `service` istekleri baÅŸka bir replikaya yÃ¶nlendirilecektir, bu ÅŸekilde uygulamamÄ±z kullanÄ±cÄ±lar iÃ§in hala eriÅŸilebilir olacaktÄ±r.

![](images/16.png)

Åimdi muhtemelen ÅŸunu merak ediyorsunuzdur, peki database pod'u ne olacak? Ã‡Ã¼nkÃ¼ eÄŸer database pod'u Ã¶lÃ¼rse, uygulamanÄ±z da eriÅŸilemez olacaktÄ±r. Bu yÃ¼zden, bir database replikasÄ±na da ihtiyacÄ±mÄ±z var. Ancak, `deployment kullanarak bir databese'i kopyalayamayÄ±z`. Bunun nedeni, database'in bir state'i olmasÄ±dÄ±r, yani veridir. Bu da demektir ki eÄŸer database'in replikalarÄ± veya klonlarÄ± olsaydÄ±, hepsi aynÄ± paylaÅŸÄ±lan data storage volume'Ã¼ne eriÅŸmek zorunda kalacaktÄ±. Bu durumda da, hangi pod'larÄ±n anlÄ±k olarak depolama birimine yazdÄ±ÄŸÄ±nÄ± veya hangi pod'larÄ±n depolama biriminden okuduÄŸunu yÃ¶neten bir mekanizmaya ihtiyacÄ±mÄ±z olacaktÄ±.

Bu mekanizma, Ã§oÄŸaltma Ã¶zelliklerinin yanÄ± sÄ±ra baÅŸka bir Kubernetes componenti olan `StatefulSet` ile saÄŸlanÄ±r.

![](images/17.png)

Bu component Ã¶zellikle database gibi uygulamalar iÃ§in tasarlanmÄ±ÅŸtÄ±r. Yani, MySQL, MongoDB, Elasticsearch veya herhangi bir diÄŸer stateful applications veya databaseleri; deployments yerine `StatefulSets` kullanÄ±larak oluÅŸturulmalÄ±dÄ±r.
Bu Ã§ok Ã¶nemli bir ayrÄ±mdÄ±r. StatefulSet, aynÄ± deployment gibi, pod'larÄ± replikalamayÄ± yapar ve bunlarÄ± scaling'e alÄ±r. Database reading ve writing iÅŸlemlerinin senkronize olduÄŸundan emin olur, bÃ¶ylece database tutarsÄ±zlÄ±klarÄ± olmaz.

![](images/18.png)

Ancak, bir Kubernetes kÃ¼mesinde StatefulSets kullanarak database uygulamalarÄ±nÄ± deploy etmek biraz zahmetli olabilir. Bu yÃ¼zden, database uygulamalarÄ±nÄ± Kubernetes kÃ¼mesinin dÄ±ÅŸÄ±nda barÄ±ndÄ±rmak ve yalnÄ±zca daÄŸÄ±tÄ±mlarÄ± veya durumsuz uygulamalarÄ± Kubernetes kÃ¼mesinin iÃ§inde sorunsuz bir ÅŸekilde Ã§oÄŸaltmak ve Ã¶lÃ§eklendirmek ve dÄ±ÅŸ database ile iletiÅŸim kurmak yaygÄ±n bir uygulamadÄ±r.

Åimdi, uygulama pod'umuzun iki replikasÄ± ve database'in iki kopyasÄ± olduÄŸunda ve hepsi load-balanced olduÄŸunda, kurulumumuz daha gÃ¼venlidir. Bu senaryoda eÄŸer Node 1 yeniden baÅŸlatÄ±lsaydÄ± veya Ã§Ã¶kseydi, hala uygulama ve database pod'larÄ±nÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ± ikinci bir node'umuz olurdu. Uygulama, bu iki replikadan yeniden oluÅŸturulana kadar kullanÄ±cÄ± tarafÄ±ndan eriÅŸilebilir olacaktÄ±r, bu yÃ¼zden kesintiyi Ã¶nlemiÅŸ oluruz.

![](images/19.png)

Ã–zetlemek gerekirse, en Ã§ok kullanÄ±lan Kubernetes bileÅŸenlerini inceledik. ParÃ§alar arasÄ±nda iletiÅŸim kurmak iÃ§in `pod`lar ve `service`lerle baÅŸladÄ±k, ve trafiÄŸi clusterlara yÃ¶nlendirmek iÃ§in kullanÄ±lan `Ingress` bileÅŸenini inceledik. AyrÄ±ca, `ConfigMaps` ve `Secret` kullanarak external configuration, ve `Volumes` kullanarak veri kalÄ±cÄ±lÄ±ÄŸÄ±nÄ± inceledik. Ve son olarak, `Deployments` ve `StatefulSets` gibi replicating ve blueprintlere baktÄ±k.

Burada `stateful applications` Ã¶zellikle databaseler gibi stateful applications iÃ§in kullanÄ±lÄ±r. Ve evet, Kubernetes'in sunduÄŸu Ã§ok daha fazla bileÅŸen var, ama bunlar Ã§ekirdek, temel olanlarÄ±. Bu temel bileÅŸenleri kullanarak oldukÃ§a gÃ¼Ã§lÃ¼ Kubernetes kÃ¼mesi oluÅŸturabiliriz.

## Kubernetes Mimarisi

Kubernetes'in temel mimarisinden bahsedeceÄŸiz. Bu yÃ¼zden Kubernetes'in Ã§alÄ±ÅŸtÄ±ÄŸÄ± iki tÃ¼r node'u inceleyeceÄŸiz: biri `master` diÄŸeri ise `slave`. BunlarÄ±n arasÄ±ndaki fark nedir ve her birinin cluster iÃ§indeki rolÃ¼ nedir, onlara bakacaÄŸÄ±z.

Kubernetes'in ne yaptÄ±ÄŸÄ±nÄ± ve cluster'Ä±n nasÄ±l self-managed, self-healing ve automated olduÄŸunu gÃ¶steren temel kavramlarÄ± ele alacaÄŸÄ±z.  Bir Kubernetes cluster operatÃ¶rÃ¼ olarak, Ã§ok daha az manuel Ã§aba harcamamÄ±z gerektiÄŸini gÃ¶receÄŸiz.

### Node Process

Ä°ki application pod'unun Ã§alÄ±ÅŸtÄ±ÄŸÄ± tek bir node ile bu temel kurulumla baÅŸlayacaÄŸÄ±z. Kubernetes mimarisinin ana bileÅŸenlerinden biri worker servers veya node'dur. Her node, o node'da Ã§alÄ±ÅŸan birden fazla application pod'una sahip olacaktÄ±r.

Ve Kubernetes'in bunu yapma ÅŸekli, her node'da bulunmasÄ± gereken ve bu pod'larÄ± planlamak ve yÃ¶netmek iÃ§in kullanÄ±lan `three process` kullanmasÄ±dÄ±r. Yani node'lar, asÄ±l iÅŸi yapan cluster serverlardÄ±r. Bu yÃ¼zden bazen onlara worker nodes da denir.

#### 1) Container Runtime

Her node'da Ã§alÄ±ÅŸmasÄ± gereken ilk sÃ¼reÃ§, `container runtime`dÄ±r. Biz Docker ile iÅŸlem yapacaÄŸÄ±z, ancak baÅŸka bir teknoloji de olabilir. Applitacion pod'larÄ±nda iÃ§inde Ã§alÄ±ÅŸan containerlar olduÄŸu iÃ§in, her node'da bir `container runtime`  kurulmalÄ±dÄ±r.

#### 2) Kubelet

Kubernetes'in bir parÃ§asÄ± olan kubelet, Pod'larÄ± ve bu Pod'larÄ±n altÄ±ndaki container'larÄ± schedule eder. Container Runtime, node'un kendisiyle arayÃ¼z oluÅŸtururken, kubelet bu yapÄ±landÄ±rmayÄ± alÄ±r, bir pod'u Ã§alÄ±ÅŸtÄ±rÄ±r (veya iÃ§inde bir container baÅŸlatÄ±r) ve ardÄ±ndan o node'dan container'a CPU, RAM gibi depolama kaynaklarÄ± atar.

Bu nedenle, genellikle bir Kubernetes cluster kurulu olmalÄ±dÄ±r. Kubelet hizmetlerine sahip birden fazla node'dan oluÅŸur. Bu worker node'larÄ±, Ã–rneÄŸimizdeki application ve database pod'larÄ±nÄ±n replikalarÄ±nÄ± Ã§alÄ±ÅŸtÄ±racak yÃ¼zlerce diÄŸer node'u Ã§alÄ±ÅŸtÄ±rÄ±r.

AralarÄ±ndaki iletiÅŸim ÅŸekli, `services` ile olur, bu da isteÄŸi application parÃ§asÄ±na veya Ã¶rneÄŸin bir database'e yÃ¶nlendiren bir `load-balancer` gibi Ã§alÄ±ÅŸÄ±r ve ardÄ±ndan ilgili parÃ§aya yÃ¶nlendirir.

#### 3) Kube Proxy

Hizmetlerden pod'lara istekleri iletmekten sorumlu Ã¼Ã§Ã¼ncÃ¼ sÃ¼reÃ§ `kube-proxy`dir ve her node'da kurulmalÄ±dÄ±r. Kube-proxy, dÃ¼ÅŸÃ¼k bir iÅŸlem yÃ¼kÃ¼ ile performanslÄ± bir ÅŸekilde iletiÅŸim kurulmasÄ±nÄ± saÄŸlayan akÄ±llÄ± yÃ¶nlendirme mantÄ±ÄŸÄ±na sahiptir.

Bir uygulama veya bu uygulamanÄ±n replikasÄ±, bir database'e istek yapÄ±yorsa, hizmet sadece isteÄŸi rastgele bir replikaya yÃ¶nlendirmek yerine; isteÄŸi baÅŸlatan pod'un Ã§alÄ±ÅŸtÄ±ÄŸÄ± aynÄ± node'da Ã§alÄ±ÅŸan replikaya yÃ¶nlendirecektir. Bu ÅŸekilde, isteÄŸi baÅŸka bir makineye gÃ¶ndermekle ilgili `aÄŸ iÅŸlem yÃ¼kÃ¼nden` kaÃ§Ä±nÄ±lmÄ±ÅŸ olur.

Ã–zetlemek gerekirse; bir kubernetes cluster'Ä±nÄ±n dÃ¼zgÃ¼n Ã§alÄ±ÅŸabilmesi iÃ§in `kubelet` ve `kube-proxy` her worker node iÃ§erisine `container runtime` ile birlikte kurulmalÄ±dÄ±r.

![](images/20.png)

Ancak ÅŸimdi soru ÅŸu: Bu cluster ile nasÄ±l etkileÅŸime girilir? Yeni bir application pod'u veya database pod'u nerede schedule edilmeli? Bir replika pod'u Ã¶lÃ¼rse, hangi process monitoring, reschedule veya restart iÅŸlemleri ile ilgilenir?

### Master Node

Master servers(master nodes), iÃ§erisinde tamamen farklÄ± processler Ã§alÄ±ÅŸtÄ±rÄ±r. Ve bunlar, cluster state ve worker nodes'larÄ± kontrol eden =`her yÃ¶netici dÃ¼ÄŸÃ¼mÃ¼nde Ã§alÄ±ÅŸan dÃ¶rt sÃ¼reÃ§tir`.

#### 1) API Server

Ä°lk hizmetimiz API server. Bir Kubernetes cluster'Ä±nda yeni bir application deploy etmek istediÄŸinizde, bir kullanÄ±cÄ± olarak API server ile interact ederiz. Bir Kubernetes Dashboard gibi bir kullanÄ±cÄ± arayÃ¼zÃ¼ de olabilir, `kubectl` gibi bir command-line tool veya bir Kubernetes API'si de olabilir.

API sunucusu, cluster iÃ§ine herhangi bir gÃ¼ncelleme talebinin veya hatta clusterdan gelen sorgularÄ±n ilk isteÄŸini alÄ±r. Kimlik doÄŸrulamasÄ±(auth) yaparak, yalnÄ±zca kimliÄŸi doÄŸrulanmÄ±ÅŸ ve yetkilendirilmiÅŸ isteklerin clusterlara iletilmesini saÄŸlar.

![](images/21.png)

Bu, yeni pod'lar planlamak, yeni applications deploy etmek, yeni services oluÅŸturmak veya herhangi bir component oluÅŸturmak istediÄŸimizde, bu requestimizi master node API sunucusuna iletmek zorunda olduÄŸunuz anlamÄ±na gelir. API server daha sonra requestimizi doÄŸrular. Her ÅŸey yolundaysa, requestimizi diÄŸer sÃ¼reÃ§lere ileterek istediÄŸimiz pod'u veya bileÅŸeni schedule iÃ§in bir node'a yÃ¶nlendirir.

![](images/22.png)

AyrÄ±ca, daÄŸÄ±tÄ±mÄ±mÄ±zÄ±n durumu veya cluster health etc., gibi sorgu isteklerini yapmak isteyebiliriz. Bu sorgular API sunucusuna bir istek gÃ¶nderir ve o da bize yanÄ±t verir. Bu durum gÃ¼venlik aÃ§Ä±sÄ±ndan gayet iyidir Ã§Ã¼nkÃ¼ clusterlara yalnÄ±zca `bir entry point` vardÄ±r.

#### 2) Scheduler

BaÅŸka bir Master process ise Scheduler'dÄ±r. API serverÄ±na yeni bir pod schedule isteÄŸi gÃ¶nderdiÄŸimizi varsayalÄ±m. API server bu isteÄŸimizi doÄŸruladÄ±ktan sonra, bu pod'un bir worker node'da baÅŸlatÄ±lmasÄ± iÃ§in `Scheduler`'a teslim eder.

Ve tabii ki, herhangi bir node'a rastgele atamak yerine, Scheduler, bir sonraki pod'un hangi belirli worker node'un scheduled olacaÄŸÄ± konusunda zekice bir ÅŸekilde karar vermektedir. Ä°lk olarak, isteÄŸimizi kontrol eder ve planlamak istediÄŸimiz uygulamanÄ±n ne kadar kaynaÄŸa ihtiyacÄ± olduÄŸunu kontrol eder. Ne kadar CPU, ne kadar RAM vb.

ArdÄ±ndan, worker node'daki kullanÄ±labilir kaynaklarÄ± kontrol eder. EÄŸer bir node'un en Ã§ok kaynaÄŸa sahip olduÄŸunu sÃ¶ylÃ¼yorsa, yeni pod'u o node'a schedule eder.

![](images/23.png)

Ã–nemli bir nokta ÅŸu ki, scheduler sadece yeni bir pod'un hangi node'a schedule edileceÄŸine karar verir. AsÄ±l planlamayÄ± yapan ve pod'u konteynerla baÅŸlatan iÅŸlem ise `kubelet`'tir. Yani kubelet, scheduler'dan gelen isteÄŸi alÄ±r ve bu isteÄŸi ilgili node Ã¼zerinde yÃ¼rÃ¼tÃ¼r.

#### 3) Controller Manager

Bir sonraki Ã¶nemli bileÅŸen ise `controller manager`'dÄ±r. Bu bileÅŸen, herhangi bir dÃ¼ÄŸÃ¼mde pod'lar Ã¶ldÃ¼ÄŸÃ¼nde ne olacaÄŸÄ± sorusu aÃ§Ä±sÄ±ndan kritik Ã¶neme sahiptir. Ã–lÃ¼ node'larÄ± tespit etmek ve daha sonra bu pod'larÄ± en kÄ±sa sÃ¼rede reschedule etmek gerekir.

![](images/24.png)

DolayÄ±sÄ±yla controller manager, state changes'larÄ±, Ã¶rneÄŸin pod'larÄ±n Ã§Ã¶kmesini tespit eder. Pod'lar Ã¶ldÃ¼ÄŸÃ¼nde controller manager bunu algÄ±lar ve cluster state'ini mÃ¼mkÃ¼n olan en kÄ±sa sÃ¼rede kurtarmaya Ã§alÄ±ÅŸÄ±r.

Ã–len pod'larÄ± yeniden schedule etme amacÄ±yla scheduler'a bir istek gÃ¶nderir. Bu dÃ¶ngÃ¼ iÃ§inde, scheduler kaynak hesaplamasÄ±na gÃ¶re hangi worker node'larÄ±n bu pod'larÄ± tekrar baÅŸlatmasÄ± gerektiÄŸine karar verir ve bu worker node'lar Ã¼zerindeki ilgili `kubelet`lere, pod'larÄ± yeniden baÅŸlatmalarÄ± iÃ§in istek gÃ¶nderir.

![](images/25.png)

#### 4) Etcd

Son olarak, ana iÅŸlemlerden biri olan etcd, bir cluster state'inin key-value deposudur. Bunu aslÄ±nda bir cluster beyni olarak dÃ¼ÅŸÃ¼nebiliriz. Yani cluster'daki her deÄŸiÅŸiklik -Ã¶rneÄŸin yeni bir pod schedule edildiÄŸinde veya bir pod Ã¶ldÃ¼ÄŸÃ¼nde- etcd'nin bu key-value deposunda kaydedilir veya gÃ¼ncellenir.

![](images/26.png)

Etcd deposunun bir kÃ¼me beyni olarak adlandÄ±rÄ±lmasÄ±nÄ±n sebebi, scheduler, controller manager gibi tÃ¼m bu mekanizmalarÄ±n, etcd'nin sahip olduÄŸu veriler sayesinde Ã§alÄ±ÅŸmasÄ±dÄ±r.

![](images/27.png)

Ã–rneÄŸin, scheduler her bir worker node'unda hangi kaynaklarÄ±n mevcut olduÄŸunu nasÄ±l bilir? Veya controller manager, cluster durumunda bir deÄŸiÅŸiklik olduÄŸunu nasÄ±l tespit eder? Pod'larÄ±n Ã¶lmesi, kubelet'in scheduler'Ä±n isteÄŸi Ã¼zerine yeni pod'larÄ± baÅŸlatmasÄ±, API sunucusuna cluster health hakkÄ±nda bir sorgu gÃ¶ndermemiz veya uygulama daÄŸÄ±tÄ±m durumumuz gibi bu durum bilgilerini, API sunucusu nereden alÄ±r?

Cevap: TÃ¼m bu bilgiler etcd kÃ¼mesinde saklanÄ±r. Etcd'nin key-value deposunda saklanmayan ÅŸey ise gerÃ§ek uygulama verileridir. Ã–rneÄŸin, bir cluster iÃ§inde Ã§alÄ±ÅŸan bir database uygulamamÄ±z varsa, veriler etcd'de deÄŸil, baÅŸka bir yerde saklanÄ±r. Bu, yalnÄ±zca master iÅŸlemlerinin worker iÅŸlemleriyle ve tersiyle iletiÅŸim kurmasÄ± iÃ§in kullanÄ±lan bir cluster state bilgisidir.

ArtÄ±k muhtemelen ana iÅŸlemlerin, Ã¶zellikle de verileri gÃ¼venilir bir ÅŸekilde saklanmasÄ± veya Ã§oÄŸaltÄ±lmasÄ± gereken etcd deposunun, cluster operasyonu iÃ§in kritik Ã¶neme sahip olduÄŸunu anlamÄ±ÅŸsÄ±nÄ±zdÄ±r. Bu nedenle, uygulamada bir Kubernetes kÃ¼mesi genellikle birden fazla master'dan oluÅŸur. Her bir master dÃ¼ÄŸÃ¼mÃ¼ kendi ana iÅŸlemlerini Ã§alÄ±ÅŸtÄ±rÄ±r; elbette API sunucusu load-balanced'dÄ±r ve etcd deposu tÃ¼m master dÃ¼ÄŸÃ¼mleri arasÄ±nda distributed bir depolama oluÅŸturur.

![](images/28.png)



## Cluster YapÄ±sÄ±


Åimdi worker ve master node'larÄ±nda Ã§alÄ±ÅŸan iÅŸlemleri gÃ¶rdÃ¼kten sonra, gerÃ§ek hayattaki bir cluster kurulumuna bakalÄ±m. Ã‡ok kÃ¼Ã§Ã¼k bir cluster'da muhtemelen iki master node ve Ã¼Ã§ worker node olur.

![](images/29.png)

Burada dikkat edilmesi gereken bir diÄŸer nokta ise master node sunucularÄ±nÄ±n donanÄ±m kaynaklarÄ±nÄ±n aslÄ±nda farklÄ± olmasÄ±dÄ±r. Master iÅŸlemleri daha Ã¶nemlidir, ancak aslÄ±nda daha az iÅŸ yÃ¼kÃ¼ne sahiptirler. DolayÄ±sÄ±yla CPU, RAM ve depolama gibi daha az kaynaÄŸa ihtiyaÃ§ duyarlar. Worker node'larÄ± ise, containerlarÄ± Ã§alÄ±ÅŸtÄ±ran pod'larÄ± barÄ±ndÄ±rma gibi asÄ±l iÅŸi yaparlar.

Bu nedenle, worker node'larÄ±nÄ±n daha fazla kaynaÄŸa ihtiyacÄ± vardÄ±r. UygulamamÄ±zÄ±n karmaÅŸÄ±klÄ±ÄŸÄ± ve kaynak gereksinimi arttÄ±kÃ§a, aslÄ±nda cluster'Ä±mÄ±za daha fazla master ve worker node'u ekleyerek daha gÃ¼Ã§lÃ¼ ve saÄŸlam bir kÃ¼me oluÅŸturabiliriz. BÃ¶ylece uygulama kaynak gereksinimlerimizi karÅŸÄ±layabiliriz.

![](images/30.png)

Var olan bir Kubernetes cluster'Ä±nda yeni master veya worker serverlarÄ± eklemek aslÄ±nda oldukÃ§a kolaydÄ±r. Bir master sunucusu eklemek istiyorsak, yeni bir bare metal sunucu ediniyoruz. Ãœzerine tÃ¼m master work'leri kurup  onu Kubernetes kÃ¼mesine ekliyoruz. Bu kadar..

Yine aynÄ± ÅŸekilde, iki worker node'una ihtiyacÄ±mÄ±z varsa, bare metal sunucular ediniyoruz. Container runtime, kubelet ve kube-proxy gibi tÃ¼m worker node iÅŸlemlerini Ã¼zerlerine kurup onlarÄ± Kubernetes clusterÄ±na ekleyin.

Ä°ÅŸte bu kadar.
Bu ÅŸekilde, uygulama karmaÅŸÄ±klÄ±ÄŸÄ± ve kaynak gereksinimi arttÄ±kÃ§a, Kubernetes kÃ¼memizin gÃ¼cÃ¼nÃ¼ ve kaynaklarÄ±nÄ± sonsuza kadar artÄ±rabiliriz.


## Minikube ve Kubectl Kurulumu

### Minikube

![](images/31.png)

Genellikle Kubernetes dÃ¼nyasÄ±nda bir production cluster kurduÄŸumuzda, aÅŸaÄŸÄ±daki gibi gÃ¶rÃ¼necektir.

![](images/32.png)

En az iki olmak Ã¼zere birden fazla Master'a sahip olacaÄŸÄ±z ve birden fazla worker node olacak. Worker dÃ¼ÄŸÃ¼mlerinin kendi ayrÄ± sorumluluklarÄ± vardÄ±r. Diyagramda gÃ¶rdÃ¼ÄŸÃ¼mÃ¼z gibi, her biri bir node'u temsil eden gerÃ§ek ayrÄ± sanal veya fiziksel makinelerimiz olur.

Åimdi, yerel ortamÄ±mÄ±zda bir ÅŸey test etmek istiyorsak veya yeni bir uygulama, yeni bileÅŸenler daÄŸÄ±tarak Ã§ok hÄ±zlÄ± bir ÅŸekilde bir ÅŸey denemek istiyorsak ve bunlarÄ± yerel makinemizde test etmek istiyorsak; aÃ§Ä±kÃ§asÄ± bÃ¶yle bir cluster kurmak oldukÃ§a zor olacaktÄ±r. Bellek ve CPU gibi yeterli kaynaÄŸÄ±mÄ±z yoksa imkansÄ±z bile olabilir. Ä°ÅŸte tam olarak bu kullanÄ±m durumu iÃ§in `Minikube` adÄ± verilen aÃ§Ä±k kaynaklÄ± araÃ§ var.

Minikube'un ne olduÄŸuna gelirsek, temelde hem master processleri hem de worker processleri tek bir node'da Ã§alÄ±ÅŸtÄ±ran tek node bir clusterdÄ±r. Bu node'da Ã¶nceden yÃ¼klenmiÅŸ bir Docker container runtime olacak ÅŸekilde konteynerleri veya konteynerli pod'larÄ± Ã§alÄ±ÅŸtÄ±rabileceÄŸiz.

![](images/33.png)

DizÃ¼stÃ¼ bilgisayarÄ±mÄ±zda VirtualBox, KVM veya baÅŸka bir hipervizÃ¶r aracÄ±lÄ±ÄŸÄ±yla Ã§alÄ±ÅŸacak. Yani temel olarak, Minikube dizÃ¼stÃ¼ bilgisayarlarÄ±mÄ±zda bir sanal makine oluÅŸturacak ve burada gÃ¶rdÃ¼ÄŸÃ¼mÃ¼z node'lar bu sanal makinede Ã§alÄ±ÅŸacak.

Ã–zetleyecek olursak, Minikube, yerel kurulumumuzda Kubernetes'i test etmek iÃ§in kullanabileceÄŸimiz dizÃ¼stÃ¼ bilgisayarÄ±nÄ±zda bir sanallaÅŸtÄ±rma aracÄ±yla Ã§alÄ±ÅŸan tek node bir Kubernetes clusterdÄ±r.

![](images/34.png)

Yerel makinemizde bir cluster veya mini cluster kurduktan sonra, bu cluster ile etkileÅŸim kurmak iÃ§in bir yola ihtiyacÄ±mÄ±z olacaktÄ±r. Componentler oluÅŸturmak, yapÄ±landÄ±rmak vb. isteyeceÄŸiz. Tam burada `kubectl` devreye giriyor.

### Kubectl

local makinemizde Minikube'u temsil eden bu virtual node'a sahip olduktan sonra, bu cluster ile etkileÅŸim kurmak iÃ§in bir yola ihtiyacÄ±mÄ±z vardÄ±r. Bunu Kubernetes clusterlarÄ± iÃ§in bir command line toolu olan `kubectl` kullanarak yapabiliriz.

NasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶relim. Minikube'un hem master hem de worker processleri Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nÄ± sÃ¶ylemiÅŸtik, bu nedenle API server adÄ± verilen master processlerden biri aslÄ±nda Kubernetes clusterÄ±n entry point noktasÄ±dÄ±r.

![](images/35.png)

Kubernetes'te bir ÅŸey yapmak istiyorsak veya herhangi bir ÅŸeyi yapÄ±landÄ±rmak istiyorsak, Ã¶nce API server ile konuÅŸmamÄ±z gerekir. API server ile konuÅŸmanÄ±n yolu ise farklÄ± istemciler aracÄ±lÄ±ÄŸÄ±yla olur. Bir dashboard gibi bir UI arayÃ¼zÃ¼nÃ¼z olabilir, Kubernetes API'sini kullanarak konuÅŸabilir veya `kubectl` command line tool kullanabilirsiniz.

![](images/36.png)

`kubectl` aslÄ±nda Ã¼Ã§ istemcinin de en gÃ¼Ã§lÃ¼sÃ¼dÃ¼r Ã§Ã¼nkÃ¼ `kubectl` ile Kubernetes'te istediÄŸimiz her ÅŸeyi yapabiliriz.

Bu yazÄ±nÄ±n neredeyse sonuna kadar `kubectl` kullanÄ±lmaktadÄ±r. `kubectl` API sunucusuna component oluÅŸturmak, component silmek vb. iÃ§in komutlar gÃ¶nderdikten sonra, Minikube node'undaki worker processler bunlarÄ± gerÃ§ekleÅŸtirecektir. Pod'lar oluÅŸturmak, pod'larÄ± yok etmek, services oluÅŸturmak vb. iÃ§in komutlarÄ± yÃ¼rÃ¼teceklerdir.

Bu, Minikube Ã§alÄ±ÅŸma ÅŸeklidir. `kubectl` cluster ile nasÄ±l kullanÄ±lÄ±r? Burada Ã¶nemli bir nokta, `kubectl`'in yalnÄ±zca Minikube cluster iÃ§in olmadÄ±ÄŸÄ±dÄ±r. Bir cloud cluster'Ä±mÄ±z veya hibrit bir cluster'Ä±mÄ±z varsa, `kubectl` herhangi bir Kubernetes cluster kurulumuyla etkileÅŸim kurmak iÃ§in kullanÄ±lan araÃ§tÄ±r. Bu nedenle burada unutulmamasÄ± Ã¶nemlidir.

![](images/37.png)

ArtÄ±k Minikube ve `kubectl`'nin ne olduÄŸunu bildiÄŸimize gÃ¶re, onlarÄ± pratikte gÃ¶rmek iÃ§in kuruluma baÅŸlayalÄ±m.

### Kurulum
Daha Ã¶nce de belirttiÄŸimiz gibi Minikube bir sanallaÅŸtÄ±rmaya ihtiyaÃ§ duyar, Ã§Ã¼nkÃ¼ bazÄ± hipervizÃ¶rlerde Ã§alÄ±ÅŸacaktÄ±r. Bu nedenle bir tÃ¼r hipervizÃ¶r yÃ¼klemelisiniz.


Åimdi linux Ã¼zerinde minikube ve kubectl kurulumuna geÃ§elim.

```shell
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube && rm minikube-linux-amd64
```

Åimdi her ÅŸeyin kurulduÄŸundan emin olalÄ±m ve komutlarÄ± kontrol edelim. Yani, `minikube` komutu Ã§alÄ±ÅŸmalÄ±:

```
c3ng0@ubn:~$ minikube start

ğŸ˜„  minikube v1.33.1 on Ubuntu 22.04
âœ¨  Automatically selected the docker driver. Other choices: kvm2, qemu2, none, ssh
ğŸ“Œ  Using Docker driver with root privileges
ğŸ‘  Starting "minikube" primary control-plane node in "minikube" cluster
ğŸšœ  Pulling base image v0.0.44 ...
ğŸ’¾  Downloading Kubernetes v1.30.0 preload ...
    > preloaded-images-k8s-v18-v1...:  112.62 MiB / 342.90 MiB  32.84% 5.18 MiB
    > gcr.io/k8s-minikube/kicbase...:  70.41 MiB / 481.58 MiB  14.62% 2.65 MiB
    > index.docker.io/kicbase/sta...:  481.58 MiB / 481.58 MiB  100.00% 11.15 M
â—  minikube was unable to download gcr.io/k8s-minikube/kicbase:v0.0.44, but successfully downloaded docker.io/kicbase/stable:v0.0.44 as a fallback image
ğŸ”¥  Creating docker container (CPUs=2, Memory=2200MB) ...
    > kubectl.sha256:  64 B / 64 B [-------------------------] 100.00% ? p/s 0s
    > kubeadm.sha256:  64 B / 64 B [-------------------------] 100.00% ? p/s 0s
    > kubelet.sha256:  64 B / 64 B [-------------------------] 100.00% ? p/s 0s
    > kubectl:  49.07 MiB / 49.07 MiB [------------] 100.00% 14.58 MiB p/s 3.6s
    > kubeadm:  47.92 MiB / 47.92 MiB [--------------] 100.00% 3.97 MiB p/s 12s
    > kubelet:  95.46 MiB / 95.46 MiB [--------------] 100.00% 6.52 MiB p/s 15s

    â–ª Generating certificates and keys ...
    â–ª Booting up control plane ...
    â–ª Configuring RBAC rules ...
ğŸ”—  Configuring bridge CNI (Container Networking Interface) ...
ğŸ”  Verifying Kubernetes components...
    â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
ğŸŒŸ  Enabled addons: storage-provisioner, default-storageclass
ğŸ’¡  kubectl not found. If you need it, try: 'minikube kubectl -- get pods -A'
ğŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default
```

Ve `kubectl` indirmemiz gerekiyor:

```bash
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
```

Binary'i doÄŸrula(opsiyonel)

kubectl checksumfile indir:

```bash
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"   
```

Ä°ndirilen checksum file ile binary doÄŸrulama:

```bash
echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check
```

   DoÄŸru ise Ã§Ä±ktÄ± aÅŸaÄŸÄ±daki gibi olmalÄ±:

`kubectl: OK`

- Kubectl Kurulumu

```bash
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
```

- Son sÃ¼rÃ¼mÃ¼ kurduÄŸumuzu kontrol etme:
```bash
kubectl version --client
```

![](images/38.png)

daha fazlasÄ± iÃ§in [kubernetes.io](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/)

Minikube oldukÃ§a basit bir komut satÄ±rÄ± aracÄ± ile birlikte gelir. Tek bir komutla tÃ¼m Kubernetes kÃ¼mesini bu tek dÃ¼ÄŸÃ¼m kurulumunda hÄ±zlÄ±ca baÅŸlatabilir, durdurabilir veya silebiliriz.


## Ä°lk Cluster

Åimdi her ikisini de kurduÄŸumuza gÃ¶re, bir Minikube Kubernetes cluster oluÅŸturalÄ±m.

```bash
minikube start
```

iÅŸte Minikube ile bir Kubernetes cluster nasÄ±l baÅŸlatacaÄŸÄ±mÄ±z:

```bash
minikube start --vm-driver=kvm
```

Burada, kurulu hipervizÃ¶rÃ¼n devreye girdiÄŸini gÃ¶rebilirsiniz Ã§Ã¼nkÃ¼ Minikube'un bir Sanal ortamda Ã§alÄ±ÅŸmasÄ± gerektiÄŸinden, Minikube'a hangi hipervizÃ¶rÃ¼ kullanmasÄ± gerektiÄŸini sÃ¶yleyeceÄŸiz. Bunun iÃ§in, `--vm-driver` olarak adlandÄ±rÄ±lan bir seÃ§enek belirleyeceÄŸiz ve burada bende kurulu olan `kvm`'i ayarladÄ±m.

Bunu yÃ¼rÃ¼ttÃ¼ÄŸÃ¼mde bir ÅŸeyler indirecek, yani ilk kez yapÄ±yorsak biraz daha uzun sÃ¼rebilir.

![](images/39.png)

Ve bahsettiÄŸim gibi, makinenizde Docker yoksa bile Ã§alÄ±ÅŸacak.

TamamlandÄ±. ArtÄ±k `kubectl`, Minikube'u kullanacak ÅŸekilde yapÄ±landÄ±rÄ±lmÄ±ÅŸ durumda, bu da Minikube cluster'Ä±nÄ±n kurulduÄŸu anlamÄ±na gelir.

![](images/40.png)

Kubernetes KÃ¼mesi ile etkileÅŸimde bulunmak iÃ§in tasarlanmÄ±ÅŸ olan `kubectl` komutu da o Minikube kÃ¼mesi ile baÄŸlantÄ±lÄ±dÄ±r, eÄŸer ÅŸunu yaparsak:

```bash
kubectl get nodes
```

Bu, Kubernetes cluster node'larÄ±nÄ±n durumunu bize bildirir. Bize bir Minikube node'unun hazÄ±r olduÄŸunu sÃ¶yleyecek ve gÃ¶rdÃ¼ÄŸÃ¼mÃ¼z gibi aÃ§Ä±kÃ§a master processleri Ã§alÄ±ÅŸtÄ±rmalÄ± Ã§Ã¼nkÃ¼ sadece bir node var.

![](images/41.png)

Ve ayrÄ±ca Minikube'Ä±n durumunu alabiliriz:

```bash
minikube status
```

![](images/42.png)

Yani, ana makinede kubelet adlÄ± bir hizmetin Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶rÃ¼yoruz, bu da aslÄ±nda konteyner runtime kullanarak pod'larÄ± Ã§alÄ±ÅŸtÄ±ran bir hizmettir, yani her ÅŸey Ã§alÄ±ÅŸÄ±yor.

Buradan itibaren mini Kub kÃ¼mesi ile `kubectl` komut satÄ±rÄ± aracÄ±lÄ±ÄŸÄ±yla etkileÅŸime geÃ§eceÄŸiz. Minicube sadece cluster baÅŸlatma ve silme iÃ§in kullanÄ±lÄ±r, ancak configuring ve diÄŸer her ÅŸeyi `kubectl` aracÄ±lÄ±ÄŸÄ±yla yapacaÄŸÄ±z.

---

- **The label applied to control-plane nodes "node-role.kubernetes.io/master" is now deprecated and will be removed in a future release after a GA deprecation period.**
- **Introduce a new label "node-role.kubernetes.io/control-plane" that will be applied in parallel to "node-role.kubernetes.io/master" until the removal of the "node-role.kubernetes.io/master" label.**


## Main Kubectl KomutlarÄ±

Bu bÃ¶lÃ¼mde bazÄ± temel Kubectl komutlarÄ±nÄ± gÃ¶receÄŸiz ve minikube'da nasÄ±l create ve debug pods yapÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶receÄŸiz.

Cubectl'i clusterda herhangi bir ÅŸey yapmak iÃ§in kullanacaÄŸÄ±z. -components oluÅŸturmak, status almak, vb.-

* Ä°lk olarak, node'larÄ±n durumunu alacaÄŸÄ±z.

```bash
kubectl get nodes
```

Bu komutu kullanarak node'larÄ±n durumunu Ã¶ÄŸrenebiliyoruz.

```bash
c3ng0@ubn:~$ kubectl get nodes
NAME       STATUS   ROLES           AGE    VERSION
minikube   Ready    control-plane   145m   v1.30.0
```

GÃ¶rÃ¼yoruz ki bir node var ve her ÅŸey o node'da Ã§alÄ±ÅŸÄ±yor Ã§Ã¼nkÃ¼ bu bir `minikube`.

* Pod'larÄ± kontrol edebiliriz ve herhangi bir pod'umuz olmadÄ±ÄŸÄ± iÃ§in sonuÃ§ yok.

```bash
kubectl get pod
```

* Services kontrol edebiliriz, varsayÄ±lan bir service'imiz var.

```bash
kubectl get services
```

```bash
c3ng0@ubn:~$ kubectl get services
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   148m
```

Åimdi, herhangi bir Pod'umuz olmadÄ±ÄŸÄ± iÃ§in bir tane oluÅŸturacaÄŸÄ±z ve Kubernetes bileÅŸenleri oluÅŸturmanÄ±n bir Kubectl create komutu olduÄŸunu hatÄ±rlayalÄ±m. Kubectl create komutunu kullanarak tÃ¼m bu bileÅŸenleri oluÅŸturabiliriz.

* Ancak listede `Pod` yok Ã§Ã¼nkÃ¼ Kubernetes dÃ¼nyasÄ±nda, Pod, Kubernetes clusterÄ±nÄ±n en kÃ¼Ã§Ã¼k birimidir ve genellikle, Pod'larÄ± doÄŸrudan oluÅŸturulmaz. Veya Pod'larla doÄŸrudan Ã§alÄ±ÅŸÄ±lmaz. Pod'larÄ±n Ã¼zerinde bir soyutlama katmanÄ±`(abstraction over Pods)` vardÄ±r, buna Â·`deployment`Â· denir. Ä°ÅŸte bu oluÅŸturmak Ã¼zere olduÄŸumuz ÅŸey. Bu da, altÄ±ndaki parÃ§alarÄ± oluÅŸturacak.

```bash
Usage:
   kubectl create deployment NAME --image=image -- [COMMAND] [args...] [options]
```

* **NAME**: deployment'a isim vermeliyiz
* **--image=**: oluÅŸturacaÄŸÄ±mÄ±z container image'i

Åimdi bir nginx daÄŸÄ±tÄ±mÄ± oluÅŸturalÄ±m.

```bash
c3ng0@ubn:~$ kubectl create deployment nginx-depl --image=nginx
deployment.apps/nginx-depl created
```

 * Nginx gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼, Docker Hub'dan indirecektir. Bu komutu yÃ¼rÃ¼tÃ¼rsem, gÃ¶rdÃ¼ÄŸÃ¼nÃ¼z gibi nginx deployment oluÅŸturuldu.

```bash
c3ng0@ubn:~$ kubevtl get deployment
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
nginx-depl   0/1     1            0           15s
```

 * OluÅŸturulmuÅŸ bir deployment olduÄŸunu gÃ¶rÃ¼yoruz ve burada "hazÄ±r deÄŸil" durumunda olduÄŸunu da gÃ¶rebiliyoruz.

```bash
c3ng0@ubn:~$ kubectl get pod
NAME                          READY   STATUS    RESTARTS   AGE
nginx-depl-85c9d7c5f4-g4lwt   0/1     Creating  0          31s
```

 * ArtÄ±k bir Pod'umuz var. Name deÄŸeri, prefix ve rastgele bir hash'e sahipt. Burada "konteyner oluÅŸturuluyor" yazÄ±yor, yani henÃ¼z hazÄ±r deÄŸil. Bir sÃ¼re beklersek `Running`.

 * Bir deployment oluÅŸturduÄŸumuzda, deployment, Pod oluÅŸturmak iÃ§in gereken tÃ¼m bilgilere veya blueprintlere sahip olur.

![](images/43.png)

 * Bu en temel yapÄ±landÄ±rmadÄ±r, sadece adÄ± ve gÃ¶rÃ¼ntÃ¼sÃ¼. Bu kadar.. geri kalanÄ± default.

 * Deployment ve Pod arasÄ±nda bir baÅŸka katman vardÄ±r ve bu, otomatik olarak kubernetes tarafÄ±ndan yÃ¶netilen `replicaset`'tir.
 * `kubectl get replicaset` yaparsak, bir nginx replica set hash'imiz olduÄŸunu gÃ¶rÃ¼yoruz. Ve burada, Pod adÄ±nÄ±n bir deployment prefix, replicaset'in ID'si ve son olarak kendi ID'si olduÄŸunu gÃ¶rebiliriz. Pod adÄ± bu ÅŸekilde oluÅŸmaktadÄ±r. Replicaset, Pod'un tÃ¼m replikalarÄ±nÄ± yÃ¶netir. Biz hiÃ§bir zaman replica set oluÅŸturmayacak, silmeyecek veya gÃ¼ncellemeyeceÄŸiz. DoÄŸrudan deploymentlar ile Ã§alÄ±ÅŸacaÄŸÄ±z. Bu daha uygun Ã§Ã¼nkÃ¼ deploymentlarda Pod blueprintini tamamen yapÄ±landÄ±rabiliriz. Pod'un kaÃ§ replikasÄ±na ihtiyacÄ±mÄ±z olduÄŸunu belirtebilir ve geri kalan configuration'u orada yapabiliriz.

### LayerlarÄ±n Ã§alÄ±ÅŸma ÅŸekli:

 * Ä°lk olarak Deployment, ReplicaSet'i yÃ¶netir.
 * ReplicaSet, o Pod'un tÃ¼m replikalarÄ±nÄ± yÃ¶netir.
 * Pod, bir konteynerin soyutlamasÄ±dÄ±r.
 Deployment'tan aÅŸaÄŸÄ±daki her ÅŸey otomatik olarak kubernetes tarafÄ±ndan yÃ¶netilmelidir.

Ã–rneÄŸin, kullanÄ±ldÄ±ÄŸÄ± image gibi bir ÅŸeyi doÄŸrudan bir deployment iÃ§erisinde dÃ¼zenlemem gerekecek, Pod iÃ§inde deÄŸil. Ã–yleyse hemen yapalÄ±m.

```python
c3ng0@ubn:~$ kubectl edit deployment nginx-depl
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  creationTimestamp: "2024-05-14T11:31:36Z"
  generation: 1
  labels:
    app: nginx-depl
  name: nginx-depl
  namespace: default
  resourceVersion: "9431"
  uid: 66d185d6-b628-4d10-b3bc-4aea093dfc59
spec:
  progressDeadlineSeconds: 600
  replicas: 1
...
...
...
```


Deployment oluÅŸtururken verdiÄŸimiz iki seÃ§enek dÄ±ÅŸÄ±nda her ÅŸeyin otomatik olarak oluÅŸturulmuÅŸ bir deployment, otomatik olarak oluÅŸturulmuÅŸ bir yapÄ±landÄ±rma dosyasÄ±nÄ± alÄ±yoruz.
Åimdilik sadece resmi aÃ§Ä±p istediÄŸim versiyonu 1.16'ya sabitlemek istediÄŸimi varsayalÄ±m ve bu deÄŸiÅŸikliÄŸi kaydedelim.

```
    spec:
      containers:
-       - image: nginx
---
    spec:
      containers:
+       - image: nginx:1.16

```

Ve gÃ¶rdÃ¼ÄŸÃ¼nÃ¼z gibi daÄŸÄ±tÄ±m dÃ¼zenlendi.

![](images/44.png)

Åimdi `kubectl get pod` yaparsak, eski pod'umuzu gÃ¶rÃ¼rÃ¼z.

![](images/45.png)

* Eski Pod sona erdi ve yeni pod baÅŸladÄ±.

EÄŸer ReplicaSet'i gÃ¶rÃ¼ntÃ¼lersek, eski olanÄ±n iÃ§inde pod olmadÄ±ÄŸÄ±nÄ± ve yeni bir tane oluÅŸturulduÄŸunu gÃ¶rÃ¼yoruz.

![](images/46.png)

Yani sonuÃ§ olarak deployment yapÄ±landÄ±rmasÄ±nÄ± dÃ¼zenledik ve altÄ±ndaki her ÅŸey otomatik olarak gÃ¼ncellendi. Bu yaptÄ±ÄŸÄ±mÄ±z, Kubernetes'in sihrine ve nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±na bir Ã¶rnektir.

---

### Debugging Pods

Bir diÄŸer Ã§ok pratik komut ise `kubectl logs`, bu aslÄ±nda Pod iÃ§inde Ã§alÄ±ÅŸan uygulamanÄ±n neyi kaydettiÄŸini gÃ¶sterir.

```bash
kubectl logs [POD_NAME]
```

LoglarÄ± gÃ¶rÃ¼ntÃ¼lemeden Ã¶nce Nginx hiÃ§bir ÅŸey kaydetmediÄŸi iÃ§in baÅŸka bir daÄŸÄ±tÄ±m oluÅŸturalÄ±m. Mongodb'den oluÅŸturalÄ±m ve adÄ±na `mongo-depl` verelim.

![](images/47.png)

Åimdi mongodb deployment oluÅŸturuluyor.

![](images/48.png)

Åu anda loglara bakabiliriz:

![](images/49.png)

* `kubectl describe pod [POD_NAME]` events sekmesinde bize state deÄŸiÅŸikliklerini verir.

![](images/50.png)

Loglamak, uygulamanÄ±n gerÃ§ekte neyi yazdÄ±ÄŸÄ±nÄ± gÃ¶rmede ve hata ayÄ±klamada yardÄ±mcÄ± olmaktadÄ±r.

BaÅŸka bir Ã§ok kullanÄ±ÅŸlÄ± komut, `kubectl exec`tir. Debugging yaparken, bir ÅŸey Ã§alÄ±ÅŸmÄ±yorsa veya sadece Pod'un iÃ§eriÄŸini kontrol etmek iÃ§in kullanÄ±lÄ±r.
BasitÃ§e aÃ§Ä±klayacak olursak, Ã§alÄ±ÅŸan Pod'dan shell alÄ±r. bu yÃ¼zden:
```
kubectl exec -it [POD_NAME] -- bin/bash
```
* -it = **interactive terminal**

![](images/51.png)

Bu komutla mongodb uygulama konteynerinin terminalini alÄ±yoruz ve ÅŸu anda root kullanÄ±cÄ±sÄ± olarak mongodb konteynerinin iÃ§indeyiz.
Exec, hata ayÄ±klama veya bir ÅŸeyleri test etmek veya denemek istediÄŸinizde kullanÄ±ÅŸlÄ±dÄ±r. Konteynera girebilir veya terminali alabilir ve orada bazÄ± komutlar Ã§alÄ±ÅŸtÄ±rabilirsiniz.

---

### Deployment Silme ve Apply Configuration File

Tabii ki kubectl ile podlarÄ± silebiliriz, Ã¶nce deployment'larÄ± ve podlarÄ± gÃ¶rÃ¼ntÃ¼leyelim.

![](images/52.png)

```
kubectl delete deployment [deployment_name]
```

![](images/53.png)

kontrol ederseniz Pod'un sonlandÄ±ÄŸÄ±nÄ± ve eÄŸer replica set alÄ±rsanÄ±z, mongodb replicasetinin de gittiÄŸini gÃ¶rebilirsiniz.

TÃ¼m crud iÅŸlemleri (create,update,delete vb.) deployment seviyesinde gerÃ§ekleÅŸir ve altÄ±ndaki her ÅŸey otomatik olarak takip eder. AynÄ± ÅŸekilde Services gibi diÄŸer Kubernetes kaynaklarÄ± oluÅŸturabiliriz.

Ancak fark ettiÄŸiniz gibi, Kubectl ile deployment gibi kubernetes bileÅŸenlerini oluÅŸtururken, tÃ¼m bu seÃ§enekleri komut satÄ±rÄ±nda belirtmemiz gerekir.

* AdÄ± belirtmemiz gerekir.
* Image'i belirtmemiz gerekir
* option1
* option2..  vb. olabilir.

Elbette bir deployment'ta veya bir Pod'ta yapÄ±landÄ±rmak istediÄŸimiz birÃ§ok ÅŸey olabilir ve aÃ§Ä±kÃ§asÄ± bunlarÄ±n hepsini komut satÄ±rÄ±nda yazmak pratik olmayacaktÄ±r. Bunun iÃ§in genellikle Kubernetes yapÄ±landÄ±rma dosyalarÄ±yla Ã§alÄ±ÅŸÄ±lmaktadÄ±r. Yani oluÅŸturduÄŸumuz bileÅŸenin tÃ¼rÃ¼, adÄ±, image'i ve diÄŸer tÃ¼m seÃ§enekleri bir yapÄ±landÄ±rma dosyasÄ±nda toplanÄ±r. Sadece cubectl'e bu yapÄ±landÄ±rma dosyasÄ±nÄ± yÃ¼rÃ¼tmesini sÃ¶yleriz.

Bunu yapmanÄ±n yolu `kubectl apply` komutunu kullanmaktÄ±r.

### kubectl apply

Apply, temelde dosyayÄ±, yapÄ±landÄ±rma dosyasÄ±nÄ± bir parametre olarak alÄ±r ve orada ne yazdÄ±ysak yapar.

```bash
kubectl apply -f [file_name]
```

Apply, "-f" iÃ§in bir seÃ§enek alÄ±r ve bu dosyanÄ±n adÄ±nÄ± belirtir ve genellikle bu dosyalar iÃ§in kullanÄ±lan biÃ§im YAML'dir ve bu, dosyadaki her ÅŸeyi yÃ¼rÃ¼ten komuttur. Bu yÃ¼zden aslÄ±nda bunu yapÄ±landÄ±rma dosyasÄ± olarak adlandÄ±racaÄŸÄ±z.

Ã–rnek olarak Ã§ok basit, temel bir  `nginx-deployment.yaml` deployment dosyasÄ± oluÅŸturalÄ±m.

Deployment iÃ§in temel yapÄ±landÄ±rma:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.16
        ports:
        - containerPort: 80
```

* Åu an iÃ§in gerekli olan satÄ±rlarÄ±, aÅŸaÄŸÄ±da inceleyelim.
```yml
kind: Deployment
## Ne oluÅŸturmak istediÄŸimizi belirtiyoruz, Deployment oluÅŸturmak istiyoruz.
```

```yml
name: nginx-deployment
## OluÅŸturacaÄŸÄ±mÄ±z Deployment ismi.
```

```yml
spec:            ## specification for deployment
  replicas: 1    ## Pod'lardan oluÅŸturulacak replika sayÄ±sÄ±
```

```yml
  template:
    metadata:
      labels:
        app: nginx
    spec:    ## specification for pods
      containers:
      - name: nginx
        image: nginx:1.16   ## Konteyner image'imiz iÃ§in nginx versiyonu
        ports:
        - containerPort: 80 ## Binding Port
## Bu alan olutÅŸturacaÄŸÄ±mÄ±z deployment'a ait blueprint
```

Bu, bizim config  dosyamÄ±z ve buna bir kere sahip olduktan sonra, bu yapÄ±landÄ±rmayÄ± istediÄŸimiz zaman uygulayabiliriz.

![](images/54.png)

Deployment oluÅŸturuldu, ÅŸimdi podu gÃ¶rÃ¼ntÃ¼lersek, nginx daÄŸÄ±tÄ±mÄ± podu oluÅŸturuldu ve Ã§alÄ±ÅŸÄ±yor olduÄŸunu gÃ¶rÃ¼rÃ¼z.

![](images/55.png)

AyrÄ±ca daÄŸÄ±tÄ±mÄ±n 3 dakika 57 saniye Ã¶nce oluÅŸturulduÄŸunu gÃ¶rÃ¼yoruz. EÄŸer bu deployment'ta bir ÅŸeyleri deÄŸiÅŸtirmek istersek, sadece yerel yapÄ±landÄ±rmamÄ±zÄ± deÄŸiÅŸtirmemiz yeterlidir. Ã–rneÄŸin, bir yerine iki replika istersek, sadece dosyayÄ± dÃ¼zenleyip tekrar apply ederiz. Ve deployment, nginx daÄŸÄ±tÄ±mÄ± olarak tekrar yapÄ±landÄ±rÄ±lacaktÄ±r.

![](images/56.png)

![](images/57.png)

Fark ettiyseniz Ã§Ä±ktÄ±da bize "configured" dendi. Fark ÅŸu ki, Kubernetes, nginx deployment'Ä±nÄ±n var olmadÄ±ÄŸÄ±nÄ± algÄ±larsa, yeni bir tane oluÅŸturacak, ancak eÄŸer deployment zaten varsa, yapÄ±landÄ±rma dosyasÄ±nÄ± apply ettiÄŸimizde, onu gÃ¼ncellemesi gerektiÄŸini bilecek ve yeni bir deployment oluÅŸturmak yerine eski deployment'Ä± cofigure edecek.

![](images/58.png)

Eski deployment hala ayakta (9m45s) fakat yeni bir replika oluÅŸturuldu(3m22s) Ã§Ã¼nkÃ¼ replika sayÄ±sÄ±nÄ± arttÄ±rdÄ±k. yani `kubectl apply` ile bir component oluÅŸturabilir ve gÃ¼ncelleyebiliriz. Elbette Services, Volumes gibi diÄŸer kubernetes bileÅŸenlerine de ayar Ã§ekebiliriz.

---

Ã–zetlemek gerekirse, bu yazÄ±da birkaÃ§ kubectl komutuna baktÄ±k, bir component oluÅŸturmayÄ±, nasÄ±l configure edeceÄŸimizi ve sileceÄŸimizi gÃ¶rdÃ¼k. Pod'larÄ±n, deployment'larÄ±n, replikaset'lerinin vb. state'lerini nasÄ±l alacaÄŸÄ±mÄ±zÄ± gÃ¶rdÃ¼k. AyrÄ±ca Pod'un iÃ§indeki uygulamanÄ±n konsola yazdÄ±ÄŸÄ± her ÅŸeyi nasÄ±l kaydedeceÄŸimizi gÃ¶rdÃ¼k ve `kubectl exec`'i kullanarak Ã§alÄ±ÅŸan bir konteynerdan nasÄ±l shell alacaÄŸÄ±mÄ±zÄ± gÃ¶rdÃ¼k. Son olarak, kubernetes yapÄ±landÄ±rma dosyasÄ±nÄ± ve `kubectl apply` komutunu kullanarak componentleri nasÄ±l oluÅŸturup gÃ¼ncelleyeceÄŸimizi gÃ¶rdÃ¼k.
Son olarak azÄ±cÄ±k da `kubectl describe` komutunu gÃ¶rdÃ¼k, bu da bir konteynerin bir Pod'da sorun giderme iÃ§in ek bilgi almak istediÄŸinizde kullandÄ±ÄŸÄ±nÄ±z bir komuttu.

#### Crud Commands:
* Create deployment                  ->        `kubectl create deployment [name]`
* Edit deployment                    ->        `kubectl edit deployment [name]`
* Delete deployment                  ->        `kubectl delete deployment [name]`

#### Status of different K8s components
* `kubectl get nodes | pod | services | replicaset | deployment`

#### Debugging pods
* Log to console                     ->        `kubectl logs [pod_name]`
* Get interactive Terminal           ->        `kubectl exec -it [pod_name] -- /bin/bash`
* Get info about pod                 ->        `kubectl describe pod [pod_name]`  

#### Use configuration file for CRUD
* Apply a configuration file         ->        `kubectl apply -f [file_name]`
* Delete with configuration file     ->        `kubectl delete -f [file_name]`


---
