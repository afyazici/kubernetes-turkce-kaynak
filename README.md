# Kubernetes TÃ¼rkÃ§e DÃ¶kÃ¼man

---

[![Kubernetes](images/header.png)](https://www.kubernetes.io)

---

![Kubernetes](https://img.shields.io/badge/kubernetes-%23326ce5.svg?style=for-the-badge&logo=kubernetes&logoColor=white) ![Markdown](https://img.shields.io/badge/markdown-%23000000.svg?style=for-the-badge&logo=markdown&logoColor=white)

* TechWorld Nina kanalÄ±na ait olan 4 saatlik "Kubernetes Tutorial for Beginners [FULL COURSE in 4 Hours]" videosunu dÃ¶kÃ¼man formatÄ±nda TÃ¼rkÃ§eye Ã§eviriyorum. Ä°ngilizce konusunda sÄ±kÄ±ntÄ± yaÅŸayanlar iÃ§in faydalÄ± bir *full tutorial* olacaÄŸÄ±na inanÄ±yorum.

* **Video linki aÅŸaÄŸÄ±da yer almaktadÄ±r**.
[![TeÅŸekkÃ¼rler](images/nina.jpg)](https://www.youtube.com/watch?v=X48VuDVv0do&t=2949s)



## Ä°Ã§indekiler


- [1- Kubernetes Nedir?](#kubernetes-nedir)
- [2- Kubernetes AvantajlarÄ±?](#kubernetes-avantajlarÄ±)
- [3- Kubernetes Componentleri?](#kubernetes-componentleri)
  - [Pod](#pod)
  - [Service ve Ingress](#service-ve-ingress)
  - [ConfigMap ve Secret](#configmap-ve-secret)
  - [Volumes](#volumes)
  - [Deployment ve StatefulSet](#deployment-ve-statefulset)
- [4- Kubernetes Mimarisi](#kubernetes-mimarisi)
  - [Node Process](#node-process)
    - [1) Container Runtime](#1-container-runtime)
    - [2) Kubelet](#2-kubelet)
    - [3) Kube Proxy](#3-kube-proxy)
  - [Master Node](#master-node)
    - [1) API Server](#1-api-server)
    - [2) Scheduler](#2-scheduler)
    - [3) Controller Manager](#3-controller-manager)
    - [4) Etcd](#4-etcd)
- [5- Cluster YapÄ±sÄ±](#cluster-yapÄ±sÄ±)
- [6- Minikube ve Kubectl Kurulumu](#minikube-ve-kubectl-kurulumu)
  - [Minikube](#minikube)
  - [Kubectl](#kubectl)
  - [Kurulum](#kurulum)
- [7- Ä°lk Cluster](#Ä°lk-cluster)
- [8- Main Kubectl KomutlarÄ±](#main-kubectl-komutlarÄ±)
  - [LayerlarÄ±n Ã‡alÄ±ÅŸma Åekli](#layerlarÄ±n-Ã§alÄ±ÅŸma-ÅŸekli)
  - [Debugging Pods](#debugging-pods)
  - [Deployment Silme ve Apply Configuration File](#deployment-silme-ve-apply-configuration-file)
  - [Kubectl Apply](#kubectl-apply)
    - [KomutlarÄ± HatÄ±rlayalÄ±m](#komutlarÄ±-hatÄ±rlayalÄ±m)
- [9- Kubernetes YAML KonfigÃ¼rasyonu](#kubernetes-yaml-konfigÃ¼rasyonu)
  - [3 ParÃ§ada K8s Config DosyasÄ±](#3-parÃ§ada-k8s-config-dosyasÄ±)
  - [Config DosyasÄ±nÄ±n FormatÄ±](#config-dosyasÄ±nÄ±n-formatÄ±)
  - [Podlar iÃ§in Blueprint (Template)](#podlar-iÃ§in-blueprint-template)
  - [Connecting Components *Labels - Selectors - Ports*](#connecting-components-labels--selectors--ports)
- [10- Ä°lk Demo Uygulama](#Ä°lk-demo-uygulama)
  - [1) MongoDB Pod](#1-mongodb-pod)
  - [2) Secret ve Referans](#2-secret-ve-referans)
  - [3) MongoDB Internal Service](#3-mongodb-internal-service)
  - [4) Mongo Express Deployment & Service & ConfigMap](#4-mongo-express-deployment--service--configmap)
  - [5) Mongo Express External Service](#4-mongo-express-external-service)

---


## Kubernetes Nedir?
- **Kubernetes**, aÃ§Ä±k kaynaklÄ± bir konteyner yÃ¶netim aracÄ±dÄ±r.
- Google tarafÄ±ndan geliÅŸtirilmiÅŸtir.
- FarklÄ± daÄŸÄ±tÄ±m ortamlarÄ±nda konteynerleÅŸtirilmiÅŸ uygulamalarÄ± yÃ¶netmenize yardÄ±mcÄ± olur.
    - fiziksel ortamda
    - sanal ortamda
    - bulut ortamÄ±nda

---

## Kubernetes AvantajlarÄ±:

- **YÃ¼ksek eriÅŸilebilirlik** veya kesintisiz Ã§alÄ±ÅŸma
- **Ã–lÃ§eklenebilirlik** veya yÃ¼ksek performans
- **Afet kurtarma** - yedekleme ve geri yÃ¼kleme

---

## Kubernetes Componentleri
### Pod
* Pod Kubernetes'in en kÃ¼Ã§Ã¼k birimidir.
* Konteyner Ã¼zerinde sanallaÅŸtÄ±rma yapar. (abstraction)

![](images/1.png)


* Bizler yalnÄ±zca Kubernetes katmanÄ±yla etkileÅŸime geÃ§eriz.
* Bir pod iÃ§inde birden fazla konteyner Ã§alÄ±ÅŸtÄ±rabiliriz, ancak genellikle bir pod baÅŸÄ±na bir uygulama bulunur.
* Her pod kendi IP adresine sahiptir. Her pod, birbirleriyle bu internal IP adresini kullanarak iletiÅŸim kurabilir.

Ancak, Kubernetes'teki pod bileÅŸenleri de geÃ§icidir: yani Ã§ok kolay bir ÅŸekilde Ã¶lebilirler.
Ã–rneÄŸin, bir veritabanÄ± konteynerÄ±nÄ± kaybettiÄŸimizi dÃ¼ÅŸÃ¼nelim (konteyner iÃ§indeki uygulama Ã§Ã¶ktÃ¼ÄŸÃ¼ veya sunucu kaynaklarÄ± tÃ¼kendiÄŸi iÃ§in pod Ã¶lÃ¼r ve yerine yeni bir tane oluÅŸturulur). Bu durumda da yeni bir IP adresi atanÄ±r. VeritabanÄ±yla IP adresini kullanarak iletiÅŸim kuruyorsak elbette sakÄ±ncalÄ±dÄ±r. Her pod yeniden baÅŸladÄ±ÄŸÄ±nda, iletiÅŸimi her seferinde yeniden ayarlamamÄ±z gerekir. Bu nedenle, pod'un yeniden baÅŸladÄ±ÄŸÄ±nda IP adresini ayarlamamÄ±za gerek kalmadan, veritabanÄ±yla iletiÅŸim kurmanÄ±zÄ± saÄŸlayan baÅŸka bir Kubernetes bileÅŸeni olan `Service` kullanÄ±lÄ±r.

### Service ve Ingress

* Service, her pod'a baÄŸlanabilen sabit bir IP adresidir. UygulamamÄ±zÄ±n kendi servisi olacak ve veritabanÄ± pod'u kendi servisine sahip olacak. Buradaki gÃ¼zel ÅŸey, servis ve Pod'un yaÅŸam dÃ¶ngÃ¼leri birbirine baÄŸlÄ± deÄŸil. Bu yÃ¼zden Pod Ã¶lse bile servis ve bu servise ait IP adresi kalÄ±r. Bu ÅŸekilde endpoint'i deÄŸiÅŸtirmemize gerek yoktur.

![](images/2.png)

* Tabii ki, uygulamamÄ±zÄ±n bir tarayÄ±cÄ± aracÄ±lÄ±ÄŸÄ±yla eriÅŸilebilir olmasÄ±nÄ± isteriz deÄŸil mi? Bunun iÃ§in bir external service oluÅŸturmamÄ±z gerekiyor. `External Service`, dÄ±ÅŸ kaynaklardan iletiÅŸimi aÃ§an bir servistir. Ancak, veritabanÄ±mÄ±zÄ± halka aÃ§Ä±k isteklere aÃ§mak istemeyiz. Bunun iÃ§in `Internal Service` adÄ±nÄ± verdiÄŸimiz bir ÅŸey oluÅŸtururuz.

![](images/3.png)

* External Service URL'sinin Ã§ok pratik olmadÄ±ÄŸÄ±nÄ± fark ettik deÄŸil mi? Temelde, bir HTTP protokolÃ¼yle bir node IP adresi ve servis port numarasÄ±na sahibiz. Bu hÄ±zlÄ± bir ÅŸekilde bir ÅŸeyleri test etmek istiyorsak iyidir, ancak end-product iÃ§in iyi deÄŸildir. Genellikle, uygulamamÄ±zla gÃ¼venli bir protokol ve bir alan adÄ± kullanmak isteriz.

![](images/4.png)

Bunun iÃ§in Kubernetes'in baÅŸka bir bileÅŸeni olan `Ingress` var. Bu ÅŸekilde, istek Ã¶nce servise deÄŸil, Ingress'e gider ve oradan servise yÃ¶nlendirilir.

![](images/5.png)

### ConfigMap ve Secret

Pod'lar birbirleriyle servis aracÄ±lÄ±ÄŸÄ±yla iletiÅŸim kurar. UygulamamÄ±zÄ±n, veritabanÄ± ile iletiÅŸim kurmak iÃ§in kullandÄ±ÄŸÄ± bir database endpoint `Ã¶rneÄŸin mongodb servisi` olacak. Ancak bu veritabanÄ± URL'sini (ya da endpoint) genellikle nerede yapÄ±landÄ±rÄ±rÄ±z?

![](images/6.png)

Genellikle bunu application properties file veya bazÄ± external environmental variable olarak yaparÄ±z, ancak genellikle yapÄ±landÄ±rma, uygulamanÄ±n iÃ§erisindeki built image'tedir.

Ã–rneÄŸin, service endpoint (ya da service name) 'mongodb' olarak deÄŸiÅŸirse, uygulamadaki bu URL'i ayarlamalÄ±yÄ±z. Genellikle yeni bir sÃ¼rÃ¼mle uygulamayÄ± rebuild etmemiz ve repoya pushlamamÄ±z gerekir. ArdÄ±ndan bu yeni image'i pod'umuzda pull'layÄ±p tÃ¼m uygulamayÄ± yeniden baÅŸlatmamÄ±z gerekebilir.

![](images/7.png)

VeritabanÄ± URL'i gibi kÃ¼Ã§Ã¼k bir deÄŸiÅŸiklik iÃ§in bu gerÃ§ekten zahmetli. Bu sebeple, Kubernetes'in `configmap` adÄ±nda bir bileÅŸeni var. YapÄ±sÄ±, uygulamamÄ±za `external configuration` saÄŸlar. ConfigMap genellikle kullandÄ±ÄŸÄ±mÄ±z veritabanÄ± URL'leri gibi yapÄ±landÄ±rma verilerini iÃ§erir. Kubernetes'te bunu Pod'a baÄŸlarÄ±z. Pod, ConfigMap'in iÃ§erdiÄŸi verileri alÄ±r. Ve ÅŸimdi, servis adÄ±nÄ± deÄŸiÅŸtirirsek (service end point), sadece ConfigMap'i ayarlamamÄ±z yeterlidir. Yeni bir image oluÅŸturmamÄ±za ve tÃ¼m dÃ¶ngÃ¼yÃ¼ geÃ§irmemize gerek yoktur. BÃ¼yÃ¼k avantaj!

![](images/8.png)

Åimdi, external configuration'Ä±n bir parÃ§asÄ± aynÄ± zamanda database kullanÄ±cÄ± adÄ± ve ÅŸifresi olabilir deÄŸil mi? Bu veriler de uygulama daÄŸÄ±tÄ±m sÃ¼recinde deÄŸiÅŸebilir. Ancak, bir ÅŸifreyi veya diÄŸer kimlik bilgilerini dÃ¼z metin formatÄ±nda bir configmap'e koymamÄ±z gÃ¼vensiz olur.

>[!WARNING]
>![](images/9.png)

Bu amaÃ§la, Kubernetes'in `Secret` adÄ±nda bir baÅŸka bileÅŸeni daha var. Yani, Secret, ConfigMap gibi, ancak fark ÅŸu ki; ÅŸifre gibi gizli verileri saklamak iÃ§in kullanÄ±lÄ±r. Ve tabii ki, dÃ¼z metin formatÄ±nda deÄŸil, base64 formatÄ±nda kodlanmÄ±ÅŸ olarak saklanÄ±r. Yani, Secret, kullanÄ±cÄ± adlarÄ± gibi kimlik bilgilerini iÃ§erecek ve veritabanÄ± kullanÄ±cÄ±larÄ±nÄ± iÃ§erecektir. ConfigMap'e de koyabiliriz, ancak Ã¶nemli olan ÅŸifreler, sertifikalar, baÅŸkalarÄ±nÄ±n eriÅŸimini istemediÄŸimiz ÅŸeyler Secret'e koyulmalÄ±dÄ±r. AynÄ± ConfigMap gibi, sadece Pod'umuza baÄŸlarÄ±z, bÃ¶ylece Pod bu verileri gÃ¶rebilir ve Secret'ten okuyabilir. ConfigMap veya Secret'ten verileri, Ã¶rneÄŸin environment variables olarak veya hatta bir Ã¶zellikler dosyasÄ± olarak uygulamamÄ±zÄ±n iÃ§inde kullanabiliriz.

![](images/10.png)

AslÄ±nda en Ã§ok kullanÄ±lan Kubernetes temel bileÅŸenlerinin neredeyse tamamÄ±nÄ± gÃ¶rdÃ¼k. Pod'a gÃ¶z attÄ±k. Hizmetlerin nasÄ±l kullanÄ±ldÄ±ÄŸÄ±nÄ±, Ingress bileÅŸeninin ne iÅŸe yaradÄ±ÄŸÄ±nÄ± gÃ¶rdÃ¼k ve ayrÄ±ca ConfigMap ve Secrets'Ä± kullanan harici yapÄ±landÄ±rmayÄ± da gÃ¶rdÃ¼k.

![](images/11.png)

### Volumes

SÄ±ra geldi Ã§ok Ã¶nemli bir kavrama. `Veri depolama` nedir ve Kubernetes iÃ§erisinde nasÄ±l Ã§alÄ±ÅŸÄ±r? UygulamamÄ±zÄ±n kullandÄ±ÄŸÄ± bir database pod'umuz ve de bir miktar verimiz var. Åu anda gÃ¶rdÃ¼ÄŸÃ¼nÃ¼z bu kurulumla, eÄŸer veritabanÄ± container veya pod'u yeniden baÅŸlatÄ±lÄ±rsa veri kaybolur. Bu aÃ§Ä±kÃ§a sorunlu ve elveriÅŸsizdir Ã§Ã¼nkÃ¼ database'deki verilerinizin veya gÃ¼nlÃ¼k verilerinizin uzun sÃ¼reli gÃ¼venilir ve kalÄ±cÄ± olmasÄ±nÄ± isteriz. Bunu Kubernetes'te yapmanÄ±n yolu, Kubernetes'in baÅŸka bir bileÅŸeni olan `Volumes` kullanmaktÄ±r.

![](images/12.png)

Ã‡alÄ±ÅŸma ÅŸekli ÅŸÃ¶yledir: Temelde bir fiziksel depolama birimini -yani bir sabit diski- pod'umuza baÄŸlarÄ±z.
* Bu depolama yerel bir makinede olabilir.
* Pod'un Ã§alÄ±ÅŸtÄ±ÄŸÄ± aynÄ± sunucu node'unda da olabilir.
* Kubernetes kÃ¼mesinin dÄ±ÅŸÄ±nda(Bulut depolama, Kubernetes kÃ¼mesinin bir parÃ§asÄ± olmayan kendi yerleÅŸke depolamanÄ±z) olabilir.
Bu yÃ¼zden bununla ilgili external reference var.

![](images/13.png)

BÃ¶ylece, database pod'u veya container yeniden baÅŸlatÄ±ldÄ±ÄŸÄ±nda, tÃ¼m veri kalÄ±cÄ± bir ÅŸekilde saklanmÄ±ÅŸ olacaktÄ±r.

Kubernetes kÃ¼mesi ve tÃ¼m bileÅŸenlerinin ve depolama arasÄ±ndaki farkÄ± anlamamÄ±z Ã¶nemlidir. Yerel veya uzak bir depolama olmasÄ± fark etmeksizin, depolamayÄ± Kubernetes kÃ¼mesine takÄ±lmÄ±ÅŸ harici bir sabit diske benzetebiliriz. Ã‡Ã¼nkÃ¼ buradaki Ã¶nemli nokta; Kubernetes kÃ¼mesi aÃ§Ä±kÃ§a hiÃ§bir veri kalÄ±cÄ±lÄ±ÄŸÄ±nÄ± yÃ¶netmez. Kubernetes kullanÄ±cÄ±sÄ± veya yÃ¶neticisi olarak sizin veriyi yedeklemenizden, Ã§oÄŸaltmanÄ±zdan, yÃ¶netmenizden ve uygun donanÄ±mda saklamanÄ±zdan emin olmanÄ±z gerektiÄŸi anlamÄ±na gelir.

### Deployment ve StatefulSet

Åimdi, her ÅŸey mÃ¼kemmel bir ÅŸekilde Ã§alÄ±ÅŸÄ±yor ve bir kullanÄ±cÄ± bir tarayÄ±cÄ± aracÄ±lÄ±ÄŸÄ±yla uygulamaya eriÅŸebiliyor. Bu kurulumla, application pod'u Ã¶lÃ¼rse, crashlerse veya yeni bir container image oluÅŸturduÄŸumuz  iÃ§in pod'u restart etmemiz gerekiyorsa ne olurdu? BasitÃ§e cevap verecek olursak, bir kullanÄ±cÄ±nÄ±n uygulamamÄ±za ulaÅŸamadÄ±ÄŸÄ± bir sÃ¼re olan bir kesintimiz olurdu. BÃ¶yle bir durum end product'ta gerÃ§ekleÅŸmesi Ã§ok kÃ¶tÃ¼ bir durumdur.

![](images/14.png)

Distributed systems ve konteynerlarÄ±n avantajÄ± tam olarak budur. YalnÄ±zca 1 application pod'u ve 1 database pod'u gibi bir ÅŸeye gÃ¼venmek yerine, her ÅŸeyi birden fazla sunucuda replikasÄ±nÄ± oluÅŸturuyoruz. Yani uygulamamÄ±zÄ±n bir klonu veya Ã§oÄŸaltmasÄ± Ã§alÄ±ÅŸacaÄŸÄ± baÅŸka bir node olacak ve bu da servise baÄŸlÄ± olacak. HatÄ±rlarsak servisin, bir pod Ã¶ldÃ¼ÄŸÃ¼nde end point'i sÃ¼rekli ayarlamamÄ±za gerek olmadÄ±ÄŸÄ±, kalÄ±cÄ± statik IP adresi ve bir DNS adÄ±na sahip olduÄŸunu sÃ¶ylemiÅŸtik.

* Service aynÄ± zamanda bir `load balancer`dÄ±r. Yani, servis isteÄŸi yakalayacak ve en az meÅŸgul olan pod'a yÃ¶nlendirecektir.


Ancak application pod'unun ikinci replikasÄ±nÄ± oluÅŸturmak iÃ§in ikinci bir pod oluÅŸturmayÄ±z. Bunun yerine uygulama pod'umuzun bir blueprint'ini tanÄ±mlarÄ±z ve o pod'un kaÃ§ tane replikasÄ±nÄ±n olmasÄ±nÄ± istediÄŸimizi belirtiriz. Ve bu component veya blueprint'e `deployment` denir. Deployment, Kubernetes'in baÅŸka bir componentidir. Pratikte, pod'larla Ã§alÄ±ÅŸmayÄ±z veya pod'lar oluÅŸturmayÄ±z. Ã‡Ã¼nkÃ¼ zaten kaÃ§ tane replika olacaÄŸÄ±nÄ± belirtebilir ve ihtiyacÄ±mÄ±z olan pod'larÄ±n replika sayÄ±sÄ±nÄ± artÄ±rabilir veya azaltabiliriz. Yani pod, container'larÄ±n Ã¼zerinde bir soyutlama katmanÄ±dÄ±r(layer of abstraction). Deployment ise, podlarÄ±n Ã¼zerinde baÅŸka bir soyutlama katmanÄ±dÄ±r(layer of abstraction). Bu durum; pod'larla etkileÅŸimi, kopyalama ve diÄŸer yapÄ±landÄ±rmalarÄ± daha kullanÄ±ÅŸlÄ± hale getirir.

![](images/15.png)

Yani sonuÃ§ olarak Ã§oÄŸunlukla pod'larla deÄŸil, deployment'larla Ã§alÄ±ÅŸÄ±rÄ±z. Uygulama pod'umuzun replikalarÄ±ndan biri Ã¶lÃ¼rse, servis istekleri baÅŸka bir replikaya yÃ¶nlendirilecektir, bu ÅŸekilde uygulamamÄ±z kullanÄ±cÄ±lar iÃ§in hala eriÅŸilebilir olacaktÄ±r.

![](images/16.png)

Åimdi muhtemelen ÅŸunu merak ediyoruzdur, database pod'u ne olacak? Ã‡Ã¼nkÃ¼ eÄŸer database pod'u Ã¶lÃ¼rse, uygulamanÄ±z da eriÅŸilemez olacaktÄ±r. Bu yÃ¼zden, bir database replikasÄ±na da ihtiyacÄ±mÄ±z var. Ancak, `deployment kullanarak bir databese'i kopyalayamayÄ±z`. Bunun nedeni, database'in bir state'i olmasÄ±dÄ±r, yani veridir. Bu da demektir ki eÄŸer database'in replikalarÄ± veya klonlarÄ± olsaydÄ±, hepsi aynÄ± paylaÅŸÄ±lan data storage volume'Ã¼ne eriÅŸmek zorunda kalacaktÄ±. Bu durumda da, hangi pod'larÄ±n anlÄ±k olarak depolama birimine yazdÄ±ÄŸÄ±nÄ± veya hangi pod'larÄ±n depolama biriminden okuduÄŸunu yÃ¶neten bir mekanizmaya ihtiyacÄ±mÄ±z olacaktÄ±.

Bu mekanizma, Ã§oÄŸaltma Ã¶zelliklerinin yanÄ± sÄ±ra baÅŸka bir Kubernetes componenti olan `StatefulSet` ile saÄŸlanÄ±r.

![](images/17.png)

Bu component Ã¶zellikle database gibi uygulamalar iÃ§in tasarlanmÄ±ÅŸtÄ±r. Yani, MySQL, MongoDB, Elasticsearch veya herhangi bir diÄŸer stateful applications veya databaseleri; deployments yerine `StatefulSets` kullanÄ±larak oluÅŸturulmalÄ±dÄ±r.
Bu Ã§ok Ã¶nemli bir ayrÄ±mdÄ±r. StatefulSet, aynÄ± deployment gibi, pod'larÄ± replikalamayÄ± yapar ve bunlarÄ± scaling'e alÄ±r. Database reading ve writing iÅŸlemlerinin senkronize olduÄŸundan emin olur, bÃ¶ylece database tutarsÄ±zlÄ±klarÄ± olmaz.

![](images/18.png)

> [!TIP]
> Ancak, bir Kubernetes kÃ¼mesinde StatefulSets kullanarak database uygulamalarÄ±nÄ± deploy etmek biraz zahmetli olabilir. Bu yÃ¼zden, database uygulamalarÄ±nÄ± Kubernetes kÃ¼mesinin dÄ±ÅŸÄ±nda barÄ±ndÄ±rmak ve yalnÄ±zca daÄŸÄ±tÄ±mlarÄ± veya durumsuz uygulamalarÄ± Kubernetes kÃ¼mesinin iÃ§inde sorunsuz bir ÅŸekilde Ã§oÄŸaltmak ve Ã¶lÃ§eklendirmek ve dÄ±ÅŸ database ile iletiÅŸim kurmak yaygÄ±n bir uygulamadÄ±r.

Åimdi, uygulama pod'umuzun iki replikasÄ± ve database'in iki kopyasÄ± olduÄŸunda ve hepsi load-balanced olduÄŸunda, kurulumumuz daha gÃ¼venlidir. Bu senaryoda eÄŸer Node 1 yeniden baÅŸlatÄ±lsaydÄ± veya Ã§Ã¶kseydi, hala uygulama ve database pod'larÄ±nÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ± ikinci bir node'umuz olurdu. Uygulama, bu iki replikadan yeniden oluÅŸturulana kadar kullanÄ±cÄ± tarafÄ±ndan eriÅŸilebilir olacaktÄ±r, bu yÃ¼zden kesintiyi Ã¶nlemiÅŸ oluruz.

![](images/19.png)

Ã–zetlemek gerekirse, en Ã§ok kullanÄ±lan Kubernetes bileÅŸenlerini inceledik. ParÃ§alar arasÄ±nda iletiÅŸim kurmak iÃ§in `pod`lar ve `servis`lerle baÅŸladÄ±k, ve trafiÄŸi clusterlara yÃ¶nlendirmek iÃ§in kullanÄ±lan `Ingress` bileÅŸenini inceledik. AyrÄ±ca, `ConfigMaps` ve `Secret` kullanarak external configuration, ve `Volumes` kullanarak veri kalÄ±cÄ±lÄ±ÄŸÄ±nÄ± inceledik. Ve son olarak, `Deployments` ve `StatefulSets` gibi replicating ve blueprintlere baktÄ±k.

Burada `stateful applications` Ã¶zellikle databaseler gibi stateful applications iÃ§in kullanÄ±lÄ±r. Ve evet, Kubernetes'in sunduÄŸu Ã§ok daha fazla bileÅŸen var, ama bunlar Ã§ekirdek, temel olanlarÄ±. Bu temel bileÅŸenleri kullanarak oldukÃ§a gÃ¼Ã§lÃ¼ Kubernetes kÃ¼mesi oluÅŸturabiliriz.

---

## Kubernetes Mimarisi

Kubernetes'in temel mimarisinden bahsedeceÄŸiz. Bu yÃ¼zden Kubernetes'in Ã§alÄ±ÅŸtÄ±ÄŸÄ± iki tÃ¼r node'u inceleyeceÄŸiz: biri `master` diÄŸeri ise `slave`. BunlarÄ±n arasÄ±ndaki fark nedir ve her birinin cluster iÃ§indeki rolÃ¼ nedir, onlara bakacaÄŸÄ±z.

Kubernetes'in ne yaptÄ±ÄŸÄ±nÄ± ve cluster'Ä±n nasÄ±l self-managed, self-healing ve automated olduÄŸunu gÃ¶steren temel kavramlarÄ± ele alacaÄŸÄ±z.  Bir Kubernetes cluster operatÃ¶rÃ¼ olarak, Ã§ok daha az manuel Ã§aba harcamamÄ±z gerektiÄŸini gÃ¶receÄŸiz.

### Node Process

Ä°ki application pod'unun Ã§alÄ±ÅŸtÄ±ÄŸÄ± tek bir node ile bu temel kurulumla baÅŸlayacaÄŸÄ±z. Kubernetes mimarisinin ana bileÅŸenlerinden biri worker servers veya node'dur. Her node, o node'da Ã§alÄ±ÅŸan birden fazla application pod'una sahip olacaktÄ±r.

Ve Kubernetes'in bunu yapma ÅŸekli, her node'da bulunmasÄ± gereken ve bu pod'larÄ± planlamak ve yÃ¶netmek iÃ§in kullanÄ±lan `three process` kullanmasÄ±dÄ±r. Yani node'lar, asÄ±l iÅŸi yapan cluster serverlardÄ±r. Bu yÃ¼zden bazen onlara worker nodes da denir.

#### 1) Container Runtime

Her node'da Ã§alÄ±ÅŸmasÄ± gereken ilk sÃ¼reÃ§, `container runtime`dÄ±r. Biz Docker ile iÅŸlem yapacaÄŸÄ±z, ancak baÅŸka bir teknoloji de olabilir. Applitacion pod'larÄ±nda iÃ§inde Ã§alÄ±ÅŸan containerlar olduÄŸu iÃ§in, her node'da bir `container runtime`  kurulmalÄ±dÄ±r.

#### 2) Kubelet

Kubernetes'in bir parÃ§asÄ± olan kubelet, Pod'larÄ± ve bu Pod'larÄ±n altÄ±ndaki container'larÄ± schedule eder. Container Runtime, node'un kendisiyle arayÃ¼z oluÅŸtururken, kubelet bu yapÄ±landÄ±rmayÄ± alÄ±r, bir pod'u Ã§alÄ±ÅŸtÄ±rÄ±r (veya iÃ§inde bir container baÅŸlatÄ±r) ve ardÄ±ndan o node'dan container'a CPU, RAM gibi depolama kaynaklarÄ± atar.

Bu nedenle, genellikle bir Kubernetes cluster kurulu olmalÄ±dÄ±r. Kubelet hizmetlerine sahip birden fazla node'dan oluÅŸur. Bu worker node'larÄ±, Ã–rneÄŸimizdeki application ve database pod'larÄ±nÄ±n replikalarÄ±nÄ± Ã§alÄ±ÅŸtÄ±racak yÃ¼zlerce diÄŸer node'u Ã§alÄ±ÅŸtÄ±rÄ±r.

AralarÄ±ndaki iletiÅŸim ÅŸekli, `servisler` ile olur, bu da isteÄŸi application parÃ§asÄ±na veya Ã¶rneÄŸin bir database'e yÃ¶nlendiren bir `load-balancer` gibi Ã§alÄ±ÅŸÄ±r ve ardÄ±ndan ilgili parÃ§aya yÃ¶nlendirir.

#### 3) Kube Proxy

Hizmetlerden pod'lara istekleri iletmekten sorumlu Ã¼Ã§Ã¼ncÃ¼ sÃ¼reÃ§ `kube-proxy`dir ve her node'da kurulmalÄ±dÄ±r. Kube-proxy, dÃ¼ÅŸÃ¼k bir iÅŸlem yÃ¼kÃ¼ ile performanslÄ± bir ÅŸekilde iletiÅŸim kurulmasÄ±nÄ± saÄŸlayan akÄ±llÄ± yÃ¶nlendirme mantÄ±ÄŸÄ±na sahiptir.

Bir uygulama veya bu uygulamanÄ±n replikasÄ±, bir database'e istek yapÄ±yorsa, hizmet sadece isteÄŸi rastgele bir replikaya yÃ¶nlendirmek yerine; isteÄŸi baÅŸlatan pod'un Ã§alÄ±ÅŸtÄ±ÄŸÄ± aynÄ± node'da Ã§alÄ±ÅŸan replikaya yÃ¶nlendirecektir. Bu ÅŸekilde, isteÄŸi baÅŸka bir makineye gÃ¶ndermekle ilgili `aÄŸ iÅŸlem yÃ¼kÃ¼nden` kaÃ§Ä±nÄ±lmÄ±ÅŸ olur.

Ã–zetlemek gerekirse; bir kubernetes cluster'Ä±nÄ±n dÃ¼zgÃ¼n Ã§alÄ±ÅŸabilmesi iÃ§in `kubelet` ve `kube-proxy` her worker node iÃ§erisine `container runtime` ile birlikte kurulmalÄ±dÄ±r.

![](images/20.png)

Ancak ÅŸimdi soru ÅŸu: Bu cluster ile nasÄ±l etkileÅŸime girilir? Yeni bir application pod'u veya database pod'u nerede schedule edilmeli? Bir replika pod'u Ã¶lÃ¼rse, hangi process monitoring, reschedule veya restart iÅŸlemleri ile ilgilenir?

### Master Node

Master servers(master nodes), iÃ§erisinde tamamen farklÄ± processler Ã§alÄ±ÅŸtÄ±rÄ±r. Ve bunlar, cluster state ve worker nodes'larÄ± kontrol eden, `her yÃ¶netici dÃ¼ÄŸÃ¼mÃ¼nde Ã§alÄ±ÅŸan dÃ¶rt sÃ¼reÃ§tir`.

#### 1) API Server

Ä°lk hizmetimiz API server. Bir Kubernetes cluster'Ä±nda yeni bir application deploy etmek istediÄŸinizde, bir kullanÄ±cÄ± olarak API server ile interact ederiz. Bir Kubernetes Dashboard gibi bir kullanÄ±cÄ± arayÃ¼zÃ¼ de olabilir, `kubectl` gibi bir command-line tool veya bir Kubernetes API'si de olabilir.

API sunucusu, cluster iÃ§ine herhangi bir gÃ¼ncelleme talebinin veya hatta clusterdan gelen sorgularÄ±n ilk isteÄŸini alÄ±r. Kimlik doÄŸrulamasÄ±(auth) yaparak, yalnÄ±zca kimliÄŸi doÄŸrulanmÄ±ÅŸ ve yetkilendirilmiÅŸ isteklerin clusterlara iletilmesini saÄŸlar.

![](images/21.png)

Bu, yeni pod'lar planlamak, yeni applications deploy etmek, yeni servisler oluÅŸturmak veya herhangi bir component oluÅŸturmak istediÄŸimizde, bu requestimizi master node API sunucusuna iletmek zorunda olduÄŸunuz anlamÄ±na gelir. API server daha sonra requestimizi doÄŸrular. Her ÅŸey yolundaysa, requestimizi diÄŸer sÃ¼reÃ§lere ileterek istediÄŸimiz pod'u veya bileÅŸeni schedule iÃ§in bir node'a yÃ¶nlendirir.

![](images/22.png)

AyrÄ±ca, daÄŸÄ±tÄ±mÄ±mÄ±zÄ±n durumu veya cluster health etc., gibi sorgu isteklerini yapmak isteyebiliriz. Bu sorgular API sunucusuna bir istek gÃ¶nderir ve o da bize yanÄ±t verir.

> [!NOTE]
> Bu durum gÃ¼venlik aÃ§Ä±sÄ±ndan gayet iyidir Ã§Ã¼nkÃ¼ clusterlara yalnÄ±zca `bir entry point` vardÄ±r.

#### 2) Scheduler

BaÅŸka bir Master process ise Scheduler'dÄ±r. API serverÄ±na yeni bir pod schedule isteÄŸi gÃ¶nderdiÄŸimizi varsayalÄ±m. API server bu isteÄŸimizi doÄŸruladÄ±ktan sonra, bu pod'un bir worker node'da baÅŸlatÄ±lmasÄ± iÃ§in `Scheduler`'a teslim eder.

Ve tabii ki, herhangi bir node'a rastgele atamak yerine, Scheduler, bir sonraki pod'un hangi belirli worker node'un scheduled olacaÄŸÄ± konusunda zekice bir ÅŸekilde karar vermektedir. Ä°lk olarak, isteÄŸimizi kontrol eder ve planlamak istediÄŸimiz uygulamanÄ±n ne kadar kaynaÄŸa ihtiyacÄ± olduÄŸunu kontrol eder. Ne kadar CPU, ne kadar RAM vb.

ArdÄ±ndan, worker node'daki kullanÄ±labilir kaynaklarÄ± kontrol eder. EÄŸer bir node'un en Ã§ok kaynaÄŸa sahip olduÄŸunu sÃ¶ylÃ¼yorsa, yeni pod'u o node'a schedule eder.

![](images/23.png)

Ã–nemli bir nokta ÅŸu ki, scheduler sadece yeni bir pod'un hangi node'a schedule edileceÄŸine karar verir. AsÄ±l planlamayÄ± yapan ve pod'u konteynerla baÅŸlatan iÅŸlem ise `kubelet`'tir. Yani kubelet, scheduler'dan gelen isteÄŸi alÄ±r ve bu isteÄŸi ilgili node Ã¼zerinde yÃ¼rÃ¼tÃ¼r.

#### 3) Controller Manager

Bir sonraki Ã¶nemli bileÅŸen ise `controller manager`'dÄ±r. Bu bileÅŸen, herhangi bir dÃ¼ÄŸÃ¼mde pod'lar Ã¶ldÃ¼ÄŸÃ¼nde ne olacaÄŸÄ± sorusu aÃ§Ä±sÄ±ndan kritik Ã¶neme sahiptir. Ã–lÃ¼ node'larÄ± tespit etmek ve daha sonra bu pod'larÄ± en kÄ±sa sÃ¼rede reschedule etmek gerekir.

![](images/24.png)

DolayÄ±sÄ±yla controller manager, state changes'larÄ±, Ã¶rneÄŸin pod'larÄ±n Ã§Ã¶kmesini tespit eder. Pod'lar Ã¶ldÃ¼ÄŸÃ¼nde controller manager bunu algÄ±lar ve cluster state'ini mÃ¼mkÃ¼n olan en kÄ±sa sÃ¼rede kurtarmaya Ã§alÄ±ÅŸÄ±r.

Ã–len pod'larÄ± yeniden schedule etme amacÄ±yla scheduler'a bir istek gÃ¶nderir. Bu dÃ¶ngÃ¼ iÃ§inde, scheduler kaynak hesaplamasÄ±na gÃ¶re hangi worker node'larÄ±n bu pod'larÄ± tekrar baÅŸlatmasÄ± gerektiÄŸine karar verir ve bu worker node'lar Ã¼zerindeki ilgili `kubelet`lere, pod'larÄ± yeniden baÅŸlatmalarÄ± iÃ§in istek gÃ¶nderir.

![](images/25.png)

#### 4) Etcd

Son olarak, ana iÅŸlemlerden biri olan etcd, bir cluster state'inin key-value deposudur. Bunu aslÄ±nda bir cluster beyni olarak dÃ¼ÅŸÃ¼nebiliriz. Yani cluster'daki her deÄŸiÅŸiklik -Ã¶rneÄŸin yeni bir pod schedule edildiÄŸinde veya bir pod Ã¶ldÃ¼ÄŸÃ¼nde- etcd'nin bu key-value deposunda kaydedilir veya gÃ¼ncellenir.

![](images/26.png)

Etcd deposunun bir kÃ¼me beyni olarak adlandÄ±rÄ±lmasÄ±nÄ±n sebebi, scheduler, controller manager gibi tÃ¼m bu mekanizmalarÄ±n, etcd'nin sahip olduÄŸu veriler sayesinde Ã§alÄ±ÅŸmasÄ±dÄ±r.

![](images/27.png)

Ã–rneÄŸin, scheduler her bir worker node'unda hangi kaynaklarÄ±n mevcut olduÄŸunu nasÄ±l bilir? Veya controller manager, cluster durumunda bir deÄŸiÅŸiklik olduÄŸunu nasÄ±l tespit eder? Pod'larÄ±n Ã¶lmesi, kubelet'in scheduler'Ä±n isteÄŸi Ã¼zerine yeni pod'larÄ± baÅŸlatmasÄ±, API sunucusuna cluster health hakkÄ±nda bir sorgu gÃ¶ndermemiz veya uygulama daÄŸÄ±tÄ±m durumumuz gibi bu durum bilgilerini, API sunucusu nereden alÄ±r?

Cevap: TÃ¼m bu bilgiler etcd kÃ¼mesinde saklanÄ±r. Etcd'nin key-value deposunda saklanmayan ÅŸey ise gerÃ§ek uygulama verileridir. Ã–rneÄŸin, bir cluster iÃ§inde Ã§alÄ±ÅŸan bir database uygulamamÄ±z varsa, veriler etcd'de deÄŸil, baÅŸka bir yerde saklanÄ±r. Bu, yalnÄ±zca master iÅŸlemlerinin worker iÅŸlemleriyle ve tersiyle iletiÅŸim kurmasÄ± iÃ§in kullanÄ±lan bir cluster state bilgisidir.

ArtÄ±k muhtemelen ana iÅŸlemlerin, Ã¶zellikle de verileri gÃ¼venilir bir ÅŸekilde saklanmasÄ± veya Ã§oÄŸaltÄ±lmasÄ± gereken etcd deposunun, cluster operasyonu iÃ§in kritik Ã¶neme sahip olduÄŸunu anlamÄ±ÅŸÄ±zdÄ±r. Bu nedenle, uygulamada bir Kubernetes kÃ¼mesi genellikle birden fazla master'dan oluÅŸur. Her bir master dÃ¼ÄŸÃ¼mÃ¼ kendi ana iÅŸlemlerini Ã§alÄ±ÅŸtÄ±rÄ±r; elbette API sunucusu load-balanced'dÄ±r ve etcd deposu tÃ¼m master dÃ¼ÄŸÃ¼mleri arasÄ±nda distributed bir depolama oluÅŸturur.

![](images/28.png)

---

## Cluster YapÄ±sÄ±


Åimdi worker ve master node'larÄ±nda Ã§alÄ±ÅŸan iÅŸlemleri gÃ¶rdÃ¼kten sonra, gerÃ§ek hayattaki bir cluster kurulumuna bakalÄ±m. Ã‡ok kÃ¼Ã§Ã¼k bir cluster'da muhtemelen iki master node ve Ã¼Ã§ worker node olur.

![](images/29.png)

Burada dikkat edilmesi gereken bir diÄŸer nokta ise master node sunucularÄ±nÄ±n donanÄ±m kaynaklarÄ±nÄ±n aslÄ±nda farklÄ± olmasÄ±dÄ±r. Master iÅŸlemleri daha Ã¶nemlidir, ancak aslÄ±nda daha az iÅŸ yÃ¼kÃ¼ne sahiptirler. DolayÄ±sÄ±yla CPU, RAM ve depolama gibi daha az kaynaÄŸa ihtiyaÃ§ duyarlar. Worker node'larÄ± ise, containerlarÄ± Ã§alÄ±ÅŸtÄ±ran pod'larÄ± barÄ±ndÄ±rma gibi asÄ±l iÅŸi yaparlar.

Bu nedenle, worker node'larÄ±nÄ±n daha fazla kaynaÄŸa ihtiyacÄ± vardÄ±r. UygulamamÄ±zÄ±n karmaÅŸÄ±klÄ±ÄŸÄ± ve kaynak gereksinimi arttÄ±kÃ§a, aslÄ±nda cluster'Ä±mÄ±za daha fazla master ve worker node'u ekleyerek daha gÃ¼Ã§lÃ¼ ve saÄŸlam bir kÃ¼me oluÅŸturabiliriz. BÃ¶ylece uygulama kaynak gereksinimlerimizi karÅŸÄ±layabiliriz.

![](images/30.png)

Var olan bir Kubernetes cluster'Ä±nda yeni master veya worker serverlarÄ± eklemek aslÄ±nda oldukÃ§a kolaydÄ±r. Bir master sunucusu eklemek istiyorsak, yeni bir bare metal sunucu ediniyoruz. Ãœzerine tÃ¼m master work'leri kurup  onu Kubernetes kÃ¼mesine ekliyoruz. Bu kadar..

Yine aynÄ± ÅŸekilde, iki worker node'una ihtiyacÄ±mÄ±z varsa, bare metal sunucular ediniyoruz. Container runtime, kubelet ve kube-proxy gibi tÃ¼m worker node iÅŸlemlerini Ã¼zerlerine kurup onlarÄ± Kubernetes clusterÄ±na ekleyin.

Ä°ÅŸte bu kadar.
Bu ÅŸekilde, uygulama karmaÅŸÄ±klÄ±ÄŸÄ± ve kaynak gereksinimi arttÄ±kÃ§a, Kubernetes kÃ¼memizin gÃ¼cÃ¼nÃ¼ ve kaynaklarÄ±nÄ± sonsuza kadar artÄ±rabiliriz.

---

## Minikube ve Kubectl Kurulumu

### Minikube

![](images/31.png)

Genellikle Kubernetes dÃ¼nyasÄ±nda bir production cluster kurduÄŸumuzda, aÅŸaÄŸÄ±daki gibi gÃ¶rÃ¼necektir.

![](images/32.png)

En az iki olmak Ã¼zere birden fazla Master'a sahip olacaÄŸÄ±z ve birden fazla worker node olacak. Worker dÃ¼ÄŸÃ¼mlerinin kendi ayrÄ± sorumluluklarÄ± vardÄ±r. Diyagramda gÃ¶rdÃ¼ÄŸÃ¼mÃ¼z gibi, her biri bir node'u temsil eden gerÃ§ek ayrÄ± sanal veya fiziksel makinelerimiz olur.

Åimdi, yerel ortamÄ±mÄ±zda bir ÅŸey test etmek istiyorsak veya yeni bir uygulama, yeni bileÅŸenler daÄŸÄ±tarak Ã§ok hÄ±zlÄ± bir ÅŸekilde bir ÅŸey denemek istiyorsak ve bunlarÄ± yerel makinemizde test etmek istiyorsak; aÃ§Ä±kÃ§asÄ± bÃ¶yle bir cluster kurmak oldukÃ§a zor olacaktÄ±r. Bellek ve CPU gibi yeterli kaynaÄŸÄ±mÄ±z yoksa imkansÄ±z bile olabilir. Ä°ÅŸte tam olarak bu kullanÄ±m durumu iÃ§in `Minikube` adÄ± verilen aÃ§Ä±k kaynaklÄ± araÃ§ var.

Minikube'un ne olduÄŸuna gelirsek, temelde hem master processleri hem de worker processleri tek bir node'da Ã§alÄ±ÅŸtÄ±ran tek node bir clusterdÄ±r. Bu node'da Ã¶nceden yÃ¼klenmiÅŸ bir Docker container runtime olacak ÅŸekilde konteynerleri veya konteynerli pod'larÄ± Ã§alÄ±ÅŸtÄ±rabileceÄŸiz.

![](images/33.png)

DizÃ¼stÃ¼ bilgisayarÄ±mÄ±zda VirtualBox, KVM veya baÅŸka bir hipervizÃ¶r aracÄ±lÄ±ÄŸÄ±yla Ã§alÄ±ÅŸacak. Yani temel olarak, Minikube dizÃ¼stÃ¼ bilgisayarlarÄ±mÄ±zda bir sanal makine oluÅŸturacak ve burada gÃ¶rdÃ¼ÄŸÃ¼mÃ¼z node'lar bu sanal makinede Ã§alÄ±ÅŸacak.

Ã–zetleyecek olursak, Minikube, yerel kurulumumuzda Kubernetes'i test etmek iÃ§in kullanabileceÄŸimiz dizÃ¼stÃ¼ bilgisayarÄ±nÄ±zda bir sanallaÅŸtÄ±rma aracÄ±yla Ã§alÄ±ÅŸan tek node bir Kubernetes clusterdÄ±r.

![](images/34.png)

Yerel makinemizde bir cluster veya mini cluster kurduktan sonra, bu cluster ile etkileÅŸim kurmak iÃ§in bir yola ihtiyacÄ±mÄ±z olacaktÄ±r. Componentler oluÅŸturmak, yapÄ±landÄ±rmak vb. isteyeceÄŸiz. Tam burada `kubectl` devreye giriyor.

### Kubectl

local makinemizde Minikube'u temsil eden bu virtual node'a sahip olduktan sonra, bu cluster ile etkileÅŸim kurmak iÃ§in bir yola ihtiyacÄ±mÄ±z vardÄ±r. Bunu Kubernetes clusterlarÄ± iÃ§in bir command line toolu olan `kubectl` kullanarak yapabiliriz.

NasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶relim. Minikube'un hem master hem de worker processleri Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nÄ± sÃ¶ylemiÅŸtik, bu nedenle API server adÄ± verilen master processlerden biri aslÄ±nda Kubernetes clusterÄ±n entry point noktasÄ±dÄ±r.

![](images/35.png)

Kubernetes'te bir ÅŸey yapmak istiyorsak veya herhangi bir ÅŸeyi yapÄ±landÄ±rmak istiyorsak, Ã¶nce API server ile konuÅŸmamÄ±z gerekir. API server ile konuÅŸmanÄ±n yolu ise farklÄ± istemciler aracÄ±lÄ±ÄŸÄ±yla olur. Bir dashboard gibi bir UI arayÃ¼zÃ¼nÃ¼z olabilir, Kubernetes API'sini kullanarak konuÅŸabilir veya `kubectl` command line tool kullanabiliriz.

![](images/36.png)

`kubectl` aslÄ±nda Ã¼Ã§ istemcinin de en gÃ¼Ã§lÃ¼sÃ¼dÃ¼r Ã§Ã¼nkÃ¼ `kubectl` ile Kubernetes'te istediÄŸimiz her ÅŸeyi yapabiliriz.

Bu yazÄ±nÄ±n neredeyse sonuna kadar `kubectl` kullanÄ±lmaktadÄ±r. `kubectl` API sunucusuna component oluÅŸturmak, component silmek vb. iÃ§in komutlar gÃ¶nderdikten sonra, Minikube node'undaki worker processler bunlarÄ± gerÃ§ekleÅŸtirecektir. Pod'lar oluÅŸturmak, pod'larÄ± yok etmek, servisler oluÅŸturmak vb. iÃ§in komutlarÄ± yÃ¼rÃ¼teceklerdir.

Bu, Minikube Ã§alÄ±ÅŸma ÅŸeklidir. `kubectl` cluster ile nasÄ±l kullanÄ±lÄ±r? Burada Ã¶nemli bir nokta, `kubectl`'in yalnÄ±zca Minikube cluster iÃ§in olmadÄ±ÄŸÄ±dÄ±r. Bir cloud cluster'Ä±mÄ±z veya hibrit bir cluster'Ä±mÄ±z varsa, `kubectl` herhangi bir Kubernetes cluster kurulumuyla etkileÅŸim kurmak iÃ§in kullanÄ±lan araÃ§tÄ±r. Bu nedenle burada unutulmamasÄ± Ã¶nemlidir.

![](images/37.png)

ArtÄ±k Minikube ve `kubectl`'nin ne olduÄŸunu bildiÄŸimize gÃ¶re, onlarÄ± pratikte gÃ¶rmek iÃ§in kuruluma baÅŸlayalÄ±m.

### Kurulum
Daha Ã¶nce de belirttiÄŸimiz gibi Minikube bir sanallaÅŸtÄ±rmaya ihtiyaÃ§ duyar, Ã§Ã¼nkÃ¼ bazÄ± hipervizÃ¶rlerde Ã§alÄ±ÅŸacaktÄ±r. Bu nedenle bir tÃ¼r hipervizÃ¶r yÃ¼klemeliyiz.


Åimdi linux Ã¼zerinde minikube ve kubectl kurulumuna geÃ§elim.

```shell
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube && rm minikube-linux-amd64
```

Åimdi her ÅŸeyin kurulduÄŸundan emin olalÄ±m ve komutlarÄ± kontrol edelim. Yani, `minikube` komutu Ã§alÄ±ÅŸmalÄ±:

```
c3ng0@ubn:~$ minikube start

ğŸ˜„  minikube v1.33.1 on Ubuntu 22.04
âœ¨  Automatically selected the docker driver. Other choices: kvm2, qemu2, none, ssh
ğŸ“Œ  Using Docker driver with root privileges
ğŸ‘  Starting "minikube" primary control-plane node in "minikube" cluster
ğŸšœ  Pulling base image v0.0.44 ...
ğŸ’¾  Downloading Kubernetes v1.30.0 preload ...
    > preloaded-images-k8s-v18-v1...:  112.62 MiB / 342.90 MiB  32.84% 5.18 MiB
    > gcr.io/k8s-minikube/kicbase...:  70.41 MiB / 481.58 MiB  14.62% 2.65 MiB
    > index.docker.io/kicbase/sta...:  481.58 MiB / 481.58 MiB  100.00% 11.15 M
â—  minikube was unable to download gcr.io/k8s-minikube/kicbase:v0.0.44, but successfully downloaded docker.io/kicbase/stable:v0.0.44 as a fallback image
ğŸ”¥  Creating docker container (CPUs=2, Memory=2200MB) ...
    > kubectl.sha256:  64 B / 64 B [-------------------------] 100.00% ? p/s 0s
    > kubeadm.sha256:  64 B / 64 B [-------------------------] 100.00% ? p/s 0s
    > kubelet.sha256:  64 B / 64 B [-------------------------] 100.00% ? p/s 0s
    > kubectl:  49.07 MiB / 49.07 MiB [------------] 100.00% 14.58 MiB p/s 3.6s
    > kubeadm:  47.92 MiB / 47.92 MiB [--------------] 100.00% 3.97 MiB p/s 12s
    > kubelet:  95.46 MiB / 95.46 MiB [--------------] 100.00% 6.52 MiB p/s 15s

    â–ª Generating certificates and keys ...
    â–ª Booting up control plane ...
    â–ª Configuring RBAC rules ...
ğŸ”—  Configuring bridge CNI (Container Networking Interface) ...
ğŸ”  Verifying Kubernetes components...
    â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
ğŸŒŸ  Enabled addons: storage-provisioner, default-storageclass
ğŸ’¡  kubectl not found. If you need it, try: 'minikube kubectl -- get pods -A'
ğŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default
```

Ve `kubectl` indirmemiz gerekiyor:

```bash
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
```

Binary'i doÄŸrula(opsiyonel)

kubectl checksumfile indir:

```bash
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"   
```

Ä°ndirilen checksum file ile binary doÄŸrulama:

```bash
echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check
```

   DoÄŸru ise Ã§Ä±ktÄ± aÅŸaÄŸÄ±daki gibi olmalÄ±:

`kubectl: OK`

- Kubectl Kurulumu

```bash
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
```

- Son sÃ¼rÃ¼mÃ¼ kurduÄŸumuzu kontrol etme:
```bash
kubectl version --client
```

![](images/38.png)

daha fazlasÄ± iÃ§in [kubernetes.io](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/)

Minikube oldukÃ§a basit bir komut satÄ±rÄ± aracÄ± ile birlikte gelir. Tek bir komutla tÃ¼m Kubernetes kÃ¼mesini bu tek dÃ¼ÄŸÃ¼m kurulumunda hÄ±zlÄ±ca baÅŸlatabilir, durdurabilir veya silebiliriz.

---

## Ä°lk Cluster

Åimdi her ikisini de kurduÄŸumuza gÃ¶re, bir Minikube Kubernetes cluster oluÅŸturalÄ±m.

```bash
minikube start
```

iÅŸte Minikube ile bir Kubernetes cluster nasÄ±l baÅŸlatacaÄŸÄ±mÄ±z:

```bash
minikube start --vm-driver=kvm
```

Burada, kurulu hipervizÃ¶rÃ¼n devreye girdiÄŸini gÃ¶rebiliriz Ã§Ã¼nkÃ¼ Minikube'un bir Sanal ortamda Ã§alÄ±ÅŸmasÄ± gerektiÄŸinden, Minikube'a hangi hipervizÃ¶rÃ¼ kullanmasÄ± gerektiÄŸini sÃ¶yleyeceÄŸiz. Bunun iÃ§in, `--vm-driver` olarak adlandÄ±rÄ±lan bir seÃ§enek belirleyeceÄŸiz ve burada bende kurulu olan `kvm`'i ayarladÄ±m.

Bunu yÃ¼rÃ¼ttÃ¼ÄŸÃ¼mde bir ÅŸeyler indirecek, yani ilk kez yapÄ±yorsak biraz daha uzun sÃ¼rebilir.

![](images/39.png)

Ve bahsettiÄŸim gibi, makinenizde Docker yoksa bile Ã§alÄ±ÅŸacak.

TamamlandÄ±. ArtÄ±k `kubectl`, Minikube'u kullanacak ÅŸekilde yapÄ±landÄ±rÄ±lmÄ±ÅŸ durumda, bu da Minikube cluster'Ä±nÄ±n kurulduÄŸu anlamÄ±na gelir.

![](images/40.png)

Kubernetes KÃ¼mesi ile etkileÅŸimde bulunmak iÃ§in tasarlanmÄ±ÅŸ olan `kubectl` komutu da o Minikube kÃ¼mesi ile baÄŸlantÄ±lÄ±dÄ±r, eÄŸer ÅŸunu yaparsak:

```bash
kubectl get nodes
```

Bu, Kubernetes cluster node'larÄ±nÄ±n durumunu bize bildirir. Bize bir Minikube node'unun hazÄ±r olduÄŸunu sÃ¶yleyecek ve gÃ¶rdÃ¼ÄŸÃ¼mÃ¼z gibi aÃ§Ä±kÃ§a master processleri Ã§alÄ±ÅŸtÄ±rmalÄ± Ã§Ã¼nkÃ¼ sadece bir node var.

![](images/41.png)

Ve ayrÄ±ca Minikube'Ä±n durumunu alabiliriz:

```bash
minikube status
```

![](images/42.png)

Yani, ana makinede kubelet adlÄ± bir hizmetin Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶rÃ¼yoruz, bu da aslÄ±nda konteyner runtime kullanarak pod'larÄ± Ã§alÄ±ÅŸtÄ±ran bir hizmettir, yani her ÅŸey Ã§alÄ±ÅŸÄ±yor.

Buradan itibaren mini Kub kÃ¼mesi ile `kubectl` komut satÄ±rÄ± aracÄ±lÄ±ÄŸÄ±yla etkileÅŸime geÃ§eceÄŸiz. Minicube sadece cluster baÅŸlatma ve silme iÃ§in kullanÄ±lÄ±r, ancak configuring ve diÄŸer her ÅŸeyi `kubectl` aracÄ±lÄ±ÄŸÄ±yla yapacaÄŸÄ±z.

---

## Main Kubectl KomutlarÄ±

Bu bÃ¶lÃ¼mde bazÄ± temel Kubectl komutlarÄ±nÄ± gÃ¶receÄŸiz ve minikube'da nasÄ±l create ve debug pods yapÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶receÄŸiz.

Cubectl'i clusterda herhangi bir ÅŸey yapmak iÃ§in kullanacaÄŸÄ±z. -components oluÅŸturmak, status almak, vb.-

* Ä°lk olarak, node'larÄ±n durumunu alacaÄŸÄ±z.

```bash
kubectl get nodes
```

Bu komutu kullanarak node'larÄ±n durumunu Ã¶ÄŸrenebiliyoruz.

```bash
c3ng0@ubn:~$ kubectl get nodes
NAME       STATUS   ROLES           AGE    VERSION
minikube   Ready    control-plane   145m   v1.30.0
```

GÃ¶rÃ¼yoruz ki bir node var ve her ÅŸey o node'da Ã§alÄ±ÅŸÄ±yor Ã§Ã¼nkÃ¼ bu bir `minikube`.

* Pod'larÄ± kontrol edebiliriz ve herhangi bir pod'umuz olmadÄ±ÄŸÄ± iÃ§in sonuÃ§ yok.

```bash
kubectl get pod
```

* Services kontrol edebiliriz, varsayÄ±lan bir servisimiz var.

```bash
kubectl get services
```

```bash
c3ng0@ubn:~$ kubectl get services
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   148m
```

Åimdi, herhangi bir Pod'umuz olmadÄ±ÄŸÄ± iÃ§in bir tane oluÅŸturacaÄŸÄ±z ve Kubernetes bileÅŸenleri oluÅŸturmanÄ±n bir Kubectl create komutu olduÄŸunu hatÄ±rlayalÄ±m. Kubectl create komutunu kullanarak tÃ¼m bu bileÅŸenleri oluÅŸturabiliriz.

* Ancak listede `Pod` yok Ã§Ã¼nkÃ¼ Kubernetes dÃ¼nyasÄ±nda, Pod, Kubernetes clusterÄ±nÄ±n en kÃ¼Ã§Ã¼k birimidir ve genellikle, Pod'larÄ± doÄŸrudan oluÅŸturulmaz. Veya Pod'larla doÄŸrudan Ã§alÄ±ÅŸÄ±lmaz. Pod'larÄ±n Ã¼zerinde bir soyutlama katmanÄ±`(abstraction over Pods)` vardÄ±r, buna Â·`deployment`Â· denir. Ä°ÅŸte bu oluÅŸturmak Ã¼zere olduÄŸumuz ÅŸey. Bu da, altÄ±ndaki parÃ§alarÄ± oluÅŸturacak.

```bash
Usage:
   kubectl create deployment NAME --image=image -- [COMMAND] [args...] [options]
```

* **NAME**: deployment'a isim vermeliyiz
* **--image=**: oluÅŸturacaÄŸÄ±mÄ±z container image'i

Åimdi bir nginx daÄŸÄ±tÄ±mÄ± oluÅŸturalÄ±m.

```bash
c3ng0@ubn:~$ kubectl create deployment nginx-depl --image=nginx
deployment.apps/nginx-depl created
```

 * Nginx gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼, Docker Hub'dan indirecektir. Bu komutu yÃ¼rÃ¼ttÃ¼ÄŸÃ¼mÃ¼z zaman, gÃ¶rdÃ¼ÄŸÃ¼nÃ¼z gibi nginx deployment oluÅŸturuldu.

```bash
c3ng0@ubn:~$ kubevtl get deployment
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
nginx-depl   0/1     1            0           15s
```

 * OluÅŸturulmuÅŸ bir deployment olduÄŸunu gÃ¶rÃ¼yoruz ve burada "hazÄ±r deÄŸil" durumunda olduÄŸunu da gÃ¶rebiliyoruz.

```bash
c3ng0@ubn:~$ kubectl get pod
NAME                          READY   STATUS    RESTARTS   AGE
nginx-depl-85c9d7c5f4-g4lwt   0/1     Creating  0          31s
```

 * ArtÄ±k bir Pod'umuz var. Name deÄŸeri, prefix ve rastgele bir hash'e sahipt. Burada "konteyner oluÅŸturuluyor" yazÄ±yor, yani henÃ¼z hazÄ±r deÄŸil. Bir sÃ¼re beklersek `Running`.

 * Bir deployment oluÅŸturduÄŸumuzda, deployment, Pod oluÅŸturmak iÃ§in gereken tÃ¼m bilgilere veya blueprintlere sahip olur.

![](images/43.png)

 * Bu en temel yapÄ±landÄ±rmadÄ±r, sadece adÄ± ve gÃ¶rÃ¼ntÃ¼sÃ¼. Bu kadar.. geri kalanÄ± default.

 * Deployment ve Pod arasÄ±nda bir baÅŸka katman vardÄ±r ve bu, otomatik olarak kubernetes tarafÄ±ndan yÃ¶netilen `replicaset`'tir.
 * `kubectl get replicaset` yaparsak, bir nginx replica set hash'imiz olduÄŸunu gÃ¶rÃ¼yoruz. Ve burada, Pod adÄ±nÄ±n bir deployment prefix, replicaset'in ID'si ve son olarak kendi ID'si olduÄŸunu gÃ¶rebiliriz. Pod adÄ± bu ÅŸekilde oluÅŸmaktadÄ±r. Replicaset, Pod'un tÃ¼m replikalarÄ±nÄ± yÃ¶netir. Biz hiÃ§bir zaman replica set oluÅŸturmayacak, silmeyecek veya gÃ¼ncellemeyeceÄŸiz. DoÄŸrudan deploymentlar ile Ã§alÄ±ÅŸacaÄŸÄ±z. Bu daha uygun Ã§Ã¼nkÃ¼ deploymentlarda Pod blueprintini tamamen yapÄ±landÄ±rabiliriz. Pod'un kaÃ§ replikasÄ±na ihtiyacÄ±mÄ±z olduÄŸunu belirtebilir ve geri kalan configuration'u orada yapabiliriz.

### LayerlarÄ±n Ã§alÄ±ÅŸma ÅŸekli:

 * Ä°lk olarak Deployment, ReplicaSet'i yÃ¶netir.
 * ReplicaSet, o Pod'un tÃ¼m replikalarÄ±nÄ± yÃ¶netir.
 * Pod, bir konteynerin soyutlamasÄ±dÄ±r.
 Deployment'tan aÅŸaÄŸÄ±daki her ÅŸey otomatik olarak kubernetes tarafÄ±ndan yÃ¶netilmelidir.

Ã–rneÄŸin, kullanÄ±ldÄ±ÄŸÄ± image gibi bir ÅŸeyi doÄŸrudan bir deployment iÃ§erisinde dÃ¼zenlememiz gerekecek, Pod iÃ§inde deÄŸil. Ã–yleyse hemen yapalÄ±m.

```python
c3ng0@ubn:~$ kubectl edit deployment nginx-depl
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  creationTimestamp: "2024-05-14T11:31:36Z"
  generation: 1
  labels:
    app: nginx-depl
  name: nginx-depl
  namespace: default
  resourceVersion: "9431"
  uid: 66d185d6-b628-4d10-b3bc-4aea093dfc59
spec:
  progressDeadlineSeconds: 600
  replicas: 1
...
...
...
```


Deployment oluÅŸtururken verdiÄŸimiz iki seÃ§enek dÄ±ÅŸÄ±nda her ÅŸeyin otomatik olarak oluÅŸturulmuÅŸ bir deployment, otomatik olarak oluÅŸturulmuÅŸ bir yapÄ±landÄ±rma dosyasÄ±nÄ± alÄ±yoruz.
Åimdilik sadece resmi aÃ§Ä±p istediÄŸim versiyonu 1.16'ya sabitlemek istediÄŸimizi varsayalÄ±m ve bu deÄŸiÅŸikliÄŸi kaydedelim.

```
    spec:
      containers:
-       - image: nginx
---
    spec:
      containers:
+       - image: nginx:1.16

```

Ve gÃ¶rdÃ¼ÄŸÃ¼nÃ¼z gibi daÄŸÄ±tÄ±m dÃ¼zenlendi.

![](images/44.png)

Åimdi `kubectl get pod` yaparsak, eski pod'umuzu gÃ¶rÃ¼rÃ¼z.

![](images/45.png)

* Eski Pod sona erdi ve yeni pod baÅŸladÄ±.

EÄŸer ReplicaSet'i gÃ¶rÃ¼ntÃ¼lersek, eski olanÄ±n iÃ§inde pod olmadÄ±ÄŸÄ±nÄ± ve yeni bir tane oluÅŸturulduÄŸunu gÃ¶rÃ¼yoruz.

![](images/46.png)

Yani sonuÃ§ olarak deployment yapÄ±landÄ±rmasÄ±nÄ± dÃ¼zenledik ve altÄ±ndaki her ÅŸey otomatik olarak gÃ¼ncellendi. Bu yaptÄ±ÄŸÄ±mÄ±z, Kubernetes'in sihrine ve nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±na bir Ã¶rnektir.


### Debugging Pods

Bir diÄŸer Ã§ok pratik komut ise `kubectl logs`, bu aslÄ±nda Pod iÃ§inde Ã§alÄ±ÅŸan uygulamanÄ±n neyi kaydettiÄŸini gÃ¶sterir.

```bash
kubectl logs [POD_NAME]
```

LoglarÄ± gÃ¶rÃ¼ntÃ¼lemeden Ã¶nce Nginx hiÃ§bir ÅŸey kaydetmediÄŸi iÃ§in baÅŸka bir daÄŸÄ±tÄ±m oluÅŸturalÄ±m. Mongodb'den oluÅŸturalÄ±m ve adÄ±na `mongo-depl` verelim.

![](images/47.png)

Åimdi mongodb deployment oluÅŸturuluyor.

![](images/48.png)

Åu anda loglara bakabiliriz:

![](images/49.png)

* `kubectl describe pod [POD_NAME]` events sekmesinde bize state deÄŸiÅŸikliklerini verir.

![](images/50.png)

Loglamak, uygulamanÄ±n gerÃ§ekte neyi yazdÄ±ÄŸÄ±nÄ± gÃ¶rmede ve hata ayÄ±klamada yardÄ±mcÄ± olmaktadÄ±r.

BaÅŸka bir Ã§ok kullanÄ±ÅŸlÄ± komut, `kubectl exec`tir. Debugging yaparken, bir ÅŸey Ã§alÄ±ÅŸmÄ±yorsa veya sadece Pod'un iÃ§eriÄŸini kontrol etmek iÃ§in kullanÄ±lÄ±r.
BasitÃ§e aÃ§Ä±klayacak olursak, Ã§alÄ±ÅŸan Pod'dan shell alÄ±r. bu yÃ¼zden:
```
kubectl exec -it [POD_NAME] -- bin/bash
```
* -it = **interactive terminal**

![](images/51.png)

Bu komutla mongodb uygulama konteynerinin terminalini alÄ±yoruz ve ÅŸu anda root kullanÄ±cÄ±sÄ± olarak mongodb konteynerinin iÃ§indeyiz.
Exec, hata ayÄ±klama veya bir ÅŸeyleri test etmek veya denemek istediÄŸinizde kullanÄ±ÅŸlÄ±dÄ±r. Konteynera girebilir veya terminali alabilir ve orada bazÄ± komutlar Ã§alÄ±ÅŸtÄ±rabiliriz.


### Deployment Silme ve Apply Configuration File

Tabii ki kubectl ile podlarÄ± silebiliriz, Ã¶nce deployment'larÄ± ve podlarÄ± gÃ¶rÃ¼ntÃ¼leyelim.

![](images/52.png)

```
kubectl delete deployment [deployment_name]
```

![](images/53.png)

kontrol edersek Pod'un sonlandÄ±ÄŸÄ±nÄ± ve eÄŸer replica set alÄ±rsak, mongodb replicasetinin de gittiÄŸini gÃ¶rebiliriz.

TÃ¼m crud iÅŸlemleri (create,update,delete vb.) deployment seviyesinde gerÃ§ekleÅŸir ve altÄ±ndaki her ÅŸey otomatik olarak takip eder. AynÄ± ÅŸekilde Services gibi diÄŸer Kubernetes kaynaklarÄ± oluÅŸturabiliriz.

Ancak fark ettiÄŸiniz gibi, Kubectl ile deployment gibi kubernetes bileÅŸenlerini oluÅŸtururken, tÃ¼m bu seÃ§enekleri komut satÄ±rÄ±nda belirtmemiz gerekir.

* AdÄ± belirtmemiz gerekir.
* Image'i belirtmemiz gerekir
* option1
* option2..  vb. olabilir.

Elbette bir deployment'ta veya bir Pod'ta yapÄ±landÄ±rmak istediÄŸimiz birÃ§ok ÅŸey olabilir ve aÃ§Ä±kÃ§asÄ± bunlarÄ±n hepsini komut satÄ±rÄ±nda yazmak pratik olmayacaktÄ±r. Bunun iÃ§in genellikle Kubernetes yapÄ±landÄ±rma dosyalarÄ±yla Ã§alÄ±ÅŸÄ±lmaktadÄ±r. Yani oluÅŸturduÄŸumuz bileÅŸenin tÃ¼rÃ¼, adÄ±, image'i ve diÄŸer tÃ¼m seÃ§enekleri bir yapÄ±landÄ±rma dosyasÄ±nda toplanÄ±r. Sadece cubectl'e bu yapÄ±landÄ±rma dosyasÄ±nÄ± yÃ¼rÃ¼tmesini sÃ¶yleriz.

Bunu yapmanÄ±n yolu `kubectl apply` komutunu kullanmaktÄ±r.

### kubectl apply

Apply, temelde dosyayÄ±, yapÄ±landÄ±rma dosyasÄ±nÄ± bir parametre olarak alÄ±r ve orada ne yazdÄ±ysak yapar.

```bash
kubectl apply -f [file_name]
```

Apply, "-f" iÃ§in bir seÃ§enek alÄ±r ve bu dosyanÄ±n adÄ±nÄ± belirtir ve genellikle bu dosyalar iÃ§in kullanÄ±lan biÃ§im YAML'dir ve bu, dosyadaki her ÅŸeyi yÃ¼rÃ¼ten komuttur. Bu yÃ¼zden aslÄ±nda bunu yapÄ±landÄ±rma dosyasÄ± olarak adlandÄ±racaÄŸÄ±z.

Ã–rnek olarak Ã§ok basit, temel bir  `nginx-deployment.yaml` deployment dosyasÄ± oluÅŸturalÄ±m.

Deployment iÃ§in temel yapÄ±landÄ±rma:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.16
        ports:
        - containerPort: 80
```

* Åu an iÃ§in gerekli olan satÄ±rlarÄ±, aÅŸaÄŸÄ±da inceleyelim.
```yml
kind: Deployment
## Ne oluÅŸturmak istediÄŸimizi belirtiyoruz, Deployment oluÅŸturmak istiyoruz.
```

```yml
name: nginx-deployment
## OluÅŸturacaÄŸÄ±mÄ±z Deployment ismi.
```

```yml
spec:            ## specification for deployment
  replicas: 1    ## Pod'lardan oluÅŸturulacak replika sayÄ±sÄ±
```

```yml
  template:
    metadata:
      labels:
        app: nginx
    spec:    ## specification for pods
      containers:
      - name: nginx
        image: nginx:1.16   ## Konteyner image'imiz iÃ§in nginx versiyonu
        ports:
        - containerPort: 80 ## Binding Port
## Bu alan olutÅŸturacaÄŸÄ±mÄ±z deployment'a ait blueprint
```

Bu, bizim config  dosyamÄ±z ve buna bir kere sahip olduktan sonra, bu yapÄ±landÄ±rmayÄ± istediÄŸimiz zaman uygulayabiliriz.

![](images/54.png)

Deployment oluÅŸturuldu, ÅŸimdi podu gÃ¶rÃ¼ntÃ¼lersek, nginx daÄŸÄ±tÄ±mÄ± podu oluÅŸturuldu ve Ã§alÄ±ÅŸÄ±yor olduÄŸunu gÃ¶rÃ¼rÃ¼z.

![](images/55.png)

AyrÄ±ca daÄŸÄ±tÄ±mÄ±n 3 dakika 57 saniye Ã¶nce oluÅŸturulduÄŸunu gÃ¶rÃ¼yoruz. EÄŸer bu deployment'ta bir ÅŸeyleri deÄŸiÅŸtirmek istersek, sadece yerel yapÄ±landÄ±rmamÄ±zÄ± deÄŸiÅŸtirmemiz yeterlidir. Ã–rneÄŸin, bir yerine iki replika istersek, sadece dosyayÄ± dÃ¼zenleyip tekrar apply ederiz. Ve deployment, nginx daÄŸÄ±tÄ±mÄ± olarak tekrar yapÄ±landÄ±rÄ±lacaktÄ±r.

![](images/56.png)

![](images/57.png)

Fark ettiysek Ã§Ä±ktÄ±da bize "configured" dendi. Fark ÅŸu ki, Kubernetes, nginx deployment'Ä±nÄ±n var olmadÄ±ÄŸÄ±nÄ± algÄ±larsa, yeni bir tane oluÅŸturacak, ancak eÄŸer deployment zaten varsa, yapÄ±landÄ±rma dosyasÄ±nÄ± apply ettiÄŸimizde, onu gÃ¼ncellemesi gerektiÄŸini bilecek ve yeni bir deployment oluÅŸturmak yerine eski deployment'Ä± cofigure edecek.

![](images/58.png)

Eski deployment hala ayakta (9m45s) fakat yeni bir replika oluÅŸturuldu(3m22s) Ã§Ã¼nkÃ¼ replika sayÄ±sÄ±nÄ± arttÄ±rdÄ±k. yani `kubectl apply` ile bir component oluÅŸturabilir ve gÃ¼ncelleyebiliriz. Elbette Services, Volumes gibi diÄŸer kubernetes bileÅŸenlerine de ayar Ã§ekebiliriz.

---

Ã–zetlemek gerekirse, bu yazÄ±da birkaÃ§ kubectl komutuna baktÄ±k, bir component oluÅŸturmayÄ±, nasÄ±l configure edeceÄŸimizi ve sileceÄŸimizi gÃ¶rdÃ¼k. Pod'larÄ±n, deployment'larÄ±n, replikaset'lerinin vb. state'lerini nasÄ±l alacaÄŸÄ±mÄ±zÄ± gÃ¶rdÃ¼k. AyrÄ±ca Pod'un iÃ§indeki uygulamanÄ±n konsola yazdÄ±ÄŸÄ± her ÅŸeyi nasÄ±l kaydedeceÄŸimizi gÃ¶rdÃ¼k ve `kubectl exec`'i kullanarak Ã§alÄ±ÅŸan bir konteynerdan nasÄ±l shell alacaÄŸÄ±mÄ±zÄ± gÃ¶rdÃ¼k. Son olarak, kubernetes yapÄ±landÄ±rma dosyasÄ±nÄ± ve `kubectl apply` komutunu kullanarak componentleri nasÄ±l oluÅŸturup gÃ¼ncelleyeceÄŸimizi gÃ¶rdÃ¼k.
Son olarak azÄ±cÄ±k da `kubectl describe` komutunu gÃ¶rdÃ¼k, bu da bir konteynerin bir Pod'da sorun giderme iÃ§in ek bilgi almak istediÄŸinizde kullandÄ±ÄŸÄ±nÄ±z bir komuttu.



### KomutlarÄ± HatÄ±rlayalÄ±m
> [!NOTE]
> **Crud KomutlarÄ±**
> * Deployment OluÅŸturma                 ->        `kubectl create deployment [name]`
> * Deployment DÃ¼zenleme                 ->        `kubectl edit deployment [name]`
> * Deployment Silme                     ->        `kubectl delete deployment [name]`
>
> **FarklÄ± Kubernetes Componenetlerin Durumu**
> * `kubectl get nodes | pod | services | replicaset | deployment`
>
> **Podlar ile Debugging**
> * Pod LoglarÄ±                          ->        `kubectl logs [pod_name]`
> * Terminal ile Poda BaÄŸlanma           ->        `kubectl exec -it [pod_name] -- /bin/bash`
> * Pod Bilgisi                          ->        `kubectl describe pod [pod_name]`  
>
> **CRUD iÃ§in Config DosyasÄ± Kullanma**
> * KonfigÃ¼rasyon DosyasÄ±nÄ± Uygulama     ->        `kubectl apply -f [file_name]`
> * KonfigÃ¼rasyon DosyasÄ±yla Silme       ->        `kubectl delete -f [file_name]`


---


## Kubernetes YAML KonfigÃ¼rasyonu

Bu konu baÅŸlÄ±ÄŸÄ±nda Kubernetes yapÄ±landÄ±rma dosyasÄ±nÄ±n sÃ¶zdizimini ve iÃ§eriÄŸini gÃ¶receÄŸiz.

![](images/59.png)

Bu dosya, Kubernetes clusterÄ±nda componentler oluÅŸturmak ve yapÄ±landÄ±rmak iÃ§in ana tooldur. BÃ¼yÃ¼k yapÄ±landÄ±rma dosyalarÄ±nÄ± gÃ¶rdÃ¼ysek biraz karÄ±ÅŸÄ±k olduklarÄ±nÄ± dÃ¼ÅŸÃ¼nebiliriz, ancak aslÄ±nda oldukÃ§a basit ve mantÄ±klÄ± bir yapÄ±ya sahiptir. Ã–yleyse baÅŸlayalÄ±m!

###  3 ParÃ§ada K8s Config DosyasÄ±

![](images/60.png)

YukarÄ±daki fotoÄŸrafta yan yana bir `deployment` ve `service` yapÄ±landÄ±rma dosyasÄ± Ã¶rnekleri var.

Ã–ncelikle her Kubernetes config(yapÄ±landÄ±rma) dosyasÄ± Ã¼Ã§ kÄ±sÄ±mdan oluÅŸur.

* Ä°lk kÄ±sÄ±m, oluÅŸturduÄŸunuz componentin `metadata`'larÄ±nÄ±n bulunduÄŸu yerdir.
* Ä°kinci kÄ±sÄ±m, yapÄ±landÄ±rma dosyasÄ±ndaki `specification`'dur(Ã¶zellik). Her component config dosyasÄ±nda, o component iÃ§in uygulamak istediÄŸimiz her tÃ¼rlÃ¼ yapÄ±landÄ±rmayÄ± bu bÃ¶lÃ¼me koyarÄ±z.

![](images/61.png)

Config dosyasÄ±ndaki Ä°lk iki satÄ±r, tam olarak neyi oluÅŸturmak istediÄŸimizi belirtiyor. Soldaki kÄ±sÄ±mda `deployment` oluÅŸturuyoruz, saÄŸdaki kÄ±sÄ±mda ise bir `service` oluÅŸturuyoruz.
Ä°lk satÄ±rlarda ise API versiyonu belirtiyoruz. Her component iÃ§in API versiyonu farklÄ± olabilir.
Ve bu, her component iÃ§in bir API versiyonu araÅŸtÄ±rmanÄ±z gerektiÄŸi anlamÄ±na gelir.

* *Kubernetes'te her bileÅŸenin farklÄ± API sÃ¼rÃ¼mÃ¼ olmasÄ±nÄ±n temel nedeni, her bileÅŸenin farklÄ± gereksinimlere ve Ã¶zelliklere sahip olmasÄ±dÄ±r. Her bileÅŸenin farklÄ± bir API sÃ¼rÃ¼mÃ¼ olmasÄ±, bu bileÅŸenin Ã¶zelliklerini ve davranÄ±ÅŸÄ±nÄ± belirleyen Ã¶zelleÅŸtirilmiÅŸ bir yapÄ±ya sahip olmasÄ±nÄ± saÄŸlar.*

Specification kÄ±smÄ±nda belirteceÄŸimiz Ã¶zellikler, oluÅŸturduÄŸumuz component tÃ¼rÃ¼ne Ã¶zgÃ¼ olacaktÄ±r. AÅŸaÄŸÄ±daki resimde gÃ¶rÃ¼ldÃ¼ÄŸÃ¼ gibi:

![](images/63.png)

Deployment kendi Ã¶zelliklerine sahip olacak ve tabii servis de kendi ÅŸeylerine sahip olacak.

HatÄ±rlarsak config dosyalarÄ±nÄ±n Ã¼Ã§ parÃ§adan olduÄŸunu sÃ¶ylemiÅŸtik ama ÅŸu an sadece `metadata` ve `specification`'u gÃ¶rdÃ¼k. Peki Ã¼Ã§Ã¼ncÃ¼ parÃ§amÄ±z nerede?

* ÃœÃ§Ã¼ncÃ¼ kÄ±sÄ±m `status`'tur. Kubernetes tarafÄ±ndan otomatik olarak oluÅŸturulup eklenir.

Ã‡alÄ±ÅŸma ÅŸekli ÅŸÃ¶yledir:

* Kubernetes her zaman `desired state`(istenen durum) ve `actual state`'i(gerÃ§ek durum) karÅŸÄ±laÅŸtÄ±rÄ±r. EÄŸer gerÃ§ek durum ve istenen durum uyuÅŸmuyorsa, Kubernetes bir ÅŸeylerin dÃ¼zeltilmesi gerektiÄŸini bilir ve onu dÃ¼zeltmeye Ã§alÄ±ÅŸÄ±r.
* Bu, Kubernetes'in saÄŸladÄ±ÄŸÄ± `self-healing` Ã¶zelliÄŸinin temelidir.

* Ã–rneÄŸin, yukarÄ±daki deployment kodundaki specification bÃ¶lÃ¼mÃ¼nde, nginx deployment iÃ§in iki replika istediÄŸimizi belirtmiÅŸiz. Bu config dosyasÄ±nÄ± kullanarak deployment oluÅŸturduÄŸumuzda(apply), Kubernetes, deployment status(durumunu) takip edecek, config dosyamÄ±za status parÃ§amÄ±zÄ± da ekleyecek ve bunu sÃ¼rekli olarak gÃ¼ncelleyecektir.

![](images/65.png)

* EÄŸer status, bir noktada sadece bir replikanÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± sÃ¶ylÃ¼yorsa, Kubernetes bu statusu Ã¶zellikle belirttiÄŸimiz iki replikayla karÅŸÄ±laÅŸtÄ±racak ve bir sorun olduÄŸunu bilecektir. BaÅŸka bir replika oluÅŸturmasÄ± gerektiÄŸini anlayacak ve bunu otomatik olarak gerÃ§ekleÅŸtirecektir. `self-healing`.

![](images/66.png)

BunlarÄ± gÃ¶rÃ¼nce aklÄ±mÄ±za Kubernetes'in, otomatik olarak buraya ekleyerek veya sÃ¼rekli olarak gÃ¼ncellemek iÃ§in status verisini nereden aldÄ±ÄŸÄ± takÄ±labilir.

* Bu bilgi daha Ã¶nce bahsettiÄŸimiz `etcd`'den gelir. Master process'lerden biri olan etcd, aslÄ±nda cluster verilerini depolar. Etcd **her zaman** herhangi bir Kubernetes bileÅŸeninin mevcut durumunu tutar ve bu status bilgisi buradan gelir.

### Config DosyasÄ±nÄ±n FormatÄ±

YukarÄ±daki gÃ¶rÃ¼ntÃ¼lerden de gÃ¶rdÃ¼ÄŸÃ¼mÃ¼z gibi config dosyalarÄ±nÄ±n formatÄ± `YAML`'dÄ±r. YAML oldukÃ§a basit bir formattÄ±r, ancak girintiler konusunda Ã§ok katÄ±dÄ±r.
YanlÄ±ÅŸ girintiye sahip bir dosyamÄ±z varsa, bu dosya geÃ§ersiz olacaktÄ±r. Bu yÃ¼zden 200 satÄ±rdan oluÅŸan bir yapÄ±landÄ±rma dosyamÄ±z varsa, `YAMLlint` gibi bir online yaml validator kullanmak mantÄ±klÄ± olacaktÄ±r.

BaÅŸka bahsedilecek bir konu da, bu config dosyalarÄ±nÄ± nereye kaydedeceÄŸimizdir. Klasik bir uygulama, kodumuzla birlikte saklamaktÄ±r. Ã‡Ã¼nkÃ¼ deployment ve servis, uygulamaya uygulanacaktÄ±r. Bu yapÄ±landÄ±rma dosyalarÄ±mÄ±zÄ±, uygulama kodunun bir parÃ§asÄ± olarak saklamak iyi bir yÃ¶ntemdir.
Bunlar `Infrastructure as a Code` **IaC** kavramÄ±nÄ±n bir parÃ§asÄ± olabilir veya config dosyalarÄ± iÃ§in kendi git depolarÄ±mÄ±zÄ± da oluÅŸturabiliriz.


### Podlar iÃ§in Blueprint (Template)

Biraz geriye gidersek deployment'larÄ±n, kendisinin altÄ±ndaki podlarÄ± yÃ¶nettiÄŸini sÃ¶ylemiÅŸtik.

![](images/67.png)

Yani bir deployment'ta bir ÅŸeyi dÃ¼zenlediÄŸimizde, bunun altÄ±ndaki bÃ¼tÃ¼n podlara yayÄ±lÄ±r ve birkaÃ§ pod oluÅŸturmak istediÄŸimizde aslÄ±nda bir deployment oluÅŸtururuz ve bu deployment gerisini halleder.

*Bunlar nasÄ±l gerÃ§ekleÅŸir? Bu bahsettiÄŸimiz ÅŸeyler konfigÃ¼rasyonda nerede tanÄ±mlanÄ±r?

Config dosyamÄ±zdaki, `specification` kÄ±smÄ±nda yer alan `template`'i geniÅŸletirsek; template'imizin de kendi `metadata`'sÄ±nÄ±n ve `specification`'u olduÄŸunu gÃ¶rÃ¼rÃ¼z.

![](images/71.png)
Yani basitÃ§e tabir edecek olursak **"configuration file inside of a configuration file"**

Bunun sebebi, template iÃ§erisindeki konfigÃ¼rasyonun bir `pod` iÃ§in geÃ§erli olmasÄ±dÄ±r. Bir podun bir deployment config dosyasÄ± iÃ§inde kendi yapÄ±landÄ±rmasÄ± olmalÄ±dÄ±r. Bu tÃ¼m deployment'larÄ±n nasÄ±l tanÄ±mlanacaÄŸÄ±nÄ± gÃ¶sterir.

![](images/72.png)
Bu sarÄ± alan ise bir pod iÃ§in bir blueprint'tir. Hangi image'e dayanacaÄŸÄ±mÄ±zÄ±, hangi portu aÃ§acaÄŸÄ±mÄ±zÄ±, konteynerin adÄ±nÄ±n ne olacaÄŸÄ±nÄ± vb. belirler.


### Connecting Components (Labels & Selectors & Ports)

BaÄŸlantÄ±nÄ±n kurulduÄŸu ÅŸekil `labels` ve `selectors` kullanÄ±larak gerÃ§ekleÅŸir.

![](images/73.png)
* Metadata bÃ¶lÃ¼mÃ¼ `labels` iÃ§erir.

![](images/74.png)
* Specification bÃ¶lÃ¼mÃ¼ ise `selectors` iÃ§erir.

---

Metadata kÄ±smÄ±nda bir componente bir `key-value` (anahtar-deÄŸer) Ã§ifti veririz.
YukarÄ±daki Ã¶rnekte app: nginx var. Bu label, bu componente yapÄ±ÅŸÄ±yor.

Bu ÅŸekilde, bu blueprint kullanÄ±larak oluÅŸturulan podlara `app: nginx` labeli veriyoruz ve deploymenti, "app: nginx" label'iyle eÅŸleÅŸen tÃ¼m label'lara baÄŸlamak veya eÅŸleÅŸtirmek iÃ§in ayarlÄ±yoruz.

![](images/75.png)

Bu sayede deployment, hangi podlarÄ±n kendisine ait olduÄŸunu bilecektir.

DaÄŸÄ±tÄ±mÄ±n kendi "app nginx" labeli var ve bu iki label, `Service` iÃ§indeki `selector` tarafÄ±ndan kullanÄ±lÄ±r. Bu nedenle, Service iÃ§erisinde specification bÃ¶lÃ¼mÃ¼nde bir selector tanÄ±mlarÄ±z ki bu, temelde bir connection oluÅŸturur. (Service ile Deployment veya parÃ§alarÄ± arasÄ±nda)

![](images/76.png)

Ã‡Ã¼nkÃ¼ Service, kendisine kayÄ±tlÄ± olan podlarÄ±, hangi podlarÄ±n o servise ait olduÄŸunu bilmesi gerekir ve bu baÄŸlantÄ± `label`'Ä±n `selector` bÃ¶lÃ¼mÃ¼yle yapÄ±lÄ±r.

* Service ve pod iÃ§inde yapÄ±landÄ±rÄ±lmasÄ± gereken baÅŸka bir ÅŸey de `Port` lardÄ±r.

Yani, servis iÃ§erisinde, servisin kendisinin eriÅŸilebilir olduÄŸu bir *Port* vardÄ±r.

![](images/77.png)

Pod iÃ§erisindeki KonteynerÄ±n da kendisine ait bir Port'u vardÄ±r.

![](images/78.png)

Bu nasÄ±l yapÄ±landÄ±rÄ±lÄ±r? Temelde, bir servisin kendisinin eriÅŸilebilir olduÄŸu bir portu vardÄ±r, bu yÃ¼zden diÄŸer servisler bir istek gÃ¶nderdiÄŸinde bu nginx servisine isteÄŸi 80 numaralÄ± baÄŸlantÄ± noktasÄ±nda gÃ¶ndermelidir, ancak bu servisin, isteÄŸi hangi pod'a yÃ¶nlendirmesi gerektiÄŸini, aynÄ± zamanda o pod'un hangi portu dinlediÄŸini de bilmesi gerekir ve bu da `target port`tur.

![](images/79.png)


Alttaki resimdeki gibi saÄŸdaki servis config dosyamÄ±zdaki `targetPort: 8080`, Deployment config dosyasÄ±ndaki container'Ä±n portuyla `containerPort:8080` eÅŸleÅŸmelidir.

![](images/80.png)

BÃ¶ylelikle deployment ve servisimizin temel yapÄ±landÄ±rmalarÄ±nÄ± tamamlamÄ±ÅŸ oluruz. Burada dikkate alÄ±nmasÄ± gereken Ã¶nemli bir nokta, AÅŸaÄŸÄ±daki resimdeki her iki bÃ¶lÃ¼mde de gÃ¶rdÃ¼ÄŸÃ¼mÃ¼z Ã§oÄŸu Ã¶zelliÄŸin zorunlu olduÄŸudur.

![](images/81.png)
* Bu aslÄ±nda deployment ve servis iÃ§in minimum yapÄ±landÄ±rmadÄ±r.

DosyalarÄ±mÄ±zÄ± elde ettikten sonra, bunlarÄ± uygulayabilir veya bunlarÄ± kullanarak bileÅŸenler oluÅŸturabiliriz. Konsola geÃ§ip oluÅŸturmadan Ã¶nce `nginx-deployment.yaml` dosyasÄ± ile `nginx-service.yaml` dosyalarÄ±nÄ±n iÃ§eriklerini buraya yazÄ±yoruz.

nginx-deployment.yaml
```yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.16
        ports:
        - containerPort: 8080
```

```yml
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
```

![](images/82.png)
![](images/84.png)

`kubectl apply deployment` komutuyla deployment ve servis oluÅŸturuldu.

Åimdi eÄŸer podlarÄ± listelersem, burada iki replikanÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶rÃ¼yoruz Ã§Ã¼nkÃ¼ config dosyamÄ±zda bÃ¶yle tanÄ±mlamÄ±ÅŸtÄ±k.
![](images/85.png)

AyrÄ±ca aÅŸaÄŸÄ±da gÃ¶rÃ¼ldÃ¼ÄŸÃ¼ gibi; nginx servisimiz de var.

![](images/86.png)

`kubernetes` default servistir, her zaman oradadÄ±r. Bizim oluÅŸturduÄŸumuz `nginx-service` ve Port 80'de dinlemede.

Åimdi, servisin istekleri doÄŸru pod'lara ilettiÄŸini nasÄ±l doÄŸrulayabiliriz? Bunun iÃ§in:

```bash
kubectl describe service [service_name]
```

![](images/87.png)

Ve burada, status bilgilerinin olduÄŸunu gÃ¶rebiliriz. Hedef BaÄŸlantÄ± NoktasÄ±nÄ± tanÄ±mlarÄ±z ve burada endpoint'imiz var ve bunlar servisin isteÄŸi ileteceÄŸi pod'larÄ±n IP adresleri ve baÄŸlantÄ± noktalarÄ± olmalÄ±dÄ±r. Peki, bunlarÄ±n doÄŸru pod'larÄ±n IP adresleri olduÄŸunu nasÄ±l bileceÄŸiz?
`kubectl get pod` komutu, IP adresi bilgisi vermez. Bunun iÃ§in;

```bash
kubectl get pod -o wide
```

`-o wide` ile daha fazla bilgi istiyoruz.

![](images/88.png)

ArtÄ±k IP adresini de gÃ¶rÃ¼yoruz ve baktÄ±ÄŸÄ±mÄ±z zaman, servisin doÄŸru end-point'e sahip olduÄŸunu biliyoruz.

Åimdi, config dosyasÄ±nÄ±n Ã¼Ã§Ã¼ncÃ¼ partÄ±nÄ± gÃ¶relim. HatÄ±rlarsak bu Kubernetes'in otomatik olarak oluÅŸturduÄŸu bir durumdu. Bunu yapmanÄ±n yolu, config dosyasÄ±nÄ± yaml formatÄ±nda alabiliriz.

```bash
kubectl get deployment nginx-deployment -o yaml
```

bu komutu Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±mÄ±zda sonuÃ§larÄ± veya gÃ¼ncellenmiÅŸ configi alÄ±yoruz. Konsolumuzda yaml Ã§Ä±ktÄ±sÄ±nÄ± aldÄ±k fakat bunu bir dosyaya kaydedelim ve yazdÄ±ÄŸÄ±mÄ±z config ile karÅŸÄ±laÅŸtÄ±ralÄ±m.

```bash
kubectl get deployment nginx-deployment -o yaml > result.yaml
```

![](images/90.png)

Burada eklenen birÃ§ok ÅŸey var ama sadece status kÄ±smÄ±na gÃ¶z atalÄ±m. Daha Ã¶nce de belirttiÄŸimiz gibi tÃ¼m bunlar otomatik olarak kubernetes tarafÄ±ndan dÃ¼zenlenir ve sÃ¼rekli olarak gÃ¼ncellenir. KaÃ§ replikanÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ±, bu replikalarÄ±n durumunu ve bazÄ± diÄŸer bilgileri belirtir. Bu kÄ±sÄ±m ayrÄ±ca debugging yaparken de yardÄ±mcÄ± olabilir. EÄŸer fark ettiysek, metadata ve specification kÄ±smÄ±na da yeni ÅŸeyler eklenmiÅŸ.

Burada dikkate alÄ±nmasÄ± gereken bir ÅŸey:
* Ã¶rneÄŸin, bir deploymenti kopyalamak istersek, *-belki otomatikleÅŸtirilmiÅŸ betikler kullanarak-*  yapmanÄ±z gereken bu oluÅŸturulan extra verileri kaldÄ±rmak ve temizlemektir.
* Yani bu son result deployment config dosyasÄ±nÄ± Ã¶nce temizlersek, ardÄ±ndan o blueprint konfigÃ¼rasyonundan baÅŸka bir deployment oluÅŸturabiliriz.
* Son olarak, deployment'Ä± veya servisi silmek istersem, bunu config dosyasÄ±nÄ± kullanarak da yapabiliriz.

```bash
kubectl delete -f nginx-deployment.yml
```

![](images/91.png)

---

## Ä°lk Demo Uygulama

Bu bÃ¶lÃ¼mde iki uygulama deploy edeceÄŸiz: MongoDB ve Express. Bu iki uygulama ile basit bir web app ve database kurulumunun tipik bir Ã¶rneÄŸini Ã§ok iyi gÃ¶rebileceÄŸiz. O zaman yapmaya baÅŸlayalÄ±m..
![](images/92.png)

* Ä°lk olarak bir MongoDB pod'u oluÅŸturacaÄŸÄ±z.
* Bu pod ile iletiÅŸim kurabilmek iÃ§in bir servise ihtiyacÄ±mÄ±z olacak. Internal servis oluÅŸturacaÄŸÄ±z, yani bu pod'a dÄ±ÅŸarÄ±dan gelen(external request) istekler engellenecek ve yalnÄ±zca aynÄ± cluster iÃ§indeki componentler iletiÅŸim kurabilecek. Bu bizim istediÄŸimiz ÅŸey.
* Daha sonra bir Mongo Express deployment oluÅŸturacaÄŸÄ±z. Ä°ki ÅŸeye ihtiyacÄ±mÄ±z olacak: biri MongoDB'nin veritabanÄ± URL'si, bÃ¶ylece Express bu URL'ye baÄŸlanabilecek; diÄŸeri ise veritabanÄ±nÄ±n kullanÄ±cÄ± adÄ± ve ÅŸifresi, bÃ¶ylece kimlik doÄŸrulamasÄ± yapabilecek.

* Bu bilgileri Mongo Express deployment'a geÃ§irebilmenin yolu, deployment config dosyasÄ±nda `environmental variables`(Ã§evresel deÄŸiÅŸkenler) aracÄ±lÄ±ÄŸÄ±yla olacak, Ã§Ã¼nkÃ¼ uygulama bu ÅŸekilde yapÄ±landÄ±rÄ±lmÄ±ÅŸ.
* VeritabanÄ± URL'sini iÃ§eren bir `ConfigMap` ve kimlik bilgilerini iÃ§eren bir `Secret` oluÅŸturacaÄŸÄ±z ve bunlarÄ± deployment dosyasÄ±nÄ±n iÃ§ine referans olarak ekleyeceÄŸiz.
* Bu kurulum tamamlandÄ±ÄŸÄ±nda, Mongo Express'in tarayÄ±cÄ± Ã¼zerinden eriÅŸilebilir olmasÄ± gerekecek. Bunu yapmak iÃ§in, `external request`(dÄ±ÅŸ istek)lerin pod'a iletilmesine izin verecek bir `external service` oluÅŸturacaÄŸÄ±z.

![](images/93.png)

Bu kurulumla birlikte istek akÄ±ÅŸÄ± aÅŸaÄŸÄ±daki gibi gÃ¶rÃ¼necek.

![](images/94.png)

* Ä°stek tarayÄ±cÄ±dan gelecek ve Mongo Express'in external servisine gidecek.
* Bu servis isteÄŸi Mongo Express pod'una iletecek.
* Pod, MongoDB'nin internal servisine baÄŸlanacak ve isteÄŸi MongoDB pod'una iletecek. Burada kimlik doÄŸrulama yapÄ±lacak.
**Åimdi bu kurulumun tamamÄ±nÄ± Kubernetes yapÄ±landÄ±rma dosyalarÄ±nÄ± kullanarak oluÅŸturalÄ±m.**

### 1) MongoDB Pod
Ä°lk olarak, Ã§alÄ±ÅŸan bir Minikube clusterÄ±mÄ±z var. `kubectl get all` komutunu kullanarak kÃ¼medeki tÃ¼m bileÅŸenleri listelediÄŸimde yalnÄ±zca default Kubernetes servisini gÃ¶rÃ¼yoruz.

![](images/95.png)

Yani clusterÄ±mÄ±z boÅŸ ve sÄ±fÄ±rdan baÅŸlÄ±yoruz. Ä°lk yapacaÄŸÄ±mÄ±z ÅŸey bir MongoDB deployment oluÅŸturmak.

![](images/96.png)

MongoDB iÃ§in hazÄ±r olan deployment dosyasÄ± aÅŸaÄŸÄ±daki ÅŸekilde.
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodb-deployment
  labels:
    app: mongodb
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
      - name: mongodb
        image: mongo
```

Bu daÄŸÄ±tÄ±ma `mongodb-deployment` adÄ±nÄ± verelim. Konteyner `mongodb` olarak adlandÄ±rÄ±lacak ve kullanacaÄŸÄ±mÄ±z image bu olacak. Hadi Docker-Hub'a gidip MongoDB image yapÄ±landÄ±rmasÄ±nÄ± kontrol edelim.
* https://hub.docker.com/_/mongo

AradÄ±ÄŸÄ±mÄ±z ÅŸey, bu konteyneri nasÄ±l kullanacaÄŸÄ±mÄ±z, yani hangi portlarÄ± aÃ§acaÄŸÄ± ve hangi harici yapÄ±landÄ±rmalarÄ± alacaÄŸÄ±z.

![](images/97.png)

MongoDB konteynerinin varsayÄ±lan portu 27017'miÅŸ, bu yÃ¼zden bunu kullanacaÄŸÄ±z.

AyrÄ±ca `Environment Variables`(Ã§evresel deÄŸiÅŸkenler) kullanacaÄŸÄ±z.

![](images/98.png)

root kullanÄ±cÄ± adÄ± ve root ÅŸifresi. Yani konteyner baÅŸlatÄ±ldÄ±ÄŸÄ±nda admin kullanÄ±cÄ± adÄ± ve ÅŸifresini tanÄ±mlayabiliriz. Åimdi bunu config dosyamÄ±zda ayarlayalÄ±m.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodb-deployment
  labels:
    app: mongodb
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
      - name: mongodb
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value:
        - name: MONGO_INITDB_ROOT_PASSWORD
          value:
```
 * `ports` Ã¶zelliÄŸi ve `containerPort` ile hangi portu aÃ§mak istediÄŸimizi belirttik. Standart portu kullandÄ±k.
 AltÄ±nda iki environment variables(`env`) belirttik.
 * Birincisi `MONGO_INITDB_ROOT_USERNAME` ve boÅŸ bÄ±rakacaÄŸÄ±z.
 * DiÄŸeri ise `MONGO_INITDB_ROOT_PASSWORD` olacak ve bunu da boÅŸ bÄ±rakacaÄŸÄ±z.

> [!CAUTION]
> **Åunu unutmamalÄ±yÄ±z ki bu, bir repository iÃ§ine kaydedilecek bir config dosyasÄ±dÄ±r. Bu yÃ¼zden admin kullanÄ±cÄ± adÄ± ve ÅŸifresini yapÄ±landÄ±rma dosyasÄ±nÄ±n iÃ§ine yazmamalÄ±yÄ±z.**

### 2) Secret ve Referans

Åimdi yapacaÄŸÄ±mÄ±z ÅŸey, deÄŸerleri referans alacaÄŸÄ±mÄ±z bir `secret` oluÅŸturmak olacak.

![](images/100.png)

Yani bu gizli dosya Kubernetes'te yer alacak ve kimse git reposunda buna eriÅŸemeyecek.
**Secret lives in K8s, not in the repository**

Åimdilik az Ã¶nce yazdÄ±ÄŸÄ±mÄ±z henÃ¼z bitmemiÅŸ olan deployment config dosyasÄ±nÄ± kaydedelim. AdÄ±nÄ± `mongo-depoyment.yml` olarak koyup kaydedelim.

Åimdi bu yapÄ±landÄ±rmayÄ± apply etmeden Ã¶nce, root kullanÄ±cÄ± adÄ± ve ÅŸifresinin yer alacaÄŸÄ± `Secret` oluÅŸturacaÄŸÄ±z. Yeni bir dosya oluÅŸturalÄ±m ve aÅŸaÄŸÄ±daki *Secret* configi yapÄ±ÅŸtÄ±ralÄ±m.

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: mongodb-secret
type: Opaque
data:
  username:
  password:
```

OldukÃ§a basit bir yapÄ±sÄ± var.
* `kind: Secret` var.
* `metadata` var, ve bu sadece adÄ±nÄ± iÃ§eriyor. Buna `mongodb-secret` diyeceÄŸiz.
* `type: Opaque` aslÄ±nda en temel `key-value` secret tÃ¼rÃ¼dÃ¼r. DiÄŸer tÃ¼rler Ã¶rneÄŸin TLS sertifikalarÄ±nÄ± iÃ§erir. Yani bir gizli dosya oluÅŸturabiliriz ve bunu TLS sertifikasÄ± tÃ¼rÃ¼nde yapabiliriz. Ancak Ã§oÄŸunlukla default tÃ¼rÃ¼ (Opaque) kullanacaÄŸÄ±z.
* `data` var ve burada `key-value` Ã§iftleri bulunuyor. Elbette bunlar bizim belirlediÄŸimiz isimler olacak. Mesela data kÄ±smÄ±nda deÄŸiÅŸiklik yapalÄ±m.
```yaml
data:
  mongo-root-username:
  mongo-root-password:
```
* KullanÄ±cÄ± adÄ±nÄ± `root-username` ve ÅŸifreyi `root-password` olarak belirleyeceÄŸiz.

> [!IMPORTANT]
>**Buradaki Ã¶nemli nokta, bu anahtar-deÄŸer Ã§iftlerindeki deÄŸerlerin dÃ¼z metin olmamasÄ±dÄ±r. Bir gizli dosya oluÅŸtururken deÄŸerlerin base64 ile kodlanmasÄ± gerekir.**

Bunu yapmanÄ±n en basit yolu terminale gitmektir. Burada, `echo -n` komutunu kullanacaÄŸÄ±z.
```bash
echo -n "username" | base64
echo -n "password" | base64
```
Buraya istediÄŸim dÃ¼z metin deÄŸerini koyalÄ±m. `username` ve `password` kullanalÄ±m ve base64 ile kodlayalÄ±m. Elde ettiÄŸimiz deÄŸerleri secret confige kopyalayalÄ±m.

![](images/101.png)

TamamladÄ±ktan sonra dosyayÄ± `mongo-secret.yaml` olarak kaydedelim.
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: mongodb-secret
type: Opaque
data:
  mongo-root-username: dXNlcm5hbWU=
  mongo-root-password: cGFzc3dvcmQ=
```

Åimdiye kadar sadece config dosyalarÄ± yazdÄ±k, clusterda henÃ¼z bir ÅŸey oluÅŸturmadÄ±k. Deploymenti referans alacaÄŸÄ±mÄ±z secret'tan Ã¶nce oluÅŸturmalÄ±yÄ±z. EÄŸer secret iÃ§ermeyen bir deployment oluÅŸturursak, hata alÄ±rÄ±z ve deployment baÅŸlatÄ±lamaz.

Ä°lk componentimiz hazÄ±r olduÄŸuna gÃ¶re, ÅŸimdi config dosyasÄ±ndan bir secret oluÅŸturabiliriz. Åu an her iki dosyamÄ±z da mevcut.

![](images/102.png)

AÅŸaÄŸÄ±daki komutu Ã§alÄ±ÅŸtÄ±rÄ±yoruz ve `secret` oluÅŸturuluyor.

```bash
kubectl apply -f mongo-secret-yaml
```

![](images/103.png)

SecretlarÄ±mÄ±zÄ± listelemek iÃ§in de aÅŸaÄŸÄ±daki komutu Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±mÄ±zda, Yeni Secret'Ä±mÄ±zÄ±n oluÅŸturulduÄŸunu gÃ¶rmeliyiz.

```bash
kubectl get secret
```

![](images/104.png)

ArtÄ±k Secret'Ä±mÄ±zÄ± deployment config dosyamÄ±zda referans alabiliriz. Config dosyamÄ±za geri dÃ¶nelim ve Secret'taki belirli `key-value` verilerini referans alalÄ±m.

```yaml
          env:
          - name: MONGO_INITDB_ROOT_USERNAME
            valueFrom:
              secretKeyRef:
                name: mongodb-secret
                key: mongo-root-username
          - name: MONGO_INITDB_ROOT_PASSWORD
            valueFrom:
              secretKeyRef:
                name: mongodb-secret
                key: mongo-root-password
```

* `value` yerine `valueFrom` yazÄ±yoruz ve altÄ±na `secretKeyRef` yazÄ±yoruz.
* `secretKeyRef`  name'i, `Secret` dosyamÄ±zÄ±n adÄ± olacak.
* `key` deÄŸerini almak iÃ§in de, Secret dosyamÄ±zdan isimleriyle referans alacaÄŸÄ±z. `mongo-root-username`, `mongo-root-password`.

![](images/105.png)

Bu ÅŸekilde referans almayÄ± tamamlÄ±yoruz.

* UnutmayÄ±n, YAML dosyasÄ± girintilere Ã§ok dikkat eder.

BÃ¶ylece root kullanÄ±cÄ± adÄ± ve ÅŸifresi, Secret'tan referans alÄ±nacak ve config dosyasÄ±nÄ±n iÃ§inde gerÃ§ek deÄŸerler bulunmayacak. Bu gÃ¼venlik aÃ§Ä±sÄ±ndan oldukÃ§a iyidir Ã§Ã¼nkÃ¼ kimlik bilgilerimizin kod reposunda bulunmasÄ±nÄ± asla istemeyiz.

Deployment dosyamÄ±z artÄ±k hazÄ±r, ArtÄ±k apply edebiliriz.

```bash
kubectl apply -f [YAML_file]
```

![](images/106.png)
![](images/107.png)

Deployment oluÅŸturuldu, yani `get all` komutunu Ã§alÄ±ÅŸtÄ±rÄ±rsam Pod'un baÅŸlatÄ±ldÄ±ÄŸÄ±nÄ±, deploymenti ve ReplicaSet'i gÃ¶rmeliyim.

Åimdi Pod'un statusunu kontrol edelim. Konteyner oluÅŸturuluyor, bu yÃ¼zden izlememiz gerekiyor. EÄŸer uzun sÃ¼rerse ve bir sorun olup olmadÄ±ÄŸÄ±nÄ± gÃ¶rmek isterseniz, `kubectl describe pod` ve Pod adÄ±mÄ±zÄ± yazÄ±yoruz.

```bash
kubectl describe pod [POD_NAME]
```

![](images/108.png)

GÃ¶rÃ¼yoruz ki sadece image Ã§ekiyor, bu yÃ¼zden uzun sÃ¼rÃ¼yor.

Tekrar `kubectl get pod` komutunu Ã§alÄ±ÅŸtÄ±rÄ±rsak, Pod'un Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶receÄŸiz. Åu anda bir MongoDB deployment ve bir replika Pod Ã§alÄ±ÅŸÄ±yor.

![](images/109.png)

### 3) MongoDB Internal Service

![](images/110.png)

Åimdiki adÄ±mda, diÄŸer componentlerin veya diÄŸer Pod'larÄ±n bu MongoDB ile konuÅŸabilmesi iÃ§in bir `internal service` oluÅŸturacaÄŸÄ±z. Ã–nce servisconfiguration oluÅŸturalÄ±m.

YAML dosyamÄ±za geri dÃ¶nelim.

> [!TIP]
> **YAML'da tek dosyada, birden fazla yaml yazabiliriz. YAML'da 3 tire `---`  belge ayÄ±rma sÃ¶zdizimidir. Yani yeni bir belge baÅŸladÄ±ÄŸÄ±nÄ± belirtmiÅŸ oluyoruz.**

Hatta deployment ve servisi aynÄ± config dosyasÄ±na koyabiliriz Ã§Ã¼nkÃ¼ genelde bu iki dosya birlikte bulunur.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodb-deployment
  labels:
    app: mongodb
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
      - name: mongodb
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-username
        - name: MONGO_INITDB_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-password
---
apiVersion: v1
kind: Service
metadata:
  name: mongodb-service
spec:
  selector:
    app: mongodb
  ports:
    - protocol: TCP
      port: 27017
      targetPort: 27017
```

Bu MongoDB iÃ§in bir servis. BazÄ± nitelikleri gÃ¶zden geÃ§irelim.

* `kind: Service` ve `name: mongodb-service` olarak adlandÄ±rdÄ±k.
* `selector` Ã¶nemli Ã§Ã¼nkÃ¼ biz bu oluÅŸturacaÄŸÄ±mÄ±z servisin Pod'a baÄŸlanmasÄ±nÄ± istiyoruz ve bunu yapmanÄ±n yolu Servisten `selector`, Deployment'tan `label` kullanmaktÄ±r. Bu etiketleri kullanarak, deployment ve Pod'un sahip olduÄŸu labellarÄ±, servis bulup baÄŸlanabilir.
* `ports:` Burada servisin baÄŸlantÄ± noktasÄ±nÄ± (port) aÃ§Ä±yoruz.
	* `port: 27017` Servis portudur.
	* `targetPort: 27017` Konteyner veya Pod portudur.
	Deployment config iÃ§erisindeki `containerPort` ile, Servisteki `targetPort` birbiriyle eÅŸleÅŸmelidir. Tabii ki, bu iki baÄŸlantÄ± noktasÄ± *farklÄ± olabilir*, ama biz bu seferlik aynÄ± portu kullanalÄ±m.

Hadi ÅŸimdi servisi oluÅŸturalÄ±m. Bu dosyayÄ± kaydedelim ve aynÄ± dosyayÄ± tekrar uygulayalÄ±m. BÃ¶ylece az Ã¶nce deployment oluÅŸturulduÄŸu gibi ÅŸimdi de servis oluÅŸacak. BakalÄ±m ne olacak?

![](images/111.png)

Hem daÄŸÄ±tÄ±m hem de servis yapÄ±landÄ±rmasÄ± olmasÄ±na raÄŸmen, Kubernetes deploymenti deÄŸiÅŸtirmediÄŸimizi anlÄ±yor. Bu nedenle sadece servisi oluÅŸturuyor.

Åimdi servisimizin oluÅŸturulduÄŸunu kontrol edelim.

```bash
kubectl get services
```

![](images/112.png)

OluÅŸan servisimiz 27017 portunda dinliyor. HatÄ±rlarsanÄ±z, servisin doÄŸru Pod'a baÄŸlÄ± olup olmadÄ±ÄŸÄ±nÄ± da kontrol edebiliyorduk. Bunu yapmak iÃ§in;

```bash
kubectl describe service [SERVICE_NAME]
```

![](images/113.png)

Burada bir IP adresi ve Pod'un iÃ§inde dinleyen uygulama portu var. Bu doÄŸru Pod olup olmadÄ±ÄŸÄ±nÄ± kontrol edelim.  `-o wide` ile ek Ã§Ä±ktÄ±yla Ã§alÄ±ÅŸtÄ±rÄ±yorum.

```bash
kubectl get pod -o wide
```

![](images/114.png)

Bu IP adresi, Pod IP adresi ile eÅŸleÅŸiyor ve uygulamanÄ±n Pod iÃ§inde dinlediÄŸi port doÄŸru.

Her ÅŸey mÃ¼kemmel ÅŸekilde ayarlandÄ±. MongoDB daÄŸÄ±tÄ±mÄ± ve servisi oluÅŸturuldu.

> [!TIP]
> Bir uygulamanÄ±n tÃ¼m componentlerini gÃ¶rmek istersek, `kubectl get all` ve `grep` komutunu birlikte kullanabiliriz. BÃ¶ylece hem tÃ¼m componentleri listeleyip hem de isme gÃ¶re  filtreleyebiliriz.

![](images/115.png)

Ä°ÅŸte bu ÅŸekilde tek komutla servisimizi, deploymentÄ±mÄ±zÄ±, replicasetimizi ve Podumuzu gÃ¶rÃ¼ntÃ¼leyebiliriz. TÃ¼m component tipleri burada gÃ¶rÃ¼nÃ¼yor.

### 4) Mongo Express Deployment & Service & ConfigMap

![](images/116.png)

Åimdi sÄ±radaki adÄ±mda, `Mongo Express Deployment ve Servisi` oluÅŸturacaÄŸÄ±z. AyrÄ±ca MongoDB iÃ§in veritabanÄ± URL'sini iÃ§eren `External Configuration` oluÅŸturacaÄŸÄ±z. Yeni bir dosya oluÅŸturarak baÅŸlayalÄ±m.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongo-express
  labels:
    app: mongo-express
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongo-express
  template:
    metadata:
      labels:
        app: mongo-express
    spec:
      containers:
      - name: mongo-express
        image: mongo-express
```

Bu, MongoExpress deployment taslaÄŸÄ±. Ä°sim `express`.
* `template:` Pod tanÄ±mÄ±mÄ±z var. Image adÄ± `express`. Bu imajÄ± da kontrol edelim. image adÄ± `mongo-express`.

*https://hub.docker.com/_/mongo-express*

![](images/117.png)

Konteyner iÃ§indeki MongoExpress Port 8081'deymiÅŸ.
Biraz aÅŸaÄŸÄ± inersek de Ã§evresel deÄŸiÅŸkenleri(environmental variables) gÃ¶rebiliriz.

* MongoExpress iÃ§in Ã¼Ã§ ÅŸeye ihtiyacÄ±mÄ±z var: MongoDB adresi(veritabanÄ± adresi) ve kimlik doÄŸrulama bilgileri.
	* MongoDB Adresi                        ->          ...MONGODB_SERVER
	* Kimlik DoÄŸrulama BÄ°lgileri          ->          ...ADMIN_USERNAME, ...ADMIN_PASSWORD

![](images/118.png)

Bu Ã§evresel deÄŸiÅŸkenleri kullanacaÄŸÄ±z. O zaman ilk olarak konteyner portlarÄ±nÄ± aÃ§abiliriz.

```yaml
...  
...
  template:
    metadata:
      labels:
        app: mongo-express
    spec:
      containers:
      - name: mongo-express
        image: mongo-express
        ports:
        - containerPort: 8081
```

* Ä°Ã§erideki Pod'da birden fazla port aÃ§abileceÄŸimiz iÃ§in birden fazla port olabileceÄŸi anlamÄ±na gelen `ports` kullanÄ±lÄ±r. Bu deÄŸerimiz, 8081 olacak.

Åimdi baÄŸlantÄ± iÃ§in `ENV`(environment variables) ekleyeceÄŸiz.

```yaml
...
...
        ports:
        - containerPort: 8081
        env:
        - name: ME_CONFIG_MONGODB_ADMINUSERNAME
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-username
        - name: ME_CONFIG_MONGODB_ADMINPASSWORD
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-username
```

Tabiiki tanÄ±mladÄ±ÄŸÄ±mÄ±z aynÄ± kullanÄ±cÄ± adÄ± ve ÅŸifre, deploymentta tanÄ±mladÄ±ÄŸÄ±mÄ±z ile aynÄ± olacak. Bu yÃ¼zden yaptÄ±ÄŸÄ±mÄ±z ÅŸey onlarÄ± kopyalamak. AynÄ± `valueFrom` ile zaten orada olan secret iÃ§erisinden okumuÅŸ olacaÄŸÄ±z.

Son olarak database server:

```yaml
...
...
        - name: ME_CONFIG_MONGODB_ADMINPASSWORD
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-username
        - name: ME_CONFIG_MONGODB_SERVER
          value:
```

Bu da harici bir yapÄ±landÄ±rma olduÄŸu iÃ§in ya burada `value` kullanabilir ve mongodb sunucu adresini doÄŸrudan buraya yazabiliriz. Ya da diyagramda gÃ¶sterildiÄŸi gibi, merkezi bir yapÄ±landÄ±rma olan bir configmap'e koyabiliriz. Bu ÅŸekilde merkezi bir yerde saklanÄ±r ve diÄŸer componentler de kullanabilir.
* Ã–rneÄŸin, iki uygulamam mongodb veritabanÄ±nÄ± kullanÄ±yorsa, bu harici yapÄ±landÄ±rmayÄ± referans alabiliriz ve bir noktada deÄŸiÅŸiklik yapmamÄ±z gerekirse; sadece bir yerde deÄŸiÅŸiklik yaparak diÄŸer dosyalarÄ± gÃ¼ncellemeden devam edebiliriz.
Bu nedenle, ÅŸu an iÃ§in eksik olan bu deployment configi kenara kaydedip, mongodb sunucu adresini iÃ§eren configmap'i oluÅŸturacaÄŸÄ±z. Yeni bir dosya oluÅŸturalÄ±m, bu eksik daÄŸÄ±tÄ±mÄ± kaydedelim, adÄ±nÄ± `mongo-express.yaml` koyalÄ±m ve daha sonra geri dÃ¶nelim!

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mongodb-configmap
data:
  database_url:
```

Bu da secret key gibi oldukÃ§a basit. Beraber gÃ¶z atalÄ±m:
* `kind: ConfigMap` olan aynÄ± Secret gibi bir yapÄ± gÃ¶rÃ¼yoruz.

![](images/119.png)
* TÄ±pkÄ± yukarÄ±da gÃ¶rdÃ¼ÄŸÃ¼nÃ¼z gibi `data` iÃ§erisinde `key-value` Ã§ifti var.
* `type:` yok Ã§Ã¼nkÃ¼ sadece configmapin sadece bir tÃ¼rÃ¼ var. SonuÃ§ olarak veritabanÄ± URL'si ve server adÄ± aslÄ±nda Servisin adÄ±. Bu kadar basit.

Hizmetimize  `mongodb-service` adÄ±nÄ± vermiÅŸtik. Bu yÃ¼zden hizmet adÄ±nÄ± kopyalayalÄ±m ve bunu veritabanÄ± sunucusu URL'sine yazalÄ±m. DosyayÄ± mongo-configmap.yaml olarak kaydedelim.

> [!NOTE]
> Gizli anahtar gibi, Ã§alÄ±ÅŸtÄ±rma veya oluÅŸturma sÄ±rasÄ± Ã¶nemlidir. ConfigMap'in zaten clusterda olmasÄ± gerekiyor ki onu referans alabilelim. Bu yÃ¼zden iÅŸimiz bittiÄŸinde, Ã¶nce configmap'i oluÅŸturmalÄ± daha sonra deploymenti yapmalÄ±yÄ±z.

YapÄ±landÄ±rma haritasÄ±nÄ± deployment iÃ§inde referans almaya geldik. mongo-express.yaml dosyamÄ±za geri dÃ¶nelim.

> [!TIP]
> YapÄ±landÄ±rma haritasÄ±nÄ± deployment iÃ§inde referans almanÄ±n yolu gizli anahtara Ã§ok benzer. Tek fark burada `secret` yerine `configMap` yazacaÄŸÄ±z. TamamÄ± kÃ¼Ã§Ã¼k ve bÃ¼yÃ¼k harf karÄ±ÅŸÄ±k olacak ve elbette adÄ± `config map` olacak.
>
> ![](images/120.png)

Deploymenti tamamladÄ±k. Åimdi Ã¶nce config map'i ve sonra Express daÄŸÄ±tÄ±mÄ±nÄ± oluÅŸturalÄ±m.

```bash
kubectl apply -f mongo-configmap.yaml
kubectl apply -f mongo-express.yaml
```

![](images/121.png)

Herhangi bir hata almadÄ±k. Pod'un durumuna bakalÄ±m.

```bash
kubectl get pod
```

![](images/122.png)

Konteyner oluÅŸturuluyor. Gayet iyi. Biraz bekleyip tekrar kontrol edelim ve bum!

![](images/123.png)

DoÄŸru Ã§alÄ±ÅŸÄ±p Ã§alÄ±ÅŸmadÄ±ÄŸÄ±nÄ± Ã¶ÄŸrenmek iÃ§in LoglarÄ± gÃ¶rÃ¼ntÃ¼lemek Ã¶nemlidir.

```bash
kubectl logs [POD_NAME]
```

![](images/124.png)

### 5) Mongo Express External Service

![](images/131.png)

Åimdi son adÄ±mÄ±mÄ±z, Express'e bir tarayÄ±cÄ±dan eriÅŸmektir. Bunu yapmak iÃ§in mongo-express iÃ§in bir External Service ihtiyacÄ±mÄ±z olacak. Ã–yleyse hadi bunu da oluÅŸturalÄ±m.

>[!TIP]
>Daha Ã¶nce yaptÄ±ÄŸÄ±mÄ±z gibi yine MongoExpress servisimizi, deploymentÄ± ile aynÄ± dosyada oluÅŸturacaÄŸÄ±z. Ã‡Ã¼nkÃ¼ zaten pratikte hiÃ§bir zaman servis olmadan deploymentÄ± olmaz. Bu yÃ¼zden onlarÄ± aynÄ± yaml dosyasÄ±nda yazmak mantÄ±klÄ±dÄ±r.

```yaml
---
apiVersion: v1
kind: Service
metadata:
  name: mongo-express-service
spec:
  selector:
    app: mongo-express
  ports:
    - protocol: TCP
      port: 8081
      targetPort: 8081
```

Deployment dosyamÄ±zÄ±n devamÄ±na yukarÄ±daki servis configi yapÄ±ÅŸtÄ±rÄ±yoruz. Bu, Mongo Express'in external servisi ve bu farkettiyseniz tamamen mongodb servisinin yapÄ±landÄ±rmasÄ±yla aynÄ± gÃ¶rÃ¼nÃ¼yor.

![](images/125.png)


* Servis portunu 8081 olarak aÃ§tÄ±k ve yine target Port, container Port'un dinlediÄŸi yerdir.

**Peki bu servisi external(harici) yapacak olan ÅŸey nedir?**
Bu servisi iki ÅŸey yaparak external yaparÄ±z:

* *1) `spec` bÃ¶lÃ¼mÃ¼nde, `selector` altÄ±na `type: Loadbalancer` yazarÄ±z.

>[!INFO]
>SanÄ±yoruz ki `external servis` iÃ§in bu isim "LoadBalancer" olarak dÃ¼zgÃ¼n seÃ§ilmemiÅŸ Ã§Ã¼nkÃ¼ `internal servis` de istekleri dengeleyip Load Balance yapabilir. Ä°ki mongodb podumuz olsa, internal servis de bu podlara gelen istekleri dengeleyebilirdi. Yani gerÃ§ekten de tÃ¼r adÄ± olarak `Load Balancer` seÃ§ilmesi Ã§ok iyi deÄŸil gibi Ã§Ã¼nkÃ¼ kafa karÄ±ÅŸÄ±klÄ±ÄŸÄ±na neden olabilir. Ancak, bu yÃ¼k dengeleyici tÃ¼rÃ¼ basitÃ§e servise `external IP adres`i atar ve `external request`leri kabul eder.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: mongo-express-service
spec:
  selector:
    app: mongo-express
  type: LoadBalancer         ## Tam burasÄ±..
  ports:
    - protocol: TCP
      port: 8081
      targetPort: 8081
```

* 2) `nodePort: 30000`

YapacaÄŸÄ±mÄ±z ikinci ÅŸey ise, bu servisi harici yapmak iÃ§in Ã¼Ã§Ã¼ncÃ¼ bir port saÄŸlamak. Bu da node port olacak ve bu, External IP adresinin aÃ§Ä±k olacaÄŸÄ± port olacak. AyrÄ±ca belirtelim ki bu port, TarayÄ±cÄ±dan eriÅŸmek iÃ§in kullanacaÄŸÄ±mÄ±z port olacak.

Bu port aslÄ±nda bir aralÄ±ÄŸa sahiptir ve bu aralÄ±k 30000 ile 32767 arasÄ±ndadÄ±r, yani bu aralÄ±kta bir port vermemiz gerekiyor, bu yÃ¼zden sadece 30000 ile gidelim, aralÄ±ktaki minimumdur.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: mongo-express-service
spec:
  selector:
    app: mongo-express
  type: LoadBalancer
  ports:
    - protocol: TCP
      port: 8081
      targetPort: 8081
      nodePort: 30000
```

ArtÄ±k hazÄ±r olduÄŸumuza gÃ¶re uygulamaya geÃ§ebiliriz. ArdÄ±ndan bu baÄŸlantÄ± noktalarÄ±nÄ±n nasÄ±l farklÄ± olduklarÄ±na bakalÄ±m.

```bash
kubectl apply -f mongo-express.yaml
```

Hizmet oluÅŸturuldu ve hizmeti `kubectl get service` ile gÃ¶rÃ¼ntÃ¼lersek, Ã¶nceden oluÅŸturduÄŸumuz mongodb servisinin `ClusterIP`' tipinde olduÄŸunu gÃ¶rÃ¼yoruz ve yeni oluÅŸturduÄŸumuz mongo express servisinin bir `LoadBalancer` olduÄŸunu gÃ¶rÃ¼yoruz.

```bash
c3ng0@ubn:~/hxhdle/kubernetto$ kubectl get service
NAME                    TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
kubernetes              ClusterIP      10.96.0.1        <none>        443/TCP          6d1h
mongo-express-service   LoadBalancer   10.96.100.133    <pending>     8081:30000/TCP   9s
mongodb-service         ClusterIP      10.111.249.213   <none>        27017/TCP        95m
```

Internal servis oluÅŸtururken herhangi bir tÃ¼r belirtmedik Ã§Ã¼nkÃ¼ zaten default olarak internal IP hizmeti tÃ¼rÃ¼dÃ¼r.

>[!NOTE]
>Ä°nternal Servis oluÅŸtururken `type` belirtmeye gerek yok Ã§Ã¼nkÃ¼ zaten default olarak tanÄ±mlanÄ±r.
>![](images/126.png)

Fark ise;
* ClusterIP, servise internal IP adresi verir. AÅŸaÄŸÄ±da gÃ¶rÃ¼ldÃ¼ÄŸÃ¼ gibi:

```bash
mongodb-service         ClusterIP      10.111.249.213   <none>        27017/TCP        95m
```

* LoadBalancer, servise bir internal IP adresi verir, ancak bununla birlikte external isteklerin geleceÄŸi external IP adresi de verir. *(pending)*

```bash
mongo-express-service   LoadBalancer   10.96.100.133    <pending>     8081:30000/TCP   9s
```

* Åu an bize `pending` diyor Ã§Ã¼nkÃ¼ minicube'deyiz ve bu normal kubernetes kurulumunda biraz daha farklÄ± Ã§alÄ±ÅŸÄ±r.  

DediÄŸim gibi, "pending" durumu external IP adresini henÃ¼z almadÄ±ÄŸÄ± anlamÄ±na gelir. Bu durumu Minikube'da yapmanÄ±n yolu `minikube service` komutunu kullanmaktÄ±r ve servisin adÄ±na ihtiyacÄ±mÄ±z olacak.

```bash
minikube service mongo-express-service
```

Bu komut temelde external servisimize bir genel IP adresi atayacak.
Komutu Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±mÄ±zda tarayÄ±cÄ± aÃ§Ä±lacak ve karÅŸÄ±mÄ±za ÅŸu sayfa Ã§Ä±kacak:

![](images/127.png)

EÄŸer aÃ§Ä±lÄ±rken kullanÄ±cÄ± adÄ± ve ÅŸifre sorarsa `admin:pass` olarak yazabiliriz.

Ve Mongo Express sayfamÄ±zÄ± gÃ¶rÃ¼yoruz. Komut satÄ±rÄ±na geri dÃ¶nersek, buradaki bu komut Express servisini Public IP adresli URL atadÄ±ÄŸÄ±nÄ± ve bizim belirttiÄŸimiz 30000 portunu kullandÄ±ÄŸÄ±nÄ± gÃ¶rÃ¼yoruz.

![](images/128.png)

Burada deÄŸiÅŸiklikler yaparsak, Ã¶rneÄŸin yeni bir veritabanÄ± oluÅŸturalÄ±m, ona testDB adÄ±nÄ± verelim ve Create Database ile isteÄŸi gÃ¶nderelim.

![](images/129.png)

Arka planda olan ÅŸeyi ÅŸu ÅŸekilde aÃ§Ä±klayabiliriz.
* Bu isteÄŸin Mongo Express'in external servisine ulaÅŸmasÄ±
* ArdÄ±ndan Mongo Express poduna yÃ¶nlendirilmesidir.
* Express podu, Internal servis olan mongodb servisine baÄŸlanÄ±r.
* Mongodb servisi, isteÄŸimizi sonunda mongodb Poduna iletir.
* Sonra tÃ¼m bu yol geri gelir ve burada deÄŸiÅŸiklikleri gÃ¶rÃ¼rÃ¼z.

![](images/130.png)

Basit bir uygulama kurulumunu bir Kubernetes kÃ¼mesinde nasÄ±l daÄŸÄ±tacaÄŸÄ±nÄ±zÄ± bÃ¶yle anlatmÄ±ÅŸ olduk.
